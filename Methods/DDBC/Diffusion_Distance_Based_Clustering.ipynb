{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Documentation\n",
    "* Resolution = 1.0\n",
    "* aggregated sim matrix with 0.5 itp\n",
    "* leiden clustering\n",
    "* final avg times [2,4,6,8,10,12]\n",
    "* seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from scipy.sparse import load_npz,save_npz,diags,csr_matrix\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "import os\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from pypdf import PdfReader, PdfWriter\n",
    "from tempfile import NamedTemporaryFile\n",
    "import networkx as nx\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import community as community_louvain\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import gc\n",
    "from pympler import muppy, asizeof\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Building Multilayer Transition Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the matrices needed\n",
    "DISEASE = \"BIPOLAR\"\n",
    "OUTPUT_DIRECTORY = f\"../output/{DISEASE}/\"\n",
    "DGIDB_DIRECTORY = f\"../../Gen_Hypergraph/output/DGIDB_{DISEASE}/\"\n",
    "MSIGDB_DIRECTORY = \"../../Gen_Hypergraph/output/MSigDB_Full/\"\n",
    "\n",
    "## DGIDB\n",
    "DGIDB_binary_matrix = load_npz(DGIDB_DIRECTORY + \"hypergraph_incidence_matrix_binary.npz\")\n",
    "DGIDB_weighted_matrix = load_npz(DGIDB_DIRECTORY + \"hypergraph_incidence_matrix_weighted.npz\")\n",
    "DGIDB_gene_weight_diag_matrix = load_npz(DGIDB_DIRECTORY + \"diag_gene_weight_matrix.npz\")\n",
    "DGIDB_diag_node_degree_matrix = load_npz(DGIDB_DIRECTORY + \"diag_node_degree_matrix.npz\")\n",
    "DGIDB_inverse_diag_edge_degree_matrix = load_npz(\n",
    "    DGIDB_DIRECTORY + \"inverse_diag_edge_degree_matrix.npz\"\n",
    "    )\n",
    "\n",
    "## MSIGDB\n",
    "MSIGDB_binary_matrix = load_npz(MSIGDB_DIRECTORY + \"hypergraph_incidence_matrix_binary.npz\")\n",
    "MSIGDB_weighted_matrix = load_npz(MSIGDB_DIRECTORY + \"hypergraph_incidence_matrix_weighted.npz\")\n",
    "MSIGDB_gene_weight_diag_matrix = load_npz(MSIGDB_DIRECTORY + \"gene_weight_diag_matrix.npz\")\n",
    "MSIGDB_diag_node_degree_matrix = load_npz(MSIGDB_DIRECTORY + \"diag_node_degree_matrix.npz\")\n",
    "MSIGDB_inverse_diag_edge_degree_matrix = load_npz(\n",
    "    MSIGDB_DIRECTORY + \"inverse_diag_edge_degree_matrix.npz\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# queue = [[i] for i in range(1,25)]\n",
    "# resolution = 1.0\n",
    "# for T in queue:\n",
    "#     path = f\"../output/{DISEASE}/diffusion_dist_matrices/ddm_{T}_res-{resolution}.npy\"\n",
    "#     arr = np.load(path)\n",
    "#     arr = arr.astype(np.float32)\n",
    "#     np.save(path, arr)\n",
    "#     print(f\"Overwritten (float32): {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful Functions\n",
    "def csr_equal_tol(A, B, atol=1e-8):\n",
    "    # First check shapes and sparsity pattern\n",
    "    if A.shape != B.shape or not np.array_equal(A.indptr, B.indptr) or not np.array_equal(A.indices, B.indices):\n",
    "        return False\n",
    "    # Compare numeric values within tolerance\n",
    "    return np.allclose(A.data, B.data, atol=atol, rtol=0)\n",
    "\n",
    "def is_symmetric(W,tol = 1e-8):\n",
    "    diff = (W - W.T)\n",
    "    check = np.all(np.abs(diff.data) < tol)\n",
    "    return check\n",
    "\n",
    "def degree_array(W, a=1):\n",
    "    return np.asarray(W.sum(axis=a)).ravel()\n",
    "\n",
    "def degree_diagonal_matrix(W, a=1):\n",
    "    d = degree_array(W,a)\n",
    "    return sp.diags(d, offsets=0, format='csr')\n",
    "\n",
    "def symmetrically_normalize(W, a=1):\n",
    "    D = np.asarray(W.sum(axis=a)).ravel()\n",
    "    D_inv_sqrt = np.zeros_like(D)\n",
    "    nze = D != 0\n",
    "    D_inv_sqrt[nze] = 1 / np.sqrt(D[nze])\n",
    "\n",
    "    W_sym = W.multiply(D_inv_sqrt)              # scale columns\n",
    "    W_sym = W_sym.multiply(D_inv_sqrt[:, None]) # scale rows    \n",
    "    return W_sym.tocsr()\n",
    "\n",
    "def big_objects(n=10, min_mb=1):\n",
    "    \"\"\"\n",
    "    Show the largest objects currently in memory.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int\n",
    "        Number of top objects to show.\n",
    "    min_mb : float\n",
    "        Minimum size (in MB) to include.\n",
    "    \"\"\"\n",
    "    import sys\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import scipy.sparse as sp\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    def get_size(obj):\n",
    "        try:\n",
    "            if isinstance(obj, np.ndarray):\n",
    "                return obj.nbytes\n",
    "            elif isinstance(obj, pd.DataFrame) or isinstance(obj, pd.Series):\n",
    "                return obj.memory_usage(deep=True).sum()\n",
    "            elif sp.issparse(obj):\n",
    "                return (obj.data.nbytes +\n",
    "                        obj.indptr.nbytes +\n",
    "                        obj.indices.nbytes)\n",
    "            else:\n",
    "                return sys.getsizeof(obj)\n",
    "        except Exception:\n",
    "            return 0\n",
    "\n",
    "    ip = get_ipython()\n",
    "    if ip is None:\n",
    "        ns = globals()\n",
    "    else:\n",
    "        ns = ip.user_ns\n",
    "\n",
    "    items = []\n",
    "    for name, val in ns.items():\n",
    "        if name.startswith('_'):\n",
    "            continue  # skip internals\n",
    "        size = get_size(val)\n",
    "        if size > min_mb * 1024 ** 2:\n",
    "            items.append((name, type(val).__name__, size))\n",
    "\n",
    "    items.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    print(f\"{'Variable':30s} {'Type':25s} {'Size (MB)':>10s}\")\n",
    "    print(\"-\" * 70)\n",
    "    for name, t, size in items[:n]:\n",
    "        print(f\"{name:30s} {t:25s} {size / 1024 ** 2:10.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Adjacency Matrices\n",
    "## DGIDB\n",
    "H,W_v,D_v,D_e_inv = DGIDB_weighted_matrix, DGIDB_gene_weight_diag_matrix, DGIDB_diag_node_degree_matrix, DGIDB_inverse_diag_edge_degree_matrix\n",
    "\n",
    "# Construct D_v^(-1/2)\n",
    "d = (D_v @ W_v).diagonal()\n",
    "d_inv_sqrt = np.zeros_like(d)\n",
    "nonzero_mask = d > 0\n",
    "d_inv_sqrt[nonzero_mask] = 1.0 / np.sqrt(d[nonzero_mask])\n",
    "D_v_sqrt_inv = diags(d_inv_sqrt)\n",
    "\n",
    "DGIDB_adjacency_matrix = D_v_sqrt_inv @ H @ D_e_inv @ H.T @ D_v_sqrt_inv\n",
    "\n",
    "\n",
    "## MSIGDB\n",
    "H,W_v,D_v,D_e_inv = MSIGDB_weighted_matrix, MSIGDB_gene_weight_diag_matrix, MSIGDB_diag_node_degree_matrix, MSIGDB_inverse_diag_edge_degree_matrix\n",
    "\n",
    "# Construct D_v^(-1/2)\n",
    "d = (D_v @ W_v).diagonal()\n",
    "d_inv_sqrt = np.zeros_like(d)\n",
    "nonzero_mask = d > 0\n",
    "d_inv_sqrt[nonzero_mask] = 1.0 / np.sqrt(d[nonzero_mask])\n",
    "D_v_sqrt_inv = diags(d_inv_sqrt)\n",
    "\n",
    "MSIGDB_adjacency_matrix = D_v_sqrt_inv @ H @ D_e_inv @ H.T @ D_v_sqrt_inv\n",
    "\n",
    "# Compute Degree Diagonal Matrices\n",
    "DGIDB_rows_sums = degree_array(DGIDB_adjacency_matrix)\n",
    "MSIGDB_rows_sums = degree_array(MSIGDB_adjacency_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(is_symmetric(DGIDB_adjacency_matrix,tol=1e-7))\n",
    "print(is_symmetric(MSIGDB_adjacency_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Symmetric Normalization\n",
    "DGIDB_adjacency_matrix = symmetrically_normalize(DGIDB_adjacency_matrix)\n",
    "MSIGDB_adjacency_matrix = symmetrically_normalize(MSIGDB_adjacency_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(is_symmetric(DGIDB_adjacency_matrix,tol=1e-7))\n",
    "print(is_symmetric(MSIGDB_adjacency_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# density = DGIDB_adjacency_matrix.nnz / (DGIDB_adjacency_matrix.shape[0] * DGIDB_adjacency_matrix.shape[1])\n",
    "# print(\"DGIDB Density:\", density)\n",
    "# density = MSIGDB_adjacency_matrix.nnz / (MSIGDB_adjacency_matrix.shape[0] * MSIGDB_adjacency_matrix.shape[1])\n",
    "# print(\"MSIGDB Density:\", density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check nonzero average\n",
    "DGIDB_nonzero_average = DGIDB_adjacency_matrix[DGIDB_adjacency_matrix != 0].mean()\n",
    "MSIGDB_nonzero_average = MSIGDB_adjacency_matrix[MSIGDB_adjacency_matrix != 0].mean()\n",
    "\n",
    "print(\"DGIDB nonzero average:\",DGIDB_nonzero_average,\"\\nMSIGDB nonzero average:\",MSIGDB_nonzero_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Density normalization version 1\n",
    "# target_average = MSIGDB_nonzero_average\n",
    "# DGIDB_adjacency_matrix = (target_average / DGIDB_nonzero_average) * DGIDB_adjacency_matrix\n",
    "# MSIGDB_adjacency_matrix = (target_average / MSIGDB_nonzero_average) * MSIGDB_adjacency_matrix\n",
    "\n",
    "# DGIDB_nonzero_average = DGIDB_adjacency_matrix[DGIDB_adjacency_matrix != 0].mean()\n",
    "# MSIGDB_nonzero_average = MSIGDB_adjacency_matrix[MSIGDB_adjacency_matrix != 0].mean()\n",
    "\n",
    "# print(DGIDB_nonzero_average,MSIGDB_nonzero_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density normalization version 2\n",
    "DGIDB_mean_degree = DGIDB_rows_sums[DGIDB_rows_sums != 0].mean()\n",
    "MSIGDB_mean_degree = MSIGDB_rows_sums[MSIGDB_rows_sums != 0].mean()\n",
    "print(DGIDB_mean_degree,MSIGDB_mean_degree)\n",
    "# print(DGIDB_rows_sums,MSIGDB_rows_sums)\n",
    "\n",
    "DGIDB_weight = (1/DGIDB_mean_degree) / ((1/DGIDB_mean_degree)+(1/MSIGDB_mean_degree))\n",
    "MSIGDB_weight = (1/MSIGDB_mean_degree) / ((1/DGIDB_mean_degree)+(1/MSIGDB_mean_degree))\n",
    "geo_mean_weight = (DGIDB_weight * MSIGDB_weight)**(1/2)\n",
    "print(DGIDB_weight,MSIGDB_weight)\n",
    "print(geo_mean_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build interlayer coupling matrices between the two layers\n",
    "\n",
    "# Open the JSON file and load its content into a dictionary\n",
    "with open(DGIDB_DIRECTORY + f\"gene_to_index_{DISEASE}.json\", \"r\") as file:\n",
    "    DGIDB_gene_to_index = json.load(file)\n",
    "with open(MSIGDB_DIRECTORY + \"gene_to_index.json\", \"r\") as file:\n",
    "    MSIGDB_gene_to_index = json.load(file)\n",
    "    \n",
    "# Jump probability for matching genes\n",
    "w = 1\n",
    "\n",
    "# Number of genes (assuming they are both of same size or matchable)\n",
    "num_genes_dgidb = len(DGIDB_gene_to_index)\n",
    "num_genes_msigdb = len(MSIGDB_gene_to_index)\n",
    "\n",
    "# Initialize the inter-layer matrix with zeros\n",
    "interlayer_transition_matrix = np.zeros((num_genes_msigdb,num_genes_dgidb))\n",
    "i = 0\n",
    "# Build the inter-layer matrix\n",
    "for gene_dgidb, idx_dgidb in DGIDB_gene_to_index.items():\n",
    "    # If the gene exists in both gene-to-index mappings\n",
    "    if gene_dgidb in MSIGDB_gene_to_index:      \n",
    "        idx_msigdb = MSIGDB_gene_to_index[gene_dgidb]\n",
    "        interlayer_transition_matrix[idx_msigdb,idx_dgidb] = w  # Set jump probability\n",
    "        i += 1\n",
    "    else:\n",
    "        print(f\"Gene {gene_dgidb} not found in MSIGDB mapping.\")\n",
    "rows_with_high_sum = np.where(interlayer_transition_matrix.sum(axis=1) > 0)[0]\n",
    "print(i/len(DGIDB_gene_to_index), \"of DGIDB genes have a match in MSIGDB\")\n",
    "\n",
    "interlayer_transition_matrix = interlayer_transition_matrix.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(interlayer_transition_matrix.shape)\n",
    "print(len(DGIDB_gene_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check for Row-stochastic\n",
    "# row_sums = np.array(DGIDB_adjacency_matrix.sum(axis=1)).ravel()\n",
    "# print(row_sums)\n",
    "# ok = np.all(np.isclose(row_sums, 1.0)|np.isclose(row_sums, 0))\n",
    "# print(\"Every row sums to 0 or 1?\", ok)\n",
    "\n",
    "# row_sums = np.array(MSIGDB_adjacency_matrix.sum(axis=1)).ravel()\n",
    "# print(row_sums)\n",
    "# ok = np.all(np.isclose(row_sums, 1.0)|np.isclose(row_sums, 0))\n",
    "# print(\"Every row sums to 0 or 1?\", ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build the multilayer transition matrix\n",
    "# interlayer_transition_prob = target_average\n",
    "interlayer_transition_prob = 0.5\n",
    "\n",
    "A = (1-interlayer_transition_prob) * DGIDB_weight * DGIDB_adjacency_matrix\n",
    "B = interlayer_transition_prob * geo_mean_weight * interlayer_transition_matrix.T\n",
    "C = interlayer_transition_prob * geo_mean_weight * interlayer_transition_matrix\n",
    "D =(1-interlayer_transition_prob) * MSIGDB_weight * MSIGDB_adjacency_matrix\n",
    "\n",
    "multilayer_transition_matrix = sp.bmat([\n",
    "    [A, B],\n",
    "    [C, D]\n",
    "]).tocsr()\n",
    "\n",
    "num_genes = multilayer_transition_matrix.shape[0]\n",
    "\n",
    "multilayer_transition_matrix = multilayer_transition_matrix.astype(np.float32)\n",
    "\n",
    "del A,B,C,D, DGIDB_adjacency_matrix,MSIGDB_adjacency_matrix\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "density = multilayer_transition_matrix.nnz / (multilayer_transition_matrix.shape[0] * multilayer_transition_matrix.shape[1])\n",
    "print(\"MTM Density:\", density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "MTM_average = multilayer_transition_matrix[multilayer_transition_matrix != 0].mean()\n",
    "print(\"MTM nonzero average:\",MTM_average)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "# Computing Eigenvalues & Eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_eigenvalues = 100\n",
    "num_neighbors = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute top k eigenvalues by magnitude\n",
    "def top_k_eigenvalues(M,k):\n",
    "    vals, vecs = eigsh(M, k=k, which='LM')  # 'LM' = Largest Magnitude\n",
    "    idx = np.argsort(np.abs(vals))[::-1]\n",
    "    vals, vecs = vals[idx], vecs[:, idx]\n",
    "    return vals, vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = top_k_eigenvalues(multilayer_transition_matrix,num_eigenvalues)\n",
    "print(\"Eigenvalues:\", eigenvalues)\n",
    "print(\"Eigenvectors shape:\", eigenvectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "# KNN Graph Methods & Testing (commented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stationary_distribution(W, tol=1e-12, maxit=10000):\n",
    "    # power iteration for left stationary of row-stochastic W\n",
    "    n = W.shape[0]\n",
    "    pi = np.ones(n) / n\n",
    "    for _ in range(maxit):\n",
    "        pi_next = pi @ W\n",
    "        if np.linalg.norm(pi_next - pi, 1) < tol:\n",
    "            break\n",
    "        pi = pi_next\n",
    "    return pi / pi.sum()\n",
    "\n",
    "def layer_weights_stationary(W, n):\n",
    "    pi = stationary_distribution(W)\n",
    "    w1 = float(pi[:n].sum()); w2 = 1.0 - w1\n",
    "    return w1, w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1,w2 = layer_weights_stationary(multilayer_transition_matrix, num_genes_dgidb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w1,w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build diffusion matrix from eigenvalues and eigenvectors (double check)\n",
    "def build_diffusion_dist_matrix(vals,vecs,t):\n",
    "    X_t = vecs * (vals ** (t))\n",
    "    D_t2 = squareform(pdist(X_t, metric='sqeuclidean'))\n",
    "    D_t2 = D_t2.astype(np.float32)\n",
    "    return D_t2\n",
    "\n",
    "def build_diffusion_dist_matrix_avg(vals,vecs,T):\n",
    "    diffusion_dist_matrix = np.zeros((num_genes,num_genes))\n",
    "    for t in T:\n",
    "        np.add(diffusion_dist_matrix, \n",
    "            build_diffusion_dist_matrix(vals,vecs,t), \n",
    "            out=diffusion_dist_matrix)\n",
    "    diffusion_dist_matrix = diffusion_dist_matrix / len(T)\n",
    "    return diffusion_dist_matrix\n",
    "\n",
    "# Build kNN\n",
    "def build_kNN(diffusion_dist_matrix,k, sym_method='average'):\n",
    "    # Choose sigma to be the median of pair-wise distance\n",
    "    tmp = diffusion_dist_matrix.astype(np.float32, copy=True)\n",
    "    np.sqrt(tmp, out=tmp)\n",
    "    sigma = np.median(tmp[tmp != 0], overwrite_input=True)\n",
    "\n",
    "    n = diffusion_dist_matrix.shape[0]\n",
    "\n",
    "    # Used to indicate position of nonzero value needed to record (constructing a sparse matrix for efficiency)\n",
    "    rows, cols, vals = [], [], []\n",
    "    for i in range(n):\n",
    "        profile = np.exp(-diffusion_dist_matrix[i] / (2* (sigma**2))) \n",
    "        idx = np.argpartition(-profile, k+1)[:k+1]  # top-k+1 (includes self)\n",
    "        idx = idx[idx != i]                      # drop self\n",
    "        rows += [i]*k\n",
    "        cols += list(idx[:k])\n",
    "        vals += list(profile[idx[:k]])\n",
    "    adj_mat = csr_matrix((vals, (rows, cols)), shape=(n, n))\n",
    "\n",
    "    # Symmetrize the matrix by adding edges to one way edges and set weight to average\n",
    "    if (sym_method == 'average'):\n",
    "        adj_mat = (adj_mat + adj_mat.T).multiply(0.5).tocsr()\n",
    "\n",
    "    # Symmetrize the matrix by unioning\n",
    "    elif (sym_method == 'union'):\n",
    "        adj_mat = adj_mat.maximum(adj_mat.T)\n",
    "    elif (sym_method == 'none'):\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(\"sym_method must be 'average' or 'union'\")\n",
    "    \n",
    "    kNN_graph = nx.from_numpy_array(adj_mat)\n",
    "\n",
    "    \n",
    "    return adj_mat,kNN_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build Aggregated kNN\n",
    "\n",
    "# Reverse the dictionary\n",
    "DGIDB_index_to_gene = {index: gene for gene, index in DGIDB_gene_to_index.items()}\n",
    "\n",
    "\n",
    "### Prerequisite: interlayer_transition_matrix, num_genes_dgidb, num_genes_msigdb\n",
    "def build_aggregated_kNN(diffusion_dist_matrix,\n",
    "                         k,\n",
    "                         n1 = num_genes_dgidb,\n",
    "                         n2 = num_genes_msigdb,\n",
    "                         sym_method='average'):\n",
    "    w1,w2 = layer_weights_stationary(multilayer_transition_matrix, num_genes_dgidb)\n",
    "    \n",
    "    similarity_matrix = build_kNN(diffusion_dist_matrix,k)[0]\n",
    "    \n",
    "    DGIDB_sim_matrix = similarity_matrix[:n1, :n1]\n",
    "    MSIGDB_sim_matrix = similarity_matrix[n1:, n1:]\n",
    "\n",
    "    DGIDB_index_map = {}\n",
    "    gene_to_index_additional = {}\n",
    "    coupling_matrix = interlayer_transition_matrix.T\n",
    "    num_additional_rows = 0\n",
    "    index = 0\n",
    "    for row in coupling_matrix:\n",
    "        if(np.all(row == 0)):\n",
    "            DGIDB_index_map[index] = n2 + num_additional_rows \n",
    "            gene_to_index_additional[DGIDB_index_to_gene[index]] = n2 + num_additional_rows\n",
    "            num_additional_rows += 1\n",
    "        else:\n",
    "            l = np.nonzero(row)[0]\n",
    "            if (len(l) > 1):\n",
    "                print(\"ERROR\")\n",
    "            else:\n",
    "                msigdb_pos = l[0]\n",
    "                DGIDB_index_map[index] = msigdb_pos\n",
    "        index += 1\n",
    "\n",
    "    zero_block = sp.csr_matrix((num_additional_rows, num_additional_rows), dtype=MSIGDB_sim_matrix.dtype)\n",
    "    asm = sp.block_diag((MSIGDB_sim_matrix, zero_block), format=\"csr\")\n",
    "\n",
    "    # Build and save gene_to_index_distinct mapping\n",
    "    gene_to_index_distinct = MSIGDB_gene_to_index | gene_to_index_additional\n",
    "\n",
    "    save_path = OUTPUT_DIRECTORY + \"gene_to_index_distinct.json\"\n",
    "    if not os.path.exists(save_path):\n",
    "        with open(save_path, 'w') as pathway_file:\n",
    "            json.dump(gene_to_index_distinct, pathway_file, indent=4)\n",
    "        print(f\"Mappings saved to {save_path}\")  \n",
    "    ### Aggregation\n",
    "\n",
    "    # rows, cols = DGIDB_sim_matrix.nonzero()\n",
    "    # for i, j in zip(rows, cols):\n",
    "    #     msig_i,msig_j = DGIDB_index_map[i], DGIDB_index_map[j]\n",
    "    #     if (asm[msig_i,msig_j] == 0):\n",
    "    #         asm[msig_i, msig_j] = DGIDB_sim_matrix[i, j]\n",
    "    #     else:\n",
    "    #         dgidb_sim_val = DGIDB_sim_matrix[i, j]\n",
    "    #         msigdb_sim_val = asm[msig_i, msig_j]\n",
    "    #         asm[msig_i, msig_j] = (w1 * dgidb_sim_val + w2 * msigdb_sim_val)   \n",
    "\n",
    "    M = asm.tocsr()\n",
    "\n",
    "    # 1) Get COO to access row/col vectors\n",
    "    DGIDB = DGIDB_sim_matrix.tocoo(copy=False)\n",
    "    row = DGIDB.row.astype(np.int64, copy=False)\n",
    "    col = DGIDB.col.astype(np.int64, copy=False)\n",
    "\n",
    "    # 2) Turn the mapping into a 1-D integer array so we can do map_arr[row], map_arr[col]\n",
    "    N = DGIDB_sim_matrix.shape[0]\n",
    "\n",
    "    if isinstance(DGIDB_index_map, dict):\n",
    "        # Fill with -1 to detect missing keys\n",
    "        map_arr = np.full(N, -1, dtype=np.int64)\n",
    "        for k, v in DGIDB_index_map.items():\n",
    "            if 0 <= k < N:\n",
    "                map_arr[k] = int(v)\n",
    "        if (map_arr < 0).any():\n",
    "            missing = np.flatnonzero(map_arr < 0)\n",
    "            raise ValueError(f\"DGIDB_index_map is missing {missing.size} DGIDB indices; \"\n",
    "                            f\"first few missing: {missing[:5].tolist()}\")\n",
    "    else:\n",
    "        # list/ndarray case\n",
    "        map_arr = np.asarray(DGIDB_index_map, dtype=np.int64)\n",
    "        if map_arr.ndim != 1 or map_arr.shape[0] != N:\n",
    "            raise ValueError(f\"DGIDB_index_map must be length {N} 1-D; got shape {map_arr.shape}\")\n",
    "\n",
    "    # 3) Vectorized remap of DGIDB → MSIGDB indices\n",
    "    msig_r = map_arr[row]\n",
    "    msig_c = map_arr[col]\n",
    "\n",
    "    # 4) Build remapped DGIDB matrix D in MSIGDB space (CSR)\n",
    "    D = sp.csr_matrix((DGIDB.data, (msig_r, msig_c)), shape=M.shape)\n",
    "\n",
    "    # 5) Vectorized aggregation (exactly matches your if/else logic)\n",
    "    Mb = M.copy(); Mb.data[:] = 1   # support(M)\n",
    "    Db = D.copy(); Db.data[:] = 1   # support(D)\n",
    "\n",
    "    M_keep = M - M.multiply(Db)                             # keep M where D==0\n",
    "    \n",
    "    D_only = D - D.multiply(Mb) # take D where M==0\n",
    "    \n",
    "    overlap = w1 * D.multiply(Mb) + w2 * M.multiply(Db)     # where both nonzero\n",
    "\n",
    "    aggregated = (M_keep + D_only + overlap).tocsr()\n",
    "    aggregated.eliminate_zeros()\n",
    "\n",
    "    # Result\n",
    "    asm = aggregated\n",
    "\n",
    "    if (sym_method not in ['average','union','none']):\n",
    "        raise ValueError(\"sym_method must be either 'average' or 'union'\")\n",
    "    elif (sym_method == 'none'):\n",
    "        # No symmetrization\n",
    "        pass\n",
    "    elif (sym_method == 'average'):\n",
    "        # Symmetrize the matrix by adding edges to one way edges and set weight to average\n",
    "        asm = (asm + asm.T).multiply(0.5).tocsr()\n",
    "    elif (sym_method == 'union'):\n",
    "        # Symmetrize the matrix by unioning\n",
    "        asm = asm.maximum(asm.T)\n",
    "\n",
    "    graph = nx.from_scipy_sparse_array(asm)\n",
    "    \n",
    "    return asm, graph\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddm = build_diffusion_dist_matrix(eigenvalues, eigenvectors, [2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn_orig,_ = build_kNN(ddm,num_neighbors)\n",
    "# knn_new,_,knn_sus = build_aggregated_kNN(ddm,num_neighbors)\n",
    "# csr_equal_tol(knn_orig,knn_sus)\n",
    "# print(is_symmetric(knn_orig),is_symmetric(knn_new))\n",
    "# print(knn_orig)\n",
    "# print(knn_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(csr_diff_metrics(knn_orig, knn_new,use_top_left_overlap=True))\n",
    "# print(knn_orig.shape, knn_new.shape)\n",
    "# print(csr_diff_metrics(knn_orig[num_genes_dgidb:, num_genes_dgidb:], knn_new,use_top_left_overlap=True))\n",
    "# print(knn_orig[num_genes_dgidb:, num_genes_dgidb:].shape, knn_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# density_orig = knn_orig.nnz / (knn_orig.shape[0] * knn_orig.shape[1])\n",
    "# print(density_orig)\n",
    "# density_new = knn_new.nnz / (knn_new.shape[0] * knn_new.shape[1])\n",
    "# print(density_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(knn_orig.nnz, knn_new.nnz)\n",
    "# print(knn_orig.data.mean(), knn_new.data.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del ddm, knn_orig, knn_sus, knn_new\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "# Clustering Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Louvain\n",
    "# def louvain_from_adj(A, resolution=1.0, random_state=0, keep_lcc=False):\n",
    "#     \"\"\"\n",
    "#     A: symmetric, non-negative scipy.sparse adjacency (CSR preferred).\n",
    "#     Returns: labels (np.ndarray of length n), graph G (NetworkX), node_order\n",
    "#     \"\"\"\n",
    "#     # if not sp.isspmatrix(A):  # allow dense but convert\n",
    "#     #     A = sp.csr_matrix(A)\n",
    "#     # # Optional: ensure symmetry numerically\n",
    "#     # if (A - A.T).nnz != 0:\n",
    "#     #     raise ValueError(\"Adjacency must be symmetric. Symmetrize first.\")\n",
    "\n",
    "#     # Build graph\n",
    "#     # networkx >=3.0: from_scipy_sparse_array; older: from_scipy_sparse_matrix\n",
    "#     G = nx.from_scipy_sparse_array(A, edge_attribute='weight')  # undirected by default\n",
    "\n",
    "#     if keep_lcc:\n",
    "#         # keep only the largest connected component if you prefer\n",
    "#         largest_cc = max(nx.connected_components(G), key=len)\n",
    "#         G = G.subgraph(largest_cc).copy()\n",
    "\n",
    "#     # Run Louvain\n",
    "#     part = community_louvain.best_partition(\n",
    "#         G, weight='weight', resolution=resolution, random_state=random_state\n",
    "#     )\n",
    "#     node_order = sorted(G.nodes())\n",
    "#     labels = np.array([part[i] for i in node_order], dtype=int)\n",
    "#     return labels, G, node_order\n",
    "\n",
    "# def DDBC(MTM,num_eigenvalues,num_neighbors,resolution,T):\n",
    "#     vals,vecs = top_k_eigenvalues(M = MTM, k = num_eigenvalues)\n",
    "\n",
    "#     diffusion_dist_matrix = np.zeros((num_genes,num_genes))\n",
    "#     for t in T:\n",
    "#         np.add(diffusion_dist_matrix, \n",
    "#                build_diffusion_dist_matrix(vals,vecs,t,num_eigenvalues), \n",
    "#                out=diffusion_dist_matrix)\n",
    "#     diffusion_dist_matrix = diffusion_dist_matrix / len(T)\n",
    "    \n",
    "#     kNN_adjacency_matrix = build_kNN(diffusion_dist_matrix,num_neighbors)\n",
    "\n",
    "#     # # Symmetrize the matrix by adding edges to one way edges and set weight to average\n",
    "#     # kNN_adjacency_matrix = (kNN_adjacency_matrix + kNN_adjacency_matrix.T).multiply(0.5).tocsr()\n",
    "\n",
    "#     # Symmetrize the matrix by unioning\n",
    "#     kNN_adjacency_matrix = kNN_adjacency_matrix.maximum(kNN_adjacency_matrix.T)\n",
    "\n",
    "#     return louvain_from_adj(kNN_adjacency_matrix, resolution = resolution, keep_lcc = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leiden_from_knn_adjacency(\n",
    "    A,\n",
    "    *,\n",
    "    method=\"modularity\",      # \"modularity\" | \"rb\" | \"cpm\"\n",
    "    resolution=1.0,           # used by \"rb\" and \"cpm\"\n",
    "    n_iterations=-1,          # -1 => until no improvement\n",
    "    seed=42,\n",
    "    use_weights=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Run Leiden on a symmetric, undirected (weighted) kNN adjacency (SciPy sparse).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    labels : np.ndarray[int]\n",
    "        Community ID per node (0..k-1)\n",
    "    quality : float\n",
    "        Objective value (modularity / RB / CPM depending on 'method')\n",
    "    \"\"\"\n",
    "    if not sp.issparse(A):\n",
    "        raise TypeError(\"A must be a SciPy sparse matrix.\")\n",
    "    # Normalize format & dtype\n",
    "    A = A.tocsr().astype(np.float32, copy=False)\n",
    "\n",
    "    # Clean diagonal and robustly symmetrize\n",
    "    A.setdiag(0.0)\n",
    "    A.eliminate_zeros()\n",
    "    A = A.maximum(A.T)  # keep max weight per undirected edge\n",
    "\n",
    "    # Build igraph from upper triangle (each undirected edge once)\n",
    "    U = sp.triu(A, k=1, format=\"coo\")\n",
    "    n = A.shape[0]\n",
    "    g = ig.Graph(n=n, edges=list(zip(U.row.tolist(), U.col.tolist())), directed=False)\n",
    "    wname = None\n",
    "    if use_weights:\n",
    "        g.es[\"weight\"] = U.data.tolist()\n",
    "        wname = \"weight\"\n",
    "\n",
    "    # Pick partition class + kwargs\n",
    "    m = method.lower()\n",
    "    if m == \"modularity\":\n",
    "        part_cls = la.ModularityVertexPartition\n",
    "        kwargs = dict(weights=wname)\n",
    "    elif m == \"rb\":\n",
    "        part_cls = la.RBConfigurationVertexPartition\n",
    "        kwargs = dict(weights=wname, resolution_parameter=resolution)\n",
    "    elif m == \"cpm\":\n",
    "        part_cls = la.CPMVertexPartition\n",
    "        kwargs = dict(weights=wname, resolution_parameter=resolution)\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'modularity', 'rb', or 'cpm'\")\n",
    "\n",
    "    # Run Leiden\n",
    "    part = la.find_partition(\n",
    "        g,\n",
    "        part_cls,\n",
    "        n_iterations=n_iterations,\n",
    "        seed=seed,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    labels = np.array(part.membership, dtype=np.int32)\n",
    "    communities = [list(c) for c in part]\n",
    "    return labels, float(part.quality()), communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def leiden_clustering(G,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DDBC(vals,\n",
    "         vecs,\n",
    "         num_neighbors,\n",
    "         resolution,\n",
    "         T,\n",
    "         leiden_method = \"modularity\",\n",
    "         ddm = None):\n",
    "    if ddm is None:\n",
    "        diffusion_dist_matrix = build_diffusion_dist_matrix_avg(vals, vecs,T)\n",
    "        np.save(f\"../output/{DISEASE}/diffusion_dist_matrices/ddm_{T}_res-{resolution}_itp-{interlayer_transition_prob}.npy\",diffusion_dist_matrix)\n",
    "    else:\n",
    "        diffusion_dist_matrix = ddm\n",
    "    # kNN_adjacency_matrix, kNN_graph = build_kNN(diffusion_dist_matrix,num_neighbors)\n",
    "    kNN_adjacency_matrix, kNN_graph = build_aggregated_kNN(diffusion_dist_matrix,num_neighbors)\n",
    "\n",
    "    return (*leiden_from_knn_adjacency(kNN_adjacency_matrix,method=leiden_method,resolution=resolution,n_iterations=-1,seed=42), kNN_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def community_report_onepage(labels, score, out_pdf=\"leiden_community_report.pdf\", *,\n",
    "                             extra_text=None, bins=\"auto\", title=\"Community Sizes\", time_steps = \"N/A\"):\n",
    "    \"\"\"\n",
    "    Create ONE PDF page (top: text summary, bottom: histogram).\n",
    "    If out_pdf exists: append this page. Else: create it.\n",
    "\n",
    "    Requires: matplotlib, pypdf  (pip install pypdf)\n",
    "    \"\"\"\n",
    "    # ---- inputs ----\n",
    "    labels = np.asarray(labels)\n",
    "    if labels.ndim != 1:\n",
    "        raise ValueError(\"labels must be a 1-D array of community ids\")\n",
    "\n",
    "    # ---- stats ----\n",
    "    sizes = np.bincount(labels.astype(np.int64, copy=False))\n",
    "    sizes_sorted = np.sort(sizes)\n",
    "    n, k = sizes.sum(), sizes.size\n",
    "\n",
    "    lines = [\n",
    "        \"Leiden Partition Summary\",\n",
    "        \"========================\",\n",
    "        f\"Nodes (n):           {n:,}\",\n",
    "        f\"Time steps:          {time_steps}\",\n",
    "        f\"Communities (k):     {k:,}\",\n",
    "        f\"Size (min):          {int(sizes_sorted[0]) if k else 0:,}\",\n",
    "        f\"Size (median):       {float(np.median(sizes_sorted)) if k else 0.0:.3f}\",\n",
    "        f\"Size (mean):         {float(sizes_sorted.mean()) if k else 0.0:.6f}\",\n",
    "        f\"Size (max):          {int(sizes_sorted[-1]) if k else 0:,}\",\n",
    "        f\"Score:               {score}\",\n",
    "        \"\",\n",
    "        \"Top 10 largest communities (id: size):\",\n",
    "    ]\n",
    "    for cid in np.argsort(-sizes)[:min(10, k)]:\n",
    "        lines.append(f\"  {int(cid):5d}: {int(sizes[cid]):,}\")\n",
    "    if extra_text:\n",
    "        lines += [\"\", \"Extra:\", *([extra_text] if isinstance(extra_text, str) else list(extra_text))]\n",
    "    summary_text = \"\\n\".join(lines)\n",
    "\n",
    "    # ---- draw single-page figure ----\n",
    "    fig = plt.figure(figsize=(8.5, 11), dpi=150)          # US Letter, higher DPI\n",
    "    ax_text = fig.add_axes([0.06, 0.55, 0.88, 0.40])      # [left, bottom, width, height]\n",
    "    ax_text.axis(\"off\")\n",
    "    ax_text.text(0.0, 1.0, summary_text, va=\"top\", ha=\"left\", fontsize=11, family=\"monospace\")\n",
    "\n",
    "    ax_hist = fig.add_axes([0.10, 0.08, 0.80, 0.38])\n",
    "    ax_hist.hist(sizes, bins=bins)\n",
    "    ax_hist.set_xlabel(\"Community size\")\n",
    "    ax_hist.set_ylabel(\"Count of communities\")\n",
    "    ax_hist.set_title(f\"Distribution of {title}\")\n",
    "\n",
    "    # ---- write this page to a temp PDF on disk ----\n",
    "    with NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmpf:\n",
    "        tmp_page = Path(tmpf.name)\n",
    "    with PdfPages(tmp_page) as pdf:\n",
    "        pdf.savefig(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # ---- append/create using PdfReader/PdfWriter (robust across versions) ----\n",
    "    try:\n",
    "        from pypdf import PdfReader, PdfWriter\n",
    "    except Exception as e:\n",
    "        try: os.remove(tmp_page)\n",
    "        except: pass\n",
    "        raise RuntimeError(\"Please install 'pypdf' (e.g., `pip install pypdf`).\") from e\n",
    "\n",
    "    out_pdf = Path(out_pdf)\n",
    "    tmp_out = out_pdf.with_suffix(out_pdf.suffix + \".tmp\")\n",
    "\n",
    "    writer = PdfWriter()\n",
    "\n",
    "    # if existing, copy old pages first\n",
    "    if out_pdf.exists():\n",
    "        with open(out_pdf, \"rb\") as f_exist:\n",
    "            reader = PdfReader(f_exist)\n",
    "            for p in reader.pages:\n",
    "                writer.add_page(p)\n",
    "\n",
    "    # add the new single page\n",
    "    with open(tmp_page, \"rb\") as f_new:\n",
    "        reader_new = PdfReader(f_new)\n",
    "        for p in reader_new.pages:\n",
    "            writer.add_page(p)\n",
    "\n",
    "    # atomic write\n",
    "    with open(tmp_out, \"wb\") as f_out:\n",
    "        writer.write(f_out)\n",
    "    os.replace(tmp_out, out_pdf)\n",
    "\n",
    "    # cleanup\n",
    "    try: os.remove(tmp_page)\n",
    "    except: pass\n",
    "\n",
    "    return out_pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "# Run DDBC 1 - 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leiden Clustering\n",
    "pdf_path = f\"../output/{DISEASE}/leiden_report.pdf\"\n",
    "method = \"modularity\"\n",
    "resolution = 1.0\n",
    "num_bins = 100\n",
    "write = True\n",
    "queue = [[i] for i in range(1,25)]\n",
    "label_list = []\n",
    "score_list = []\n",
    "graph_list = []\n",
    "communities_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load variables:\n",
    "# DATA_DIRECTORY = OUTPUT_DIRECTORY + \"leiden_result_variables_temp\"\n",
    "# with open(f\"{DATA_DIRECTORY}/label.pkl\", \"rb\") as f:\n",
    "#     label_list = pickle.load(f)\n",
    "# with open(f\"{DATA_DIRECTORY}/score.pkl\", \"rb\") as f:\n",
    "#     score_list = pickle.load(f)\n",
    "# with open(f\"{DATA_DIRECTORY}/graph.pkl\", \"rb\") as f:\n",
    "#     graph_list = pickle.load(f)\n",
    "# with open(f\"{DATA_DIRECTORY}/communities.pkl\", \"rb\") as f:\n",
    "#     communities_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "for T in queue:\n",
    "    ddm = np.load(f\"../output/{DISEASE}/diffusion_dist_matrices/ddm_{T}_res-{resolution}.npy\")\n",
    "    labels, score, communities, graph = DDBC(eigenvalues, eigenvectors, num_neighbors, resolution, T, leiden_method = method, ddm = ddm)\n",
    "    if (write):\n",
    "        path = community_report_onepage(labels, score, out_pdf=pdf_path,\n",
    "                                        extra_text=[f\"method={method}\", f\"resolution={resolution}\"], \n",
    "                                        bins=num_bins,time_steps = T)\n",
    "        print(\"Wrote:\", path)\n",
    "    label_list.append(labels)\n",
    "    score_list.append(score) \n",
    "    graph_list.append(graph)\n",
    "    communities_list.append(communities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save resulting variables to save time\n",
    "# DATA_DIRECTORY = OUTPUT_DIRECTORY + \"leiden_result_variables_temp\"\n",
    "# with open(f\"{DATA_DIRECTORY}/label.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(label_list, f)\n",
    "# with open(f\"{DATA_DIRECTORY}/score.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(score_list, f)\n",
    "# with open(f\"{DATA_DIRECTORY}/graph.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(graph_list, f)\n",
    "# with open(f\"{DATA_DIRECTORY}/communities.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(communities_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def community_central_genes(G, community_nodes, weight=\"weight\", top_n=20):\n",
    "    C = set(community_nodes)\n",
    "    H = G.subgraph(C).copy()                       # induced subgraph\n",
    "    # within-community (weighted) degree\n",
    "    k = {u: H.degree(u, weight=weight) for u in H}\n",
    "    ks = np.array(list(k.values()), dtype=float)\n",
    "    mu, sigma = ks.mean(), ks.std() if ks.std() > 0 else 1.0\n",
    "    Z = {u: (k[u] - mu)/sigma for u in H}          # within-module degree z-score\n",
    "\n",
    "    # rank by z\n",
    "    ranked = sorted(H.nodes(), key=lambda u: (Z[u]), reverse=True)\n",
    "    return {u : Z[u] for u in ranked[:top_n]}\n",
    "\n",
    "def weighted_jaccard(scoresA, scoresB):\n",
    "    \"\"\"\n",
    "    Compute Weighted Jaccard similarity between two communities\n",
    "    based on gene importance scores.\n",
    "    \n",
    "    Parameters:\n",
    "        scoresA, scoresB : dict\n",
    "            {gene: importance_score}\n",
    "            Scores can be any nonnegative values (e.g., PageRank, Z-score).\n",
    "    Returns:\n",
    "        float\n",
    "            Weighted Jaccard similarity in [0, 1].\n",
    "    \"\"\"\n",
    "    genes = set(scoresA) | set(scoresB)\n",
    "    if not genes:\n",
    "        return 0.0\n",
    "    num = sum(min(scoresA.get(g, 0.0), scoresB.get(g, 0.0)) for g in genes)\n",
    "    den = sum(max(scoresA.get(g, 0.0), scoresB.get(g, 0.0)) for g in genes)\n",
    "    return num / den if den > 0 else 0.0\n",
    "\n",
    "def weighted_overlap_coefficient(dictA, dictB):\n",
    "    \"\"\"\n",
    "    Weighted Szymkiewicz–Simpson (Overlap) coefficient ∈ [0,1].\n",
    "    \"\"\"\n",
    "    if not dictA or not dictB:\n",
    "        return 0.0\n",
    "\n",
    "    common = set(dictA) & set(dictB)\n",
    "    inter_sum = sum(min(dictA[v], dictB[v]) for v in common)\n",
    "\n",
    "    sumA = sum(max(0, w) for w in dictA.values())\n",
    "    sumB = sum(max(0, w) for w in dictB.values())\n",
    "    denom = min(sumA, sumB)\n",
    "\n",
    "    return inter_sum / denom if denom > 0 else 0.0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wjacc_edges_builder(g_list,c_list,score_cap = 0,com_size_cap = 0):\n",
    "    result = []\n",
    "    for i in range(len(g_list)-1):\n",
    "        prev_G = g_list[i]\n",
    "        curr_G = g_list[i+1]\n",
    "        prev_com = c_list[i]\n",
    "        curr_com = c_list[i+1]\n",
    "        prev_z = []\n",
    "        curr_z = []\n",
    "        \n",
    "        for com in prev_com:\n",
    "            if (len(com) > com_size_cap):\n",
    "                prev_z.append(community_central_genes(prev_G,com))\n",
    "        for com in curr_com:\n",
    "            if (len(com) > com_size_cap):\n",
    "                curr_z.append(community_central_genes(curr_G,com))\n",
    "\n",
    "        for j in range(len(prev_z)):\n",
    "            for k in range(len(curr_z)):\n",
    "                jaccard_score = weighted_jaccard(prev_z[j],curr_z[k])\n",
    "                if (jaccard_score >= score_cap):\n",
    "                    result.append((queue[i][0],j,k,jaccard_score))\n",
    "    \n",
    "    return result\n",
    "\n",
    "def community_sizes(labels):\n",
    "    \"\"\"Return dict[label] -> number of nodes in that community.\"\"\"\n",
    "    lab = np.asarray(labels)\n",
    "    uniq, cnt = np.unique(lab, return_counts=True)\n",
    "    return {int(c): int(n) for c, n in zip(uniq, cnt)}\n",
    "\n",
    "def topk_at_t1(labels_t1, k=10):\n",
    "    \"\"\"Return list of top-k community labels at t=1 by size.\"\"\"\n",
    "    sizes = community_sizes(labels_t1)\n",
    "    return [c for c, _ in sorted(sizes.items(), key=lambda x: x[1], reverse=True)[:k]], sizes\n",
    "\n",
    "# ---------- Jaccard lookup ----------\n",
    "\n",
    "def build_edge_lookup(wjacc_edges):\n",
    "    \"\"\"\n",
    "    Build a dict mapping (t, comm_t) -> list of (comm_t+1, jaccard_score).\n",
    "    Each wjacc_edges element = (t, comm_t, comm_tplus1, jaccard_score)\n",
    "    \"\"\"\n",
    "    out = defaultdict(list)\n",
    "    for t, c1, c2, s in wjacc_edges:\n",
    "        out[(int(t), int(c1))].append((int(c2), float(s)))\n",
    "    return out\n",
    "\n",
    "# ---------- Tracking ----------\n",
    "\n",
    "def track_paths(seeds, wj_lookup, ts):\n",
    "    \"\"\"Track each seed community forward by max Jaccard each step.\"\"\"\n",
    "    print(ts)\n",
    "    t0 = ts[0]\n",
    "    paths = {s: [(t0, s)] for s in seeds}\n",
    "    edges = []\n",
    "    for s in seeds:\n",
    "        cur = s\n",
    "        for i in range(len(ts) - 1):\n",
    "            t = ts[i]\n",
    "            tp1 = ts[i+1]\n",
    "            cand = wj_lookup.get((t, cur), [])\n",
    "            if not cand:\n",
    "                print(f\"Seed {s}, {t} to {tp1} has no outgoing edges. Termniated.\")\n",
    "                paths[s].append((tp1, None))\n",
    "                cur = None\n",
    "                break\n",
    "            nxt, score = max(cand, key=lambda x: x[1])\n",
    "            edges.append(((t, cur), (tp1, nxt), score))\n",
    "            paths[s].append((tp1, nxt))\n",
    "            cur = nxt\n",
    "    return paths, edges\n",
    "\n",
    "# MY VERSION\n",
    "# def track_paths(labels_by_t, seeds, wj_lookup, ts,score_cap = 0.1):\n",
    "#     \"\"\"Track each seed community forward by max Jaccard each step.\"\"\"\n",
    "#     t0 = ts[0]\n",
    "#     paths = {s: [(t0, s)] for s in seeds}\n",
    "#     edges = []\n",
    "#     curs = []\n",
    "#     curs_next = []\n",
    "#     for s in seeds:\n",
    "#         curs_next.append(s)\n",
    "#         for i in range(len(ts) - 1):\n",
    "#             curs = curs_next\n",
    "#             curs_next = []\n",
    "#             for cur in curs:\n",
    "#                 t = ts[i]\n",
    "#                 tp1 = ts[i+1]\n",
    "#                 cand = wj_lookup.get((t, cur), [])\n",
    "#                 if not cand:\n",
    "#                     if (t == t0):\n",
    "#                         print(f\"Community {cur} has no outgoing edges\")\n",
    "#                     paths[s].append((tp1, None))\n",
    "#                     cur = None\n",
    "#                     continue\n",
    "                \n",
    "                \n",
    "#                 for nxt,score in cand:\n",
    "#                     if (score >= score_cap):\n",
    "#                         edges.append(((t, cur), (tp1, nxt), score))\n",
    "#                         paths[s].append((tp1, nxt))\n",
    "#                         curs_next.append(nxt)\n",
    "\n",
    "#     return paths, edges\n",
    "\n",
    "# ---------- Get node sizes ----------\n",
    "\n",
    "def node_sizes_for_paths(labels_by_t, paths):\n",
    "    \"\"\"Return dict[(t, comm)] -> size (number of vertices).\"\"\"\n",
    "    out = {}\n",
    "    for s, seq in paths.items():\n",
    "        for t, c in seq:\n",
    "            if c is None: continue\n",
    "            sizes = community_sizes(labels_by_t[t])\n",
    "            out[(t, c)] = sizes.get(c, 0)\n",
    "    return out\n",
    "\n",
    "# ---------- Draw layered network ----------\n",
    "\n",
    "def draw_layered_paths(paths, edge_list, node_sizes, ts,\n",
    "                       node_size_scale=700.0, edge_width_scale=8.0,\n",
    "                       seed_gap=1.0, col_gap=3.0, score_cap = 0.5, title=None, save_path = None):\n",
    "    seeds = list(paths.keys())\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Add nodes\n",
    "    for s in seeds:\n",
    "        for t, c in paths[s]:\n",
    "            if c is None: continue\n",
    "            G.add_node((s, t, c), seed=s, t=t, comm=c, size=node_sizes.get((t, c), 0))\n",
    "    # Add edges\n",
    "    # Set your inclusion threshold (cap)\n",
    "    # include all edges with Jaccard >= 0.5\n",
    "\n",
    "    for (t, c1), (tp, c2), w in edge_list:\n",
    "        if w < score_cap:\n",
    "            continue\n",
    "        for s in seeds:\n",
    "            seq = paths[s]\n",
    "            for i in range(len(seq) - 1):\n",
    "                if seq[i] == (t, c1) and seq[i+1] == (tp, c2):\n",
    "                    G.add_edge((s, t, c1), (s, tp, c2), jacc=w)\n",
    "\n",
    "    # Layout positions\n",
    "    pos = {}\n",
    "    nodes_by_t = defaultdict(list)\n",
    "    for n in G.nodes:\n",
    "        t = G.nodes[n]['t']\n",
    "        nodes_by_t[t].append(n)\n",
    "\n",
    "    # Sort nodes in each column by community size (largest first)\n",
    "    pos = {}\n",
    "    for t in ts:\n",
    "        column_nodes = nodes_by_t.get(t, [])\n",
    "        # sort by size descending\n",
    "        column_nodes.sort(key=lambda n: G.nodes[n].get('size', G.nodes[n].get('size_w', 0)), reverse=True)\n",
    "        for i, n in enumerate(column_nodes):\n",
    "            pos[n] = (ts.index(t) * col_gap, -i * seed_gap)\n",
    "\n",
    "    # Scale sizes and widths\n",
    "    if len(G) == 0:\n",
    "        raise ValueError(\"No nodes to draw — check your data.\")\n",
    "    vals = np.array([G.nodes[n]['size'] for n in G.nodes], dtype=float)\n",
    "    vmin, vmax = vals.min(), vals.max()\n",
    "    sizes = []\n",
    "    for n in G.nodes:\n",
    "        w = G.nodes[n]['size']\n",
    "        a = (w - vmin) / (vmax - vmin) if vmax > vmin else 1.0\n",
    "        sizes.append((0.3 + 0.7*a) * node_size_scale)\n",
    "    widths = [max(0.5, d['jacc'] * edge_width_scale) for _, _, d in G.edges(data=True)]\n",
    "\n",
    "    # Draw\n",
    "    plt.figure(figsize=(max(10, len(ts)*1.4), max(6, len(seeds)*0.55 + 2)))\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=sizes)\n",
    "    nx.draw_networkx_edges(G, pos, width=widths, arrows=False)\n",
    "\n",
    "    edge_labels = {(u, v): f\"{d['jacc']:.2f}\" for u, v, d in G.edges(data=True)}\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8)\n",
    "    \n",
    "    plt.xticks([i * col_gap for i in range(len(ts))], [str(t) for t in ts])\n",
    "    plt.yticks([])\n",
    "    plt.title(title or f\"Top {len(seeds)} communities from t={ts[0]} → t={ts[-1]} (edge ∝ Jaccard)\")\n",
    "    plt.tight_layout()\n",
    "    if (save_path != None):\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "# ---------- Main wrapper ----------\n",
    "\n",
    "def visualize_topk_from_edges(labels_by_t, wjacc_edges,\n",
    "                              t_start=1, t_end=16, top_k=10,\n",
    "                              node_size_scale=700.0, score_cap = 0.5,\n",
    "                              edge_width_scale=8.0, title=None, save_path = None):\n",
    "    ts = sorted(labels_by_t.keys())\n",
    "    if t_start not in labels_by_t:\n",
    "        raise ValueError(f\"Missing labels for t={t_start}\")\n",
    "\n",
    "    # Top-k seeds from t_start\n",
    "    seeds, _ = topk_at_t1(labels_by_t[t_start], k=top_k)\n",
    "\n",
    "    # Track forward using provided Jaccard edges\n",
    "    wj_lookup = build_edge_lookup(wjacc_edges)\n",
    "    paths, edges = track_paths(seeds, wj_lookup, ts)\n",
    "\n",
    "    # Compute community sizes (unweighted)\n",
    "    node_sizes = node_sizes_for_paths(labels_by_t, paths)\n",
    "\n",
    "    # Draw the layered visualization\n",
    "    print(ts)\n",
    "    draw_layered_paths(paths, edges, node_sizes, ts,\n",
    "                       node_size_scale=node_size_scale,\n",
    "                       edge_width_scale=edge_width_scale,\n",
    "                       title=title or f\"Top-{top_k} from t={t_start} → t={t_end}\", score_cap = score_cap,\n",
    "                       save_path = save_path)\n",
    "\n",
    "    return paths, edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load variables:\n",
    "# DATA_DIRECTORY = OUTPUT_DIRECTORY + \"leiden_result_variables_temp\"\n",
    "# with open(f\"{DATA_DIRECTORY}/label.pkl\", \"rb\") as f:\n",
    "#     label_list = pickle.load(f)\n",
    "# with open(f\"{DATA_DIRECTORY}/score.pkl\", \"rb\") as f:\n",
    "#     score_list = pickle.load(f)\n",
    "# with open(f\"{DATA_DIRECTORY}/graph.pkl\", \"rb\") as f:\n",
    "#     graph_list = pickle.load(f)\n",
    "# with open(f\"{DATA_DIRECTORY}/communities.pkl\", \"rb\") as f:\n",
    "#     communities_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(graph_list))\n",
    "print(len(communities_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = {queue[i][0]: label_list[i] for i in range(len(queue))}\n",
    "wjacc = wjacc_edges_builder(graph_list,communities_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels_dict)\n",
    "print(len(labels_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wjacc)\n",
    "print(len(wjacc))\n",
    "save_path = f\"../output/{DISEASE}/leiden_tracking_results/leiden_tracking.png\"\n",
    "# save_path = None\n",
    "_, edges = visualize_topk_from_edges(labels_dict, wjacc, 1,24,top_k = 20,score_cap = 0.03,save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = [[i] for i in range(1,25)]\n",
    "x = [T[0] for T in queue]\n",
    "y1 = score_list\n",
    "y2 = [len(communities_list[i]) for i in range(len(communities_list))]\n",
    "\n",
    "# Create a figure with 2 subplots (1 row, 2 columns)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Left graph\n",
    "axes[0].plot(x, y1, color='r')\n",
    "axes[0].set_title(\"quality score\")\n",
    "axes[0].set_xlabel(\"x\")\n",
    "axes[0].set_ylabel(\"quality_score\")\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Right graph\n",
    "axes[1].plot(x, y2, color='b')\n",
    "axes[1].set_title(\"community size\")\n",
    "axes[1].set_xlabel(\"x\")\n",
    "axes[1].set_ylabel(\"community size\")\n",
    "axes[1].grid(True)\n",
    "\n",
    "# Adjust spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_time = 1 / (1-eigenvalues[1])\n",
    "print(f\"Mixed Time: {mixed_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "# Final Average DDBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Average Clustering\n",
    "average_t = [2,4,6,8,10,12]\n",
    "pdf_path = f\"../output/{DISEASE}/leiden_report.pdf\"\n",
    "method = \"rb\"\n",
    "num_neighbors = 100\n",
    "resolution = 1.8\n",
    "label_list = []\n",
    "score_list = []\n",
    "graph_list = []\n",
    "communities_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(multilayer_transition_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddm = np.load(f\"../output/{DISEASE}/diffusion_dist_matrices/ddm_[2, 4, 6, 8, 10, 12]_res-1.5_itp-0.5.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, score, communities, graph = DDBC(eigenvalues, eigenvectors, num_neighbors, resolution, average_t, leiden_method = method, ddm = ddm)\n",
    "path = community_report_onepage(labels, score, out_pdf=pdf_path,\n",
    "                                extra_text=[f\"method={method}\", f\"resolution={resolution}\"], \n",
    "                                bins=100,time_steps = average_t)\n",
    "print(\"Wrote:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(communities)\n",
    "DATA_DIRECTORY = OUTPUT_DIRECTORY + \"leiden_results\"\n",
    "with open(f\"{DATA_DIRECTORY}/result_communities_new.pkl\", \"wb\") as f:\n",
    "    pickle.dump(communities, f)\n",
    "with open(f\"{DATA_DIRECTORY}/result_graph_new.pkl\", \"wb\") as f:\n",
    "    pickle.dump(graph, f)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "# Robustness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddm_test = build_diffusion_dist_matrix_avg(eigenvalues, eigenvectors,[2,4,6,8,10,12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddm_test = ddm_test.astype(np.float32, copy = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "del multilayer_transition_matrix\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix,_ = build_aggregated_kNN(ddm_test, k = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_original,_,_ = leiden_from_knn_adjacency(similarity_matrix, method=\"modularity\",resolution=1.0,n_iterations=-1,seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_boots = 50\n",
    "ari_scores = []\n",
    "\n",
    "for b in tqdm(range(n_boots)):\n",
    "    # 1. Sample genes with replacement\n",
    "    n = similarity_matrix.shape[0]\n",
    "    idx = np.random.choice(np.arange(n), size=n, replace=True)\n",
    "    sim_resampled = similarity_matrix[idx, :][:, idx]\n",
    "    \n",
    "    # 2. Recompute similarity and cluster\n",
    "    labels_b, _, _ = leiden_from_knn_adjacency(sim_resampled, method=\"modularity\",resolution=1.0,n_iterations=-1,seed=42)\n",
    "    \n",
    "    # 3. Compare with original clustering\n",
    "    ari = adjusted_rand_score(labels_original[idx], labels_b)\n",
    "    ari_scores.append(ari)\n",
    "\n",
    "print(\"Median ARI:\", np.median(ari_scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
