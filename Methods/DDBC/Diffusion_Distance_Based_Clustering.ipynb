{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "401e1e45",
   "metadata": {},
   "source": [
    "# Documentation\n",
    "* Resolution = 1.0\n",
    "* aggregated sim matrix with 0.5 itp\n",
    "* leiden clustering\n",
    "* final avg times [2,4,6,8,10,12]\n",
    "* seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e930760",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6b4cc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from scipy.sparse import load_npz,save_npz,diags,csr_matrix\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "import os\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from pypdf import PdfReader, PdfWriter\n",
    "from tempfile import NamedTemporaryFile\n",
    "import networkx as nx\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import community as community_louvain\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import gc\n",
    "from pympler import muppy, asizeof\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a2f0ed",
   "metadata": {},
   "source": [
    "# Building Multilayer Transition Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4778f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the matrices needed\n",
    "DISEASE = \"BIPOLAR\"\n",
    "OUTPUT_DIRECTORY = f\"../output/{DISEASE}/\"\n",
    "DGIDB_DIRECTORY = f\"../../Gen_Hypergraph/output/DGIDB_{DISEASE}/\"\n",
    "MSIGDB_DIRECTORY = \"../../Gen_Hypergraph/output/MSigDB_Full/\"\n",
    "\n",
    "## DGIDB\n",
    "DGIDB_binary_matrix = load_npz(DGIDB_DIRECTORY + \"hypergraph_incidence_matrix_binary.npz\")\n",
    "DGIDB_weighted_matrix = load_npz(DGIDB_DIRECTORY + \"hypergraph_incidence_matrix_weighted.npz\")\n",
    "DGIDB_gene_weight_diag_matrix = load_npz(DGIDB_DIRECTORY + \"gene_weight_diag_matrix.npz\")\n",
    "DGIDB_diag_node_degree_matrix = load_npz(DGIDB_DIRECTORY + \"diag_node_degree_matrix.npz\")\n",
    "DGIDB_inverse_diag_edge_degree_matrix = load_npz(\n",
    "    DGIDB_DIRECTORY + \"inverse_diag_edge_degree_matrix.npz\"\n",
    "    )\n",
    "\n",
    "## MSIGDB\n",
    "MSIGDB_binary_matrix = load_npz(MSIGDB_DIRECTORY + \"hypergraph_incidence_matrix_binary.npz\")\n",
    "MSIGDB_weighted_matrix = load_npz(MSIGDB_DIRECTORY + \"hypergraph_incidence_matrix_weighted.npz\")\n",
    "MSIGDB_gene_weight_diag_matrix = load_npz(MSIGDB_DIRECTORY + \"gene_weight_diag_matrix.npz\")\n",
    "MSIGDB_diag_node_degree_matrix = load_npz(MSIGDB_DIRECTORY + \"diag_node_degree_matrix.npz\")\n",
    "MSIGDB_inverse_diag_edge_degree_matrix = load_npz(\n",
    "    MSIGDB_DIRECTORY + \"inverse_diag_edge_degree_matrix.npz\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51bde8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# queue = [[i] for i in range(1,25)]\n",
    "# resolution = 1.0\n",
    "# for T in queue:\n",
    "#     path = f\"../output/{DISEASE}/diffusion_dist_matrices/ddm_{T}_res-{resolution}.npy\"\n",
    "#     arr = np.load(path)\n",
    "#     arr = arr.astype(np.float32)\n",
    "#     np.save(path, arr)\n",
    "#     print(f\"Overwritten (float32): {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfd6acb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful Functions\n",
    "def csr_equal_tol(A, B, atol=1e-8):\n",
    "    # First check shapes and sparsity pattern\n",
    "    if A.shape != B.shape or not np.array_equal(A.indptr, B.indptr) or not np.array_equal(A.indices, B.indices):\n",
    "        return False\n",
    "    # Compare numeric values within tolerance\n",
    "    return np.allclose(A.data, B.data, atol=atol, rtol=0)\n",
    "\n",
    "def is_symmetric(W,tol = 1e-8):\n",
    "    diff = (W - W.T)\n",
    "    check = np.all(np.abs(diff.data) < tol)\n",
    "    return check\n",
    "\n",
    "def degree_array(W, a=1):\n",
    "    return np.asarray(W.sum(axis=a)).ravel()\n",
    "\n",
    "def degree_diagonal_matrix(W, a=1):\n",
    "    d = degree_array(W,a)\n",
    "    return sp.diags(d, offsets=0, format='csr')\n",
    "\n",
    "def symmetrically_normalize(W, a=1):\n",
    "    D = np.asarray(W.sum(axis=a)).ravel()\n",
    "    D_inv_sqrt = np.zeros_like(D)\n",
    "    nze = D != 0\n",
    "    D_inv_sqrt[nze] = 1 / np.sqrt(D[nze])\n",
    "\n",
    "    W_sym = W.multiply(D_inv_sqrt)              # scale columns\n",
    "    W_sym = W_sym.multiply(D_inv_sqrt[:, None]) # scale rows    \n",
    "    return W_sym.tocsr()\n",
    "\n",
    "def big_objects(n=10, min_mb=1):\n",
    "    \"\"\"\n",
    "    Show the largest objects currently in memory.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int\n",
    "        Number of top objects to show.\n",
    "    min_mb : float\n",
    "        Minimum size (in MB) to include.\n",
    "    \"\"\"\n",
    "    import sys\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import scipy.sparse as sp\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    def get_size(obj):\n",
    "        try:\n",
    "            if isinstance(obj, np.ndarray):\n",
    "                return obj.nbytes\n",
    "            elif isinstance(obj, pd.DataFrame) or isinstance(obj, pd.Series):\n",
    "                return obj.memory_usage(deep=True).sum()\n",
    "            elif sp.issparse(obj):\n",
    "                return (obj.data.nbytes +\n",
    "                        obj.indptr.nbytes +\n",
    "                        obj.indices.nbytes)\n",
    "            else:\n",
    "                return sys.getsizeof(obj)\n",
    "        except Exception:\n",
    "            return 0\n",
    "\n",
    "    ip = get_ipython()\n",
    "    if ip is None:\n",
    "        ns = globals()\n",
    "    else:\n",
    "        ns = ip.user_ns\n",
    "\n",
    "    items = []\n",
    "    for name, val in ns.items():\n",
    "        if name.startswith('_'):\n",
    "            continue  # skip internals\n",
    "        size = get_size(val)\n",
    "        if size > min_mb * 1024 ** 2:\n",
    "            items.append((name, type(val).__name__, size))\n",
    "\n",
    "    items.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    print(f\"{'Variable':30s} {'Type':25s} {'Size (MB)':>10s}\")\n",
    "    print(\"-\" * 70)\n",
    "    for name, t, size in items[:n]:\n",
    "        print(f\"{name:30s} {t:25s} {size / 1024 ** 2:10.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd2b479c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Adjacency Matrices\n",
    "## DGIDB\n",
    "H,W_v,D_v,D_e_inv = DGIDB_weighted_matrix, DGIDB_gene_weight_diag_matrix, DGIDB_diag_node_degree_matrix, DGIDB_inverse_diag_edge_degree_matrix\n",
    "\n",
    "# Construct D_v^(-1/2)\n",
    "d = (D_v @ W_v).diagonal()\n",
    "d_inv_sqrt = np.zeros_like(d)\n",
    "nonzero_mask = d > 0\n",
    "d_inv_sqrt[nonzero_mask] = 1.0 / np.sqrt(d[nonzero_mask])\n",
    "D_v_sqrt_inv = diags(d_inv_sqrt)\n",
    "\n",
    "DGIDB_adjacency_matrix = D_v_sqrt_inv @ H @ D_e_inv @ H.T @ D_v_sqrt_inv\n",
    "\n",
    "\n",
    "## MSIGDB\n",
    "H,W_v,D_v,D_e_inv = MSIGDB_weighted_matrix, MSIGDB_gene_weight_diag_matrix, MSIGDB_diag_node_degree_matrix, MSIGDB_inverse_diag_edge_degree_matrix\n",
    "\n",
    "# Construct D_v^(-1/2)\n",
    "d = (D_v @ W_v).diagonal()\n",
    "d_inv_sqrt = np.zeros_like(d)\n",
    "nonzero_mask = d > 0\n",
    "d_inv_sqrt[nonzero_mask] = 1.0 / np.sqrt(d[nonzero_mask])\n",
    "D_v_sqrt_inv = diags(d_inv_sqrt)\n",
    "\n",
    "MSIGDB_adjacency_matrix = D_v_sqrt_inv @ H @ D_e_inv @ H.T @ D_v_sqrt_inv\n",
    "\n",
    "# Compute Degree Diagonal Matrices\n",
    "DGIDB_rows_sums = degree_array(DGIDB_adjacency_matrix)\n",
    "MSIGDB_rows_sums = degree_array(MSIGDB_adjacency_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "160caf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(is_symmetric(DGIDB_adjacency_matrix))\n",
    "print(is_symmetric(MSIGDB_adjacency_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75814a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Symmetric Normalization\n",
    "DGIDB_adjacency_matrix = symmetrically_normalize(DGIDB_adjacency_matrix)\n",
    "MSIGDB_adjacency_matrix = symmetrically_normalize(MSIGDB_adjacency_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adc7945f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(is_symmetric(DGIDB_adjacency_matrix))\n",
    "print(is_symmetric(MSIGDB_adjacency_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e19d107b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# density = DGIDB_adjacency_matrix.nnz / (DGIDB_adjacency_matrix.shape[0] * DGIDB_adjacency_matrix.shape[1])\n",
    "# print(\"DGIDB Density:\", density)\n",
    "# density = MSIGDB_adjacency_matrix.nnz / (MSIGDB_adjacency_matrix.shape[0] * MSIGDB_adjacency_matrix.shape[1])\n",
    "# print(\"MSIGDB Density:\", density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c664dcae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DGIDB nonzero average: 0.009289970824902828 \n",
      "MSIGDB nonzero average: 9.87564623806718e-05\n"
     ]
    }
   ],
   "source": [
    "# Check nonzero average\n",
    "DGIDB_nonzero_average = DGIDB_adjacency_matrix[DGIDB_adjacency_matrix != 0].mean()\n",
    "MSIGDB_nonzero_average = MSIGDB_adjacency_matrix[MSIGDB_adjacency_matrix != 0].mean()\n",
    "\n",
    "print(\"DGIDB nonzero average:\",DGIDB_nonzero_average,\"\\nMSIGDB nonzero average:\",MSIGDB_nonzero_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3b6b28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Density normalization version 1\n",
    "# target_average = MSIGDB_nonzero_average\n",
    "# DGIDB_adjacency_matrix = (target_average / DGIDB_nonzero_average) * DGIDB_adjacency_matrix\n",
    "# MSIGDB_adjacency_matrix = (target_average / MSIGDB_nonzero_average) * MSIGDB_adjacency_matrix\n",
    "\n",
    "# DGIDB_nonzero_average = DGIDB_adjacency_matrix[DGIDB_adjacency_matrix != 0].mean()\n",
    "# MSIGDB_nonzero_average = MSIGDB_adjacency_matrix[MSIGDB_adjacency_matrix != 0].mean()\n",
    "\n",
    "# print(DGIDB_nonzero_average,MSIGDB_nonzero_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21754911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5022932647447539 0.2379564756116163\n",
      "0.3214543182373284 0.6785456817626716\n",
      "0.46703473053286176\n"
     ]
    }
   ],
   "source": [
    "# Density normalization version 2\n",
    "DGIDB_mean_degree = DGIDB_rows_sums[DGIDB_rows_sums != 0].mean()\n",
    "MSIGDB_mean_degree = MSIGDB_rows_sums[MSIGDB_rows_sums != 0].mean()\n",
    "print(DGIDB_mean_degree,MSIGDB_mean_degree)\n",
    "# print(DGIDB_rows_sums,MSIGDB_rows_sums)\n",
    "\n",
    "DGIDB_weight = (1/DGIDB_mean_degree) / ((1/DGIDB_mean_degree)+(1/MSIGDB_mean_degree))\n",
    "MSIGDB_weight = (1/MSIGDB_mean_degree) / ((1/DGIDB_mean_degree)+(1/MSIGDB_mean_degree))\n",
    "geo_mean_weight = (DGIDB_weight * MSIGDB_weight)**(1/2)\n",
    "print(DGIDB_weight,MSIGDB_weight)\n",
    "print(geo_mean_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "da1e137f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gene 927 not found in MSIGDB mapping.\n",
      "Gene 11 not found in MSIGDB mapping.\n",
      "Gene 724 not found in MSIGDB mapping.\n",
      "Gene 360158 not found in MSIGDB mapping.\n",
      "Gene 469 not found in MSIGDB mapping.\n",
      "Gene 100529264 not found in MSIGDB mapping.\n",
      "Gene 2609 not found in MSIGDB mapping.\n",
      "Gene 447 not found in MSIGDB mapping.\n",
      "Gene 453 not found in MSIGDB mapping.\n",
      "Gene 459 not found in MSIGDB mapping.\n",
      "Gene 2616 not found in MSIGDB mapping.\n",
      "Gene 442 not found in MSIGDB mapping.\n",
      "Gene 600 not found in MSIGDB mapping.\n",
      "Gene 507 not found in MSIGDB mapping.\n",
      "Gene 62 not found in MSIGDB mapping.\n",
      "Gene 68 not found in MSIGDB mapping.\n",
      "Gene 2772 not found in MSIGDB mapping.\n",
      "Gene 485 not found in MSIGDB mapping.\n",
      "Gene 620 not found in MSIGDB mapping.\n",
      "Gene 121131 not found in MSIGDB mapping.\n",
      "Gene 285834 not found in MSIGDB mapping.\n",
      "Gene 6962 not found in MSIGDB mapping.\n",
      "Gene 1480 not found in MSIGDB mapping.\n",
      "Gene 2749 not found in MSIGDB mapping.\n",
      "Gene 1624 not found in MSIGDB mapping.\n",
      "Gene 63 not found in MSIGDB mapping.\n",
      "Gene 2970 not found in MSIGDB mapping.\n",
      "Gene 243 not found in MSIGDB mapping.\n",
      "Gene 455 not found in MSIGDB mapping.\n",
      "Gene 380 not found in MSIGDB mapping.\n",
      "Gene 543 not found in MSIGDB mapping.\n",
      "Gene 542 not found in MSIGDB mapping.\n",
      "Gene 1498 not found in MSIGDB mapping.\n",
      "Gene 484 not found in MSIGDB mapping.\n",
      "Gene 338099 not found in MSIGDB mapping.\n",
      "Gene 1623 not found in MSIGDB mapping.\n",
      "Gene 100422871 not found in MSIGDB mapping.\n",
      "Gene 74 not found in MSIGDB mapping.\n",
      "Gene 451 not found in MSIGDB mapping.\n",
      "Gene 450 not found in MSIGDB mapping.\n",
      "Gene 601 not found in MSIGDB mapping.\n",
      "Gene 2964 not found in MSIGDB mapping.\n",
      "Gene 403223 not found in MSIGDB mapping.\n",
      "Gene 2789 not found in MSIGDB mapping.\n",
      "Gene 544 not found in MSIGDB mapping.\n",
      "Gene 1817 not found in MSIGDB mapping.\n",
      "Gene 1818 not found in MSIGDB mapping.\n",
      "Gene 2282 not found in MSIGDB mapping.\n",
      "Gene 66 not found in MSIGDB mapping.\n",
      "Gene 1011 not found in MSIGDB mapping.\n",
      "Gene 73 not found in MSIGDB mapping.\n",
      "Gene 90632 not found in MSIGDB mapping.\n",
      "Gene 2667 not found in MSIGDB mapping.\n",
      "Gene 100302144 not found in MSIGDB mapping.\n",
      "Gene 263 not found in MSIGDB mapping.\n",
      "Gene 100302251 not found in MSIGDB mapping.\n",
      "Gene 1596 not found in MSIGDB mapping.\n",
      "Gene 1424 not found in MSIGDB mapping.\n",
      "Gene 418 not found in MSIGDB mapping.\n",
      "Gene 2751 not found in MSIGDB mapping.\n",
      "Gene 504 not found in MSIGDB mapping.\n",
      "Gene 100379251 not found in MSIGDB mapping.\n",
      "Gene 1987 not found in MSIGDB mapping.\n",
      "Gene 644314 not found in MSIGDB mapping.\n",
      "Gene 935 not found in MSIGDB mapping.\n",
      "Gene 281 not found in MSIGDB mapping.\n",
      "Gene 2716 not found in MSIGDB mapping.\n",
      "Gene 100422999 not found in MSIGDB mapping.\n",
      "Gene 728734 not found in MSIGDB mapping.\n",
      "Gene 1348 not found in MSIGDB mapping.\n",
      "Gene 2284 not found in MSIGDB mapping.\n",
      "Gene 1330 not found in MSIGDB mapping.\n",
      "Gene 1934 not found in MSIGDB mapping.\n",
      "Gene 1980 not found in MSIGDB mapping.\n",
      "Gene 1332 not found in MSIGDB mapping.\n",
      "Gene 1517 not found in MSIGDB mapping.\n",
      "Gene 79529 not found in MSIGDB mapping.\n",
      "Gene 1872 not found in MSIGDB mapping.\n",
      "Gene 100422959 not found in MSIGDB mapping.\n",
      "Gene 1578 not found in MSIGDB mapping.\n",
      "Gene 2285 not found in MSIGDB mapping.\n",
      "Gene 254948 not found in MSIGDB mapping.\n",
      "Gene 100616320 not found in MSIGDB mapping.\n",
      "Gene 2025 not found in MSIGDB mapping.\n",
      "Gene 2871 not found in MSIGDB mapping.\n",
      "Gene 100379661 not found in MSIGDB mapping.\n",
      "Gene 2601 not found in MSIGDB mapping.\n",
      "Gene 54035 not found in MSIGDB mapping.\n",
      "Gene 338091 not found in MSIGDB mapping.\n",
      "Gene 7361 not found in MSIGDB mapping.\n",
      "Gene 552859 not found in MSIGDB mapping.\n",
      "Gene 67 not found in MSIGDB mapping.\n",
      "Gene 64 not found in MSIGDB mapping.\n",
      "Gene 2884 not found in MSIGDB mapping.\n",
      "Gene 436 not found in MSIGDB mapping.\n",
      "Gene 606500 not found in MSIGDB mapping.\n",
      "Gene 236 not found in MSIGDB mapping.\n",
      "Gene 101730217 not found in MSIGDB mapping.\n",
      "Gene 102464828 not found in MSIGDB mapping.\n",
      "Gene 2951 not found in MSIGDB mapping.\n",
      "Gene 652991 not found in MSIGDB mapping.\n",
      "Gene 1338 not found in MSIGDB mapping.\n",
      "Gene 2613 not found in MSIGDB mapping.\n",
      "Gene 386607 not found in MSIGDB mapping.\n",
      "Gene 82 not found in MSIGDB mapping.\n",
      "Gene 2611 not found in MSIGDB mapping.\n",
      "Gene 1467 not found in MSIGDB mapping.\n",
      "Gene 1566 not found in MSIGDB mapping.\n",
      "Gene 7210 not found in MSIGDB mapping.\n",
      "Gene 1430 not found in MSIGDB mapping.\n",
      "Gene 100462812 not found in MSIGDB mapping.\n",
      "Gene 170541 not found in MSIGDB mapping.\n",
      "Gene 267012 not found in MSIGDB mapping.\n",
      "Gene 3186 not found in MSIGDB mapping.\n",
      "Gene 1204 not found in MSIGDB mapping.\n",
      "Gene 26584 not found in MSIGDB mapping.\n",
      "Gene 94026 not found in MSIGDB mapping.\n",
      "Gene 3492 not found in MSIGDB mapping.\n",
      "Gene 693 not found in MSIGDB mapping.\n",
      "Gene 145 not found in MSIGDB mapping.\n",
      "Gene 1648 not found in MSIGDB mapping.\n",
      "Gene 2341 not found in MSIGDB mapping.\n",
      "Gene 75 not found in MSIGDB mapping.\n",
      "Gene 83446 not found in MSIGDB mapping.\n",
      "Gene 2231 not found in MSIGDB mapping.\n",
      "Gene 100302267 not found in MSIGDB mapping.\n",
      "Gene 441108 not found in MSIGDB mapping.\n",
      "Gene 693167 not found in MSIGDB mapping.\n",
      "Gene 937 not found in MSIGDB mapping.\n",
      "Gene 2496 not found in MSIGDB mapping.\n",
      "Gene 2698 not found in MSIGDB mapping.\n",
      "Gene 2510 not found in MSIGDB mapping.\n",
      "0.9723502304147466 of DGIDB genes have a match in MSIGDB\n"
     ]
    }
   ],
   "source": [
    "## Build interlayer coupling matrices between the two layers\n",
    "\n",
    "# Open the JSON file and load its content into a dictionary\n",
    "with open(DGIDB_DIRECTORY + \"gene_to_index.json\", \"r\") as file:\n",
    "    dgidb = json.load(file)\n",
    "with open(MSIGDB_DIRECTORY + \"gene_to_index.json\", \"r\") as file:\n",
    "    msigdb = json.load(file)\n",
    "    \n",
    "# Jump probability for matching genes\n",
    "w = 1\n",
    "\n",
    "# Number of genes (assuming they are both of same size or matchable)\n",
    "num_genes_dgidb = len(dgidb)\n",
    "num_genes_msigdb = len(msigdb)\n",
    "\n",
    "# Initialize the inter-layer matrix with zeros\n",
    "interlayer_transition_matrix = np.zeros((num_genes_msigdb,num_genes_dgidb))\n",
    "i = 0\n",
    "# Build the inter-layer matrix\n",
    "for gene_dgidb, idx_dgidb in dgidb.items():\n",
    "    # If the gene exists in both gene-to-index mappings\n",
    "    if gene_dgidb in msigdb:      \n",
    "        idx_msigdb = msigdb[gene_dgidb]\n",
    "        interlayer_transition_matrix[idx_msigdb,idx_dgidb] = w  # Set jump probability\n",
    "        i += 1\n",
    "    else:\n",
    "        print(f\"Gene {gene_dgidb} not found in MSIGDB mapping.\")\n",
    "rows_with_high_sum = np.where(interlayer_transition_matrix.sum(axis=1) > 0)[0]\n",
    "print(i/len(dgidb), \"of DGIDB genes have a match in MSIGDB\")\n",
    "\n",
    "interlayer_transition_matrix = interlayer_transition_matrix.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23487d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Row-Normalization\n",
    "# # DGIDB\n",
    "# row_sums = np.array(DGIDB_adjacency_matrix.sum(axis=1)).ravel()\n",
    "# # inverse row sums (avoid division by zero)\n",
    "# inv_row_sums = np.reciprocal(row_sums, where=row_sums!=0)\n",
    "# # build diagonal matrix of inverses\n",
    "# D_inv = sp.diags(inv_row_sums)\n",
    "# DGIDB_adjacency_matrix = D_inv @ DGIDB_adjacency_matrix\n",
    "\n",
    "# # MSIGDB\n",
    "# row_sums = np.array(MSIGDB_adjacency_matrix.sum(axis=1)).ravel()\n",
    "# # inverse row sums (avoid division by zero)\n",
    "# inv_row_sums = np.reciprocal(row_sums, where=row_sums!=0)\n",
    "# # build diagonal matrix of inverses\n",
    "# D_inv = sp.diags(inv_row_sums)\n",
    "# MSIGDB_adjacency_matrix = D_inv @ MSIGDB_adjacency_matrix\n",
    "\n",
    "# # Coupling Matrix\n",
    "# row_sums = interlayer_transition_matrix.sum(axis = 1, keepdims= True)\n",
    "# row_sums[row_sums == 0] = 1 \n",
    "# interlayer_transition_matrix = interlayer_transition_matrix / row_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6a3f498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check for Row-stochastic\n",
    "# row_sums = np.array(DGIDB_adjacency_matrix.sum(axis=1)).ravel()\n",
    "# print(row_sums)\n",
    "# ok = np.all(np.isclose(row_sums, 1.0)|np.isclose(row_sums, 0))\n",
    "# print(\"Every row sums to 0 or 1?\", ok)\n",
    "\n",
    "# row_sums = np.array(MSIGDB_adjacency_matrix.sum(axis=1)).ravel()\n",
    "# print(row_sums)\n",
    "# ok = np.all(np.isclose(row_sums, 1.0)|np.isclose(row_sums, 0))\n",
    "# print(\"Every row sums to 0 or 1?\", ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197f1b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Build the multilayer transition matrix\n",
    "# interlayer_transition_prob = target_average\n",
    "interlayer_transition_prob = 0.5\n",
    "\n",
    "A = (1-interlayer_transition_prob) * DGIDB_weight * DGIDB_adjacency_matrix\n",
    "B = interlayer_transition_prob * geo_mean_weight * interlayer_transition_matrix.T\n",
    "C = interlayer_transition_prob * geo_mean_weight * interlayer_transition_matrix\n",
    "D =(1-interlayer_transition_prob) * MSIGDB_weight * MSIGDB_adjacency_matrix\n",
    "\n",
    "multilayer_transition_matrix = sp.bmat([\n",
    "    [A, B],\n",
    "    [C, D]\n",
    "]).tocsr()\n",
    "\n",
    "num_genes = multilayer_transition_matrix.shape[0]\n",
    "\n",
    "multilayer_transition_matrix = multilayer_transition_matrix.astype(np.float32)\n",
    "\n",
    "del A,B,C,D, DGIDB_adjacency_matrix,MSIGDB_adjacency_matrix\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69e8ac8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MTM Density: 0.14986548517575804\n"
     ]
    }
   ],
   "source": [
    "density = multilayer_transition_matrix.nnz / (multilayer_transition_matrix.shape[0] * multilayer_transition_matrix.shape[1])\n",
    "print(\"MTM Density:\", density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b88d6c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MTM nonzero average: 5.407776403910461e-05\n"
     ]
    }
   ],
   "source": [
    "MTM_average = multilayer_transition_matrix[multilayer_transition_matrix != 0].mean()\n",
    "print(\"MTM nonzero average:\",MTM_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7cba8e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Row-normalize the multilayer transition matrix\n",
    "# row_sums = np.array(multilayer_transition_matrix.sum(axis=1)).ravel()\n",
    "# nonzero_rows = row_sums != 0\n",
    "# inv_row_sums = np.zeros_like(row_sums)\n",
    "# inv_row_sums[nonzero_rows] = 1.0 / row_sums[nonzero_rows]\n",
    "# multilayer_transition_matrix = diags(inv_row_sums) @ multilayer_transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10882bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.46725573 0.47856281 0.23351737 ... 0.03240383 0.02832738 0.02202548]\n",
      "Every row sums to 0 or 1? False\n"
     ]
    }
   ],
   "source": [
    "row_sums = np.array(multilayer_transition_matrix.sum(axis=1)).ravel()\n",
    "print(row_sums)\n",
    "ok = np.all(np.isclose(row_sums, 1.0)|np.isclose(row_sums, 0))\n",
    "print(\"Every row sums to 0 or 1?\", ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1a99a2",
   "metadata": {},
   "source": [
    "# Computing Eigenvalues & Eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "eb638006",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_eigenvalues = 100\n",
    "num_neighbors = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4583a678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute top k eigenvalues by magnitude\n",
    "def top_k_eigenvalues(M,k):\n",
    "    vals, vecs = eigsh(M, k=k, which='LM')  # 'LM' = Largest Magnitude\n",
    "    idx = np.argsort(np.abs(vals))[::-1]\n",
    "    vals, vecs = vals[idx], vecs[:, idx]\n",
    "    return vals, vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2c2a1ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues: [0.41952252 0.41641355 0.35003126 0.33851925 0.33259445 0.33206096\n",
      " 0.32372215 0.3213992  0.3150959  0.31381208 0.31296661 0.30925614\n",
      " 0.3080701  0.3071239  0.3064803  0.3018787  0.2994868  0.2994655\n",
      " 0.29892248 0.29762894 0.2964892  0.2950117  0.29331246 0.2917004\n",
      " 0.29163155 0.29097325 0.289461   0.28938675 0.28838763 0.28605348\n",
      " 0.28510028 0.28404546 0.28330868 0.28149563 0.27967033 0.2788087\n",
      " 0.27799264 0.2778677  0.27643064 0.27537003 0.27498046 0.27490875\n",
      " 0.27456164 0.27451336 0.27424154 0.2734267  0.27274743 0.2714967\n",
      " 0.2712318  0.27067572 0.27029848 0.26963612 0.26952067 0.2685892\n",
      " 0.26839244 0.26811314 0.26794738 0.26747474 0.26728594 0.26650456\n",
      " 0.26619786 0.2659913  0.26532632 0.2652114  0.2650941  0.26477432\n",
      " 0.26455063 0.26430032 0.26372463 0.2634811  0.26341158 0.26314828\n",
      " 0.26292342 0.262814   0.26280823 0.2626629  0.2622843  0.26209405\n",
      " 0.26148495 0.2612958  0.2610038  0.26073706 0.26047376 0.26020876\n",
      " 0.26011994 0.25998044 0.2598423  0.2597562  0.2595846  0.2595659\n",
      " 0.25945994 0.25924307 0.2590857  0.25882086 0.25878924 0.2586368\n",
      " 0.25859562 0.25844494 0.25827545 0.25816306]\n",
      "Eigenvectors shape: (26755, 100)\n"
     ]
    }
   ],
   "source": [
    "# Compute eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = top_k_eigenvalues(multilayer_transition_matrix,num_eigenvalues)\n",
    "print(\"Eigenvalues:\", eigenvalues)\n",
    "print(\"Eigenvectors shape:\", eigenvectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdb0442",
   "metadata": {},
   "source": [
    "# KNN Graph Methods & Testing (commented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b0484461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stationary_distribution(W, tol=1e-12, maxit=10000):\n",
    "    # power iteration for left stationary of row-stochastic W\n",
    "    n = W.shape[0]\n",
    "    pi = np.ones(n) / n\n",
    "    for _ in range(maxit):\n",
    "        pi_next = pi @ W\n",
    "        if np.linalg.norm(pi_next - pi, 1) < tol:\n",
    "            break\n",
    "        pi = pi_next\n",
    "    return pi / pi.sum()\n",
    "\n",
    "def layer_weights_stationary(W, n):\n",
    "    pi = stationary_distribution(W)\n",
    "    w1 = float(pi[:n].sum()); w2 = 1.0 - w1\n",
    "    return w1, w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6fd98ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1,w2 = layer_weights_stationary(multilayer_transition_matrix, num_genes_dgidb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "73256356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22586992148019108 0.7741300785198089\n"
     ]
    }
   ],
   "source": [
    "print(w1,w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7611938b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csr_diff_metrics(\n",
    "    A, B,\n",
    "    *,\n",
    "    use_top_left_overlap=True,   # True => fast overlap by position; set False only if you align elsewhere\n",
    "    compute_spectral=False,      # off by default for huge matrices\n",
    "    include_support_jaccard=True,# quick structure-only difference (no weights)\n",
    "    dtype=np.float32             # compact math; set to np.float64 if you need extra precision\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare two very large square CSR matrices using only the overlapping block.\n",
    "    Default settings are safe and fast for huge sparse inputs.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict with:\n",
    "      - rmse      : per-entry root-mean-square error on overlap\n",
    "      - mae       : per-entry mean absolute error on overlap\n",
    "      - rel_fro   : ||A-B||_F / (||A||_F + ||B||_F), in [0,1] (0 = identical)\n",
    "      - cos_dist  : 1 - cosine(entrywise) on overlap (scale-insensitive)\n",
    "      - supp_jacc : (optional) Jaccard on supports; distance = 1 - supp_jacc\n",
    "      - spec_rel  : (optional) spectral norm ratio (expensive; off by default)\n",
    "    \"\"\"\n",
    "    if not (sp.isspmatrix_csr(A) and sp.isspmatrix_csr(B)):\n",
    "        raise TypeError(\"Pass CSR matrices for best performance (got types: \"\n",
    "                        f\"{type(A)}, {type(B)}).\")\n",
    "\n",
    "    # Cast data to compact dtype without copying indices/indptr\n",
    "    if A.dtype != dtype:\n",
    "        A = sp.csr_matrix((A.data.astype(dtype, copy=False), A.indices, A.indptr), shape=A.shape)\n",
    "    if B.dtype != dtype:\n",
    "        B = sp.csr_matrix((B.data.astype(dtype, copy=False), B.indices, B.indptr), shape=B.shape)\n",
    "\n",
    "    # Overlap by position (fast). If you need label alignment, do it upstream and keep this True.\n",
    "    m = min(A.shape[0], B.shape[0]) if use_top_left_overlap else min(A.shape[0], B.shape[0])\n",
    "    if m == 0:\n",
    "        return dict(rmse=np.nan, mae=np.nan, rel_fro=np.nan, cos_dist=np.nan,\n",
    "                    supp_jacc=np.nan if include_support_jaccard else None,\n",
    "                    spec_rel=np.nan if compute_spectral else None)\n",
    "\n",
    "    A_ = A[:m, :m]\n",
    "    B_ = B[:m, :m]\n",
    "    D  = A_ - B_\n",
    "\n",
    "    m2 = float(m) * float(m)\n",
    "\n",
    "    # Frobenius^2 via data arrays (no temporary matrices)\n",
    "    froA2 = float((A_.data.astype(np.float64, copy=False)**2).sum())  # accumulate in float64 for stability\n",
    "    froB2 = float((B_.data.astype(np.float64, copy=False)**2).sum())\n",
    "    froD2 = float((D.data.astype(np.float64, copy=False)**2).sum())\n",
    "\n",
    "    # Per-entry errors (size-normalized)\n",
    "    rmse = np.sqrt(froD2 / m2)\n",
    "    mae  = float(np.abs(D.data).sum()) / m2\n",
    "\n",
    "    # Relative Frobenius (bounded, scale-aware)\n",
    "    nA = np.sqrt(froA2); nB = np.sqrt(froB2); nD = np.sqrt(froD2)\n",
    "    rel_fro = nD / (nA + nB) if (nA + nB) > 0 else 0.0\n",
    "\n",
    "    # Cosine distance on entries (scale-insensitive)\n",
    "    dotAB = float(A_.multiply(B_).sum())  # only intersects nonzeros\n",
    "    cos_sim = (dotAB / (nA * nB)) if (nA > 0 and nB > 0) else 0.0\n",
    "    cos_dist = 1.0 - cos_sim\n",
    "\n",
    "    out = dict(\n",
    "        rmse=float(rmse),\n",
    "        mae=float(mae),\n",
    "        rel_fro=float(rel_fro),\n",
    "        cos_dist=float(cos_dist),\n",
    "    )\n",
    "\n",
    "    # Optional: structure-only Jaccard on supports (fast)\n",
    "    if include_support_jaccard:\n",
    "        SA = A_.astype(bool)\n",
    "        SB = B_.astype(bool)\n",
    "        inter = SA.multiply(SB).count_nonzero()\n",
    "        union = SA.maximum(SB).count_nonzero()  # elementwise OR without densifying\n",
    "        supp_jacc = inter / union if union else 1.0\n",
    "        out[\"supp_jacc\"] = float(supp_jacc)\n",
    "        out[\"supp_jacc_dist\"] = float(1.0 - supp_jacc)\n",
    "\n",
    "    # Optional: spectral norm ratio (still heavy on truly huge inputs)\n",
    "    if compute_spectral:\n",
    "        from scipy.sparse.linalg import svds\n",
    "        def spec_norm(X):\n",
    "            # For very small overlaps you could densify; for big keep svds(k=1)\n",
    "            if X.shape[0] <= 400:\n",
    "                return float(np.linalg.norm(X.toarray(), 2))\n",
    "            if X.nnz == 0:\n",
    "                return 0.0\n",
    "            s = svds(X, k=1, return_singular_vectors=False)\n",
    "            return float(abs(s[0]))\n",
    "        sA = spec_norm(A_)\n",
    "        sB = spec_norm(B_)\n",
    "        sD = spec_norm(D)\n",
    "        denom = sA + sB\n",
    "        out[\"spec_rel\"] = (sD / denom) if denom > 0 else 0.0\n",
    "    else:\n",
    "        out[\"spec_rel\"] = None\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2ed288ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build diffusion matrix from eigenvalues and eigenvectors (double check)\n",
    "def build_diffusion_dist_matrix(vals,vecs,t):\n",
    "    X_t = vecs * (vals ** (t))\n",
    "    D_t2 = squareform(pdist(X_t, metric='sqeuclidean'))\n",
    "    D_t2 = D_t2.astype(np.float32)\n",
    "    return D_t2\n",
    "\n",
    "def build_diffusion_dist_matrix_avg(vals,vecs,T):\n",
    "    diffusion_dist_matrix = np.empty((num_genes,num_genes))\n",
    "    for t in T:\n",
    "        np.add(diffusion_dist_matrix, \n",
    "            build_diffusion_dist_matrix(vals,vecs,t), \n",
    "            out=diffusion_dist_matrix)\n",
    "    diffusion_dist_matrix = diffusion_dist_matrix / len(T)\n",
    "    return diffusion_dist_matrix\n",
    "\n",
    "# Build kNN\n",
    "def build_kNN(diffusion_dist_matrix,k, sym_method='average'):\n",
    "    # Choose sigma to be the median of pair-wise distance\n",
    "    tmp = diffusion_dist_matrix.astype(np.float32, copy=True)\n",
    "    np.sqrt(tmp, out=tmp)\n",
    "    sigma = np.median(tmp[tmp != 0], overwrite_input=True)\n",
    "\n",
    "    n = diffusion_dist_matrix.shape[0]\n",
    "\n",
    "    # Used to indicate position of nonzero value needed to record (constructing a sparse matrix for efficiency)\n",
    "    rows, cols, vals = [], [], []\n",
    "    for i in range(n):\n",
    "        profile = np.exp(-diffusion_dist_matrix[i] / (2* (sigma**2))) \n",
    "        idx = np.argpartition(-profile, k+1)[:k+1]  # top-k+1 (includes self)\n",
    "        idx = idx[idx != i]                      # drop self\n",
    "        rows += [i]*k\n",
    "        cols += list(idx[:k])\n",
    "        vals += list(profile[idx[:k]])\n",
    "    adj_mat = csr_matrix((vals, (rows, cols)), shape=(n, n))\n",
    "\n",
    "    # Symmetrize the matrix by adding edges to one way edges and set weight to average\n",
    "    if (sym_method == 'average'):\n",
    "        adj_mat = (adj_mat + adj_mat.T).multiply(0.5).tocsr()\n",
    "\n",
    "    # Symmetrize the matrix by unioning\n",
    "    elif (sym_method == 'union'):\n",
    "        adj_mat = adj_mat.maximum(adj_mat.T)\n",
    "    elif (sym_method == 'none'):\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(\"sym_method must be 'average' or 'union'\")\n",
    "    \n",
    "    kNN_graph = nx.from_numpy_array(adj_mat)\n",
    "\n",
    "    \n",
    "    return adj_mat,kNN_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a7df769d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build Aggregated kNN\n",
    "\n",
    "# Load gene-to-index mappings\n",
    "with open(DGIDB_DIRECTORY+ \"gene_to_index.json\", \"r\") as f:\n",
    "    DGIDB_gene_to_index = json.load(f)\n",
    "with open(MSIGDB_DIRECTORY + \"gene_to_index.json\", \"r\") as f:\n",
    "    MSIGDB_gene_to_index = json.load(f)\n",
    "# Reverse the dictionary\n",
    "DGIDB_index_to_gene = {index: gene for gene, index in DGIDB_gene_to_index.items()}\n",
    "\n",
    "\n",
    "### Prerequisite: interlayer_transition_matrix, num_genes_dgidb, num_genes_msigdb\n",
    "def build_aggregated_kNN(diffusion_dist_matrix,\n",
    "                         k,\n",
    "                         n1 = num_genes_dgidb,\n",
    "                         n2 = num_genes_msigdb,\n",
    "                         sym_method='average'):\n",
    "    w1,w2 = layer_weights_stationary(multilayer_transition_matrix, num_genes_dgidb)\n",
    "    \n",
    "    similarity_matrix = build_kNN(diffusion_dist_matrix,k)[0]\n",
    "    \n",
    "    DGIDB_sim_matrix = similarity_matrix[:n1, :n1]\n",
    "    MSIGDB_sim_matrix = similarity_matrix[n1:, n1:]\n",
    "\n",
    "    DGIDB_index_map = {}\n",
    "    gene_to_index_additional = {}\n",
    "    coupling_matrix = interlayer_transition_matrix.T\n",
    "    num_additional_rows = 0\n",
    "    index = 0\n",
    "    for row in coupling_matrix:\n",
    "        if(np.all(row == 0)):\n",
    "            DGIDB_index_map[index] = n2 + num_additional_rows \n",
    "            gene_to_index_additional[DGIDB_index_to_gene[index]] = n2 + num_additional_rows\n",
    "            num_additional_rows += 1\n",
    "        else:\n",
    "            l = np.nonzero(row)[0]\n",
    "            if (len(l) > 1):\n",
    "                print(\"ERROR\")\n",
    "            else:\n",
    "                msigdb_pos = l[0]\n",
    "                DGIDB_index_map[index] = msigdb_pos\n",
    "        index += 1\n",
    "\n",
    "    zero_block = sp.csr_matrix((num_additional_rows, num_additional_rows), dtype=MSIGDB_sim_matrix.dtype)\n",
    "    asm = sp.block_diag((MSIGDB_sim_matrix, zero_block), format=\"csr\")\n",
    "\n",
    "    # Build and save gene_to_index_distinct mapping\n",
    "    gene_to_index_distinct = MSIGDB_gene_to_index | gene_to_index_additional\n",
    "\n",
    "    save_path = OUTPUT_DIRECTORY + \"gene_to_index_distinct.json\"\n",
    "    if not os.path.exists(save_path):\n",
    "        with open(save_path, 'w') as pathway_file:\n",
    "            json.dump(gene_to_index_distinct, pathway_file, indent=4)\n",
    "        print(f\"Mappings saved to {save_path}\")  \n",
    "    ### Aggregation\n",
    "\n",
    "    # rows, cols = DGIDB_sim_matrix.nonzero()\n",
    "    # for i, j in zip(rows, cols):\n",
    "    #     msig_i,msig_j = DGIDB_index_map[i], DGIDB_index_map[j]\n",
    "    #     if (asm[msig_i,msig_j] == 0):\n",
    "    #         asm[msig_i, msig_j] = DGIDB_sim_matrix[i, j]\n",
    "    #     else:\n",
    "    #         dgidb_sim_val = DGIDB_sim_matrix[i, j]\n",
    "    #         msigdb_sim_val = asm[msig_i, msig_j]\n",
    "    #         asm[msig_i, msig_j] = (w1 * dgidb_sim_val + w2 * msigdb_sim_val)   \n",
    "\n",
    "    M = asm.tocsr()\n",
    "\n",
    "    # 1) Get COO to access row/col vectors\n",
    "    DGIDB = DGIDB_sim_matrix.tocoo(copy=False)\n",
    "    row = DGIDB.row.astype(np.int64, copy=False)\n",
    "    col = DGIDB.col.astype(np.int64, copy=False)\n",
    "\n",
    "    # 2) Turn the mapping into a 1-D integer array so we can do map_arr[row], map_arr[col]\n",
    "    N = DGIDB_sim_matrix.shape[0]\n",
    "\n",
    "    if isinstance(DGIDB_index_map, dict):\n",
    "        # Fill with -1 to detect missing keys\n",
    "        map_arr = np.full(N, -1, dtype=np.int64)\n",
    "        for k, v in DGIDB_index_map.items():\n",
    "            if 0 <= k < N:\n",
    "                map_arr[k] = int(v)\n",
    "        if (map_arr < 0).any():\n",
    "            missing = np.flatnonzero(map_arr < 0)\n",
    "            raise ValueError(f\"DGIDB_index_map is missing {missing.size} DGIDB indices; \"\n",
    "                            f\"first few missing: {missing[:5].tolist()}\")\n",
    "    else:\n",
    "        # list/ndarray case\n",
    "        map_arr = np.asarray(DGIDB_index_map, dtype=np.int64)\n",
    "        if map_arr.ndim != 1 or map_arr.shape[0] != N:\n",
    "            raise ValueError(f\"DGIDB_index_map must be length {N} 1-D; got shape {map_arr.shape}\")\n",
    "\n",
    "    # 3) Vectorized remap of DGIDB â†’ MSIGDB indices\n",
    "    msig_r = map_arr[row]\n",
    "    msig_c = map_arr[col]\n",
    "\n",
    "    # 4) Build remapped DGIDB matrix D in MSIGDB space (CSR)\n",
    "    D = sp.csr_matrix((DGIDB.data, (msig_r, msig_c)), shape=M.shape)\n",
    "\n",
    "    # 5) Vectorized aggregation (exactly matches your if/else logic)\n",
    "    Mb = M.copy(); Mb.data[:] = 1   # support(M)\n",
    "    Db = D.copy(); Db.data[:] = 1   # support(D)\n",
    "\n",
    "    M_keep = M - M.multiply(Db)                             # keep M where D==0\n",
    "    \n",
    "    D_only = D - D.multiply(Mb) # take D where M==0\n",
    "    \n",
    "    overlap = w1 * D.multiply(Mb) + w2 * M.multiply(Db)     # where both nonzero\n",
    "\n",
    "    # aggregated = (M_keep + D_only + overlap).tocsr()\n",
    "    aggregated = (M_keep).tocsr() #WRONG NEED TO BE CHANGED\n",
    "    aggregated.eliminate_zeros()\n",
    "\n",
    "    # Result\n",
    "    asm = aggregated\n",
    "\n",
    "    if (sym_method not in ['average','union','none']):\n",
    "        raise ValueError(\"sym_method must be either 'average' or 'union'\")\n",
    "    elif (sym_method == 'none'):\n",
    "        # No symmetrization\n",
    "        pass\n",
    "    elif (sym_method == 'average'):\n",
    "        # Symmetrize the matrix by adding edges to one way edges and set weight to average\n",
    "        asm = (asm + asm.T).multiply(0.5).tocsr()\n",
    "    elif (sym_method == 'union'):\n",
    "        # Symmetrize the matrix by unioning\n",
    "        asm = asm.maximum(asm.T)\n",
    "\n",
    "    graph = nx.from_scipy_sparse_array(asm)\n",
    "    \n",
    "    return asm, graph\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "996f9baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddm = build_diffusion_dist_matrix(eigenvalues, eigenvectors, [2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "235a4651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn_orig,_ = build_kNN(ddm,num_neighbors)\n",
    "# knn_new,_,knn_sus = build_aggregated_kNN(ddm,num_neighbors)\n",
    "# csr_equal_tol(knn_orig,knn_sus)\n",
    "# print(is_symmetric(knn_orig),is_symmetric(knn_new))\n",
    "# print(knn_orig)\n",
    "# print(knn_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "de4a026f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(csr_diff_metrics(knn_orig, knn_new,use_top_left_overlap=True))\n",
    "# print(knn_orig.shape, knn_new.shape)\n",
    "# print(csr_diff_metrics(knn_orig[num_genes_dgidb:, num_genes_dgidb:], knn_new,use_top_left_overlap=True))\n",
    "# print(knn_orig[num_genes_dgidb:, num_genes_dgidb:].shape, knn_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b6435413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# density_orig = knn_orig.nnz / (knn_orig.shape[0] * knn_orig.shape[1])\n",
    "# print(density_orig)\n",
    "# density_new = knn_new.nnz / (knn_new.shape[0] * knn_new.shape[1])\n",
    "# print(density_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9797c469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(knn_orig.nnz, knn_new.nnz)\n",
    "# print(knn_orig.data.mean(), knn_new.data.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "57c0350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del ddm, knn_orig, knn_sus, knn_new\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f669ca",
   "metadata": {},
   "source": [
    "# Clustering Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "33e30836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Louvain\n",
    "# def louvain_from_adj(A, resolution=1.0, random_state=0, keep_lcc=False):\n",
    "#     \"\"\"\n",
    "#     A: symmetric, non-negative scipy.sparse adjacency (CSR preferred).\n",
    "#     Returns: labels (np.ndarray of length n), graph G (NetworkX), node_order\n",
    "#     \"\"\"\n",
    "#     # if not sp.isspmatrix(A):  # allow dense but convert\n",
    "#     #     A = sp.csr_matrix(A)\n",
    "#     # # Optional: ensure symmetry numerically\n",
    "#     # if (A - A.T).nnz != 0:\n",
    "#     #     raise ValueError(\"Adjacency must be symmetric. Symmetrize first.\")\n",
    "\n",
    "#     # Build graph\n",
    "#     # networkx >=3.0: from_scipy_sparse_array; older: from_scipy_sparse_matrix\n",
    "#     G = nx.from_scipy_sparse_array(A, edge_attribute='weight')  # undirected by default\n",
    "\n",
    "#     if keep_lcc:\n",
    "#         # keep only the largest connected component if you prefer\n",
    "#         largest_cc = max(nx.connected_components(G), key=len)\n",
    "#         G = G.subgraph(largest_cc).copy()\n",
    "\n",
    "#     # Run Louvain\n",
    "#     part = community_louvain.best_partition(\n",
    "#         G, weight='weight', resolution=resolution, random_state=random_state\n",
    "#     )\n",
    "#     node_order = sorted(G.nodes())\n",
    "#     labels = np.array([part[i] for i in node_order], dtype=int)\n",
    "#     return labels, G, node_order\n",
    "\n",
    "# def DDBC(MTM,num_eigenvalues,num_neighbors,resolution,T):\n",
    "#     vals,vecs = top_k_eigenvalues(M = MTM, k = num_eigenvalues)\n",
    "\n",
    "#     diffusion_dist_matrix = np.empty((num_genes,num_genes))\n",
    "#     for t in T:\n",
    "#         np.add(diffusion_dist_matrix, \n",
    "#                build_diffusion_dist_matrix(vals,vecs,t,num_eigenvalues), \n",
    "#                out=diffusion_dist_matrix)\n",
    "#     diffusion_dist_matrix = diffusion_dist_matrix / len(T)\n",
    "    \n",
    "#     kNN_adjacency_matrix = build_kNN(diffusion_dist_matrix,num_neighbors)\n",
    "\n",
    "#     # # Symmetrize the matrix by adding edges to one way edges and set weight to average\n",
    "#     # kNN_adjacency_matrix = (kNN_adjacency_matrix + kNN_adjacency_matrix.T).multiply(0.5).tocsr()\n",
    "\n",
    "#     # Symmetrize the matrix by unioning\n",
    "#     kNN_adjacency_matrix = kNN_adjacency_matrix.maximum(kNN_adjacency_matrix.T)\n",
    "\n",
    "#     return louvain_from_adj(kNN_adjacency_matrix, resolution = resolution, keep_lcc = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "45534523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leiden_from_knn_adjacency(\n",
    "    A,\n",
    "    *,\n",
    "    method=\"modularity\",      # \"modularity\" | \"rb\" | \"cpm\"\n",
    "    resolution=1.0,           # used by \"rb\" and \"cpm\"\n",
    "    n_iterations=-1,          # -1 => until no improvement\n",
    "    seed=42,\n",
    "    use_weights=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Run Leiden on a symmetric, undirected (weighted) kNN adjacency (SciPy sparse).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    labels : np.ndarray[int]\n",
    "        Community ID per node (0..k-1)\n",
    "    quality : float\n",
    "        Objective value (modularity / RB / CPM depending on 'method')\n",
    "    \"\"\"\n",
    "    if not sp.issparse(A):\n",
    "        raise TypeError(\"A must be a SciPy sparse matrix.\")\n",
    "    # Normalize format & dtype\n",
    "    A = A.tocsr().astype(np.float32, copy=False)\n",
    "\n",
    "    # Clean diagonal and robustly symmetrize\n",
    "    A.setdiag(0.0)\n",
    "    A.eliminate_zeros()\n",
    "    A = A.maximum(A.T)  # keep max weight per undirected edge\n",
    "\n",
    "    # Build igraph from upper triangle (each undirected edge once)\n",
    "    U = sp.triu(A, k=1, format=\"coo\")\n",
    "    n = A.shape[0]\n",
    "    g = ig.Graph(n=n, edges=list(zip(U.row.tolist(), U.col.tolist())), directed=False)\n",
    "    wname = None\n",
    "    if use_weights:\n",
    "        g.es[\"weight\"] = U.data.tolist()\n",
    "        wname = \"weight\"\n",
    "\n",
    "    # Pick partition class + kwargs\n",
    "    m = method.lower()\n",
    "    if m == \"modularity\":\n",
    "        part_cls = la.ModularityVertexPartition\n",
    "        kwargs = dict(weights=wname)\n",
    "    elif m == \"rb\":\n",
    "        part_cls = la.RBConfigurationVertexPartition\n",
    "        kwargs = dict(weights=wname, resolution_parameter=resolution)\n",
    "    elif m == \"cpm\":\n",
    "        part_cls = la.CPMVertexPartition\n",
    "        kwargs = dict(weights=wname, resolution_parameter=resolution)\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'modularity', 'rb', or 'cpm'\")\n",
    "\n",
    "    # Run Leiden\n",
    "    part = la.find_partition(\n",
    "        g,\n",
    "        part_cls,\n",
    "        n_iterations=n_iterations,\n",
    "        seed=seed,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    labels = np.array(part.membership, dtype=np.int32)\n",
    "    communities = [list(c) for c in part]\n",
    "    return labels, float(part.quality()), communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "71564780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DDBC(vals,\n",
    "         vecs,\n",
    "         num_neighbors,\n",
    "         resolution,\n",
    "         T,\n",
    "         leiden_method = \"modularity\",\n",
    "         ddm = None):\n",
    "    if ddm is None:\n",
    "        ddm = build_diffusion_dist_matrix_avg()\n",
    "        np.save(f\"../output/{DISEASE}/diffusion_dist_matrices/ddm_{T}_res-{resolution}_itp-{interlayer_transition_prob}.npy\",diffusion_dist_matrix)\n",
    "    else:\n",
    "        diffusion_dist_matrix = build_diffusion_dist_matrix_avg(eigenvalues, eigenvectors,T)\n",
    "    # kNN_adjacency_matrix, kNN_graph = build_kNN(diffusion_dist_matrix,num_neighbors)\n",
    "    kNN_adjacency_matrix, kNN_graph = build_aggregated_kNN(diffusion_dist_matrix,num_neighbors)\n",
    "\n",
    "    return (*leiden_from_knn_adjacency(kNN_adjacency_matrix,method=leiden_method,resolution=resolution,n_iterations=-1,seed=42), kNN_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "032d1e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def community_report_onepage(labels, score, out_pdf=\"leiden_community_report.pdf\", *,\n",
    "                             extra_text=None, bins=\"auto\", title=\"Community Sizes\", time_steps = \"N/A\"):\n",
    "    \"\"\"\n",
    "    Create ONE PDF page (top: text summary, bottom: histogram).\n",
    "    If out_pdf exists: append this page. Else: create it.\n",
    "\n",
    "    Requires: matplotlib, pypdf  (pip install pypdf)\n",
    "    \"\"\"\n",
    "    # ---- inputs ----\n",
    "    labels = np.asarray(labels)\n",
    "    if labels.ndim != 1:\n",
    "        raise ValueError(\"labels must be a 1-D array of community ids\")\n",
    "\n",
    "    # ---- stats ----\n",
    "    sizes = np.bincount(labels.astype(np.int64, copy=False))\n",
    "    sizes_sorted = np.sort(sizes)\n",
    "    n, k = sizes.sum(), sizes.size\n",
    "\n",
    "    lines = [\n",
    "        \"Leiden Partition Summary\",\n",
    "        \"========================\",\n",
    "        f\"Nodes (n):           {n:,}\",\n",
    "        f\"Time steps:          {time_steps}\",\n",
    "        f\"Communities (k):     {k:,}\",\n",
    "        f\"Size (min):          {int(sizes_sorted[0]) if k else 0:,}\",\n",
    "        f\"Size (median):       {float(np.median(sizes_sorted)) if k else 0.0:.3f}\",\n",
    "        f\"Size (mean):         {float(sizes_sorted.mean()) if k else 0.0:.6f}\",\n",
    "        f\"Size (max):          {int(sizes_sorted[-1]) if k else 0:,}\",\n",
    "        f\"Score:               {score}\",\n",
    "        \"\",\n",
    "        \"Top 10 largest communities (id: size):\",\n",
    "    ]\n",
    "    for cid in np.argsort(-sizes)[:min(10, k)]:\n",
    "        lines.append(f\"  {int(cid):5d}: {int(sizes[cid]):,}\")\n",
    "    if extra_text:\n",
    "        lines += [\"\", \"Extra:\", *([extra_text] if isinstance(extra_text, str) else list(extra_text))]\n",
    "    summary_text = \"\\n\".join(lines)\n",
    "\n",
    "    # ---- draw single-page figure ----\n",
    "    fig = plt.figure(figsize=(8.5, 11), dpi=150)          # US Letter, higher DPI\n",
    "    ax_text = fig.add_axes([0.06, 0.55, 0.88, 0.40])      # [left, bottom, width, height]\n",
    "    ax_text.axis(\"off\")\n",
    "    ax_text.text(0.0, 1.0, summary_text, va=\"top\", ha=\"left\", fontsize=11, family=\"monospace\")\n",
    "\n",
    "    ax_hist = fig.add_axes([0.10, 0.08, 0.80, 0.38])\n",
    "    ax_hist.hist(sizes, bins=bins)\n",
    "    ax_hist.set_xlabel(\"Community size\")\n",
    "    ax_hist.set_ylabel(\"Count of communities\")\n",
    "    ax_hist.set_title(f\"Distribution of {title}\")\n",
    "\n",
    "    # ---- write this page to a temp PDF on disk ----\n",
    "    with NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmpf:\n",
    "        tmp_page = Path(tmpf.name)\n",
    "    with PdfPages(tmp_page) as pdf:\n",
    "        pdf.savefig(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # ---- append/create using PdfReader/PdfWriter (robust across versions) ----\n",
    "    try:\n",
    "        from pypdf import PdfReader, PdfWriter\n",
    "    except Exception as e:\n",
    "        try: os.remove(tmp_page)\n",
    "        except: pass\n",
    "        raise RuntimeError(\"Please install 'pypdf' (e.g., `pip install pypdf`).\") from e\n",
    "\n",
    "    out_pdf = Path(out_pdf)\n",
    "    tmp_out = out_pdf.with_suffix(out_pdf.suffix + \".tmp\")\n",
    "\n",
    "    writer = PdfWriter()\n",
    "\n",
    "    # if existing, copy old pages first\n",
    "    if out_pdf.exists():\n",
    "        with open(out_pdf, \"rb\") as f_exist:\n",
    "            reader = PdfReader(f_exist)\n",
    "            for p in reader.pages:\n",
    "                writer.add_page(p)\n",
    "\n",
    "    # add the new single page\n",
    "    with open(tmp_page, \"rb\") as f_new:\n",
    "        reader_new = PdfReader(f_new)\n",
    "        for p in reader_new.pages:\n",
    "            writer.add_page(p)\n",
    "\n",
    "    # atomic write\n",
    "    with open(tmp_out, \"wb\") as f_out:\n",
    "        writer.write(f_out)\n",
    "    os.replace(tmp_out, out_pdf)\n",
    "\n",
    "    # cleanup\n",
    "    try: os.remove(tmp_page)\n",
    "    except: pass\n",
    "\n",
    "    return out_pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52195a5",
   "metadata": {},
   "source": [
    "# Run DDBC 1 - 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0400590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leiden Clustering\n",
    "pdf_path = f\"../output/{DISEASE}/leiden_report.pdf\"\n",
    "method = \"modularity\"\n",
    "resolution = 1.0\n",
    "num_bins = 100\n",
    "write = True\n",
    "queue = [[i] for i in range(1,25)]\n",
    "label_list = []\n",
    "score_list = []\n",
    "graph_list = []\n",
    "communities_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c46d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load variables:\n",
    "# DATA_DIRECTORY = OUTPUT_DIRECTORY + \"leiden_result_variables_temp\"\n",
    "# with open(f\"{DATA_DIRECTORY}/label.pkl\", \"rb\") as f:\n",
    "#     label_list = pickle.load(f)\n",
    "# with open(f\"{DATA_DIRECTORY}/score.pkl\", \"rb\") as f:\n",
    "#     score_list = pickle.load(f)\n",
    "# with open(f\"{DATA_DIRECTORY}/graph.pkl\", \"rb\") as f:\n",
    "#     graph_list = pickle.load(f)\n",
    "# with open(f\"{DATA_DIRECTORY}/communities.pkl\", \"rb\") as f:\n",
    "#     communities_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c240ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for T in queue:\n",
    "    ddm = np.load(f\"../output/{DISEASE}/diffusion_dist_matrices/ddm_{T}_res-{resolution}.npy\")\n",
    "    labels, score, communities, graph = DDBC(eigenvalues, eigenvectors, num_neighbors, resolution, T, leiden_method = method, ddm = ddm)\n",
    "    if (write):\n",
    "        path = community_report_onepage(labels, score, out_pdf=pdf_path,\n",
    "                                        extra_text=[f\"method={method}\", f\"resolution={resolution}\"], \n",
    "                                        bins=num_bins,time_steps = T)\n",
    "        print(\"Wrote:\", path)\n",
    "    label_list.append(labels)\n",
    "    score_list.append(score) \n",
    "    graph_list.append(graph)\n",
    "    communities_list.append(communities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957026c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save resulting variables to save time\n",
    "# DATA_DIRECTORY = OUTPUT_DIRECTORY + \"leiden_result_variables_temp\"\n",
    "# with open(f\"{DATA_DIRECTORY}/label.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(label_list, f)\n",
    "# with open(f\"{DATA_DIRECTORY}/score.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(score_list, f)\n",
    "# with open(f\"{DATA_DIRECTORY}/graph.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(graph_list, f)\n",
    "# with open(f\"{DATA_DIRECTORY}/communities.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(communities_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8068439",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a73059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def community_central_genes(G, community_nodes, weight=\"weight\", top_n=20):\n",
    "    C = set(community_nodes)\n",
    "    H = G.subgraph(C).copy()                       # induced subgraph\n",
    "    # within-community (weighted) degree\n",
    "    k = {u: H.degree(u, weight=weight) for u in H}\n",
    "    ks = np.array(list(k.values()), dtype=float)\n",
    "    mu, sigma = ks.mean(), ks.std() if ks.std() > 0 else 1.0\n",
    "    Z = {u: (k[u] - mu)/sigma for u in H}          # within-module degree z-score\n",
    "\n",
    "    # rank by z\n",
    "    ranked = sorted(H.nodes(), key=lambda u: (Z[u]), reverse=True)\n",
    "    return {u : Z[u] for u in ranked[:top_n]}\n",
    "\n",
    "def weighted_jaccard(scoresA, scoresB):\n",
    "    \"\"\"\n",
    "    Compute Weighted Jaccard similarity between two communities\n",
    "    based on gene importance scores.\n",
    "    \n",
    "    Parameters:\n",
    "        scoresA, scoresB : dict\n",
    "            {gene: importance_score}\n",
    "            Scores can be any nonnegative values (e.g., PageRank, Z-score).\n",
    "    Returns:\n",
    "        float\n",
    "            Weighted Jaccard similarity in [0, 1].\n",
    "    \"\"\"\n",
    "    genes = set(scoresA) | set(scoresB)\n",
    "    if not genes:\n",
    "        return 0.0\n",
    "    num = sum(min(scoresA.get(g, 0.0), scoresB.get(g, 0.0)) for g in genes)\n",
    "    den = sum(max(scoresA.get(g, 0.0), scoresB.get(g, 0.0)) for g in genes)\n",
    "    return num / den if den > 0 else 0.0\n",
    "\n",
    "def weighted_overlap_coefficient(dictA, dictB):\n",
    "    \"\"\"\n",
    "    Weighted Szymkiewiczâ€“Simpson (Overlap) coefficient âˆˆ [0,1].\n",
    "    \"\"\"\n",
    "    if not dictA or not dictB:\n",
    "        return 0.0\n",
    "\n",
    "    common = set(dictA) & set(dictB)\n",
    "    inter_sum = sum(min(dictA[v], dictB[v]) for v in common)\n",
    "\n",
    "    sumA = sum(max(0, w) for w in dictA.values())\n",
    "    sumB = sum(max(0, w) for w in dictB.values())\n",
    "    denom = min(sumA, sumB)\n",
    "\n",
    "    return inter_sum / denom if denom > 0 else 0.0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dedd81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wjacc_edges_builder(g_list,c_list,score_cap = 0,com_size_cap = 0):\n",
    "    result = []\n",
    "    for i in range(len(g_list)-1):\n",
    "        prev_G = g_list[i]\n",
    "        curr_G = g_list[i+1]\n",
    "        prev_com = c_list[i]\n",
    "        curr_com = c_list[i+1]\n",
    "        prev_z = []\n",
    "        curr_z = []\n",
    "        \n",
    "        for com in prev_com:\n",
    "            if (len(com) > com_size_cap):\n",
    "                prev_z.append(community_central_genes(prev_G,com))\n",
    "        for com in curr_com:\n",
    "            if (len(com) > com_size_cap):\n",
    "                curr_z.append(community_central_genes(curr_G,com))\n",
    "\n",
    "        for j in range(len(prev_z)):\n",
    "            for k in range(len(curr_z)):\n",
    "                jaccard_score = weighted_jaccard(prev_z[j],curr_z[k])\n",
    "                if (jaccard_score >= score_cap):\n",
    "                    result.append((queue[i][0],j,k,jaccard_score))\n",
    "    \n",
    "    return result\n",
    "\n",
    "def community_sizes(labels):\n",
    "    \"\"\"Return dict[label] -> number of nodes in that community.\"\"\"\n",
    "    lab = np.asarray(labels)\n",
    "    uniq, cnt = np.unique(lab, return_counts=True)\n",
    "    return {int(c): int(n) for c, n in zip(uniq, cnt)}\n",
    "\n",
    "def topk_at_t1(labels_t1, k=10):\n",
    "    \"\"\"Return list of top-k community labels at t=1 by size.\"\"\"\n",
    "    sizes = community_sizes(labels_t1)\n",
    "    return [c for c, _ in sorted(sizes.items(), key=lambda x: x[1], reverse=True)[:k]], sizes\n",
    "\n",
    "# ---------- Jaccard lookup ----------\n",
    "\n",
    "def build_edge_lookup(wjacc_edges):\n",
    "    \"\"\"\n",
    "    Build a dict mapping (t, comm_t) -> list of (comm_t+1, jaccard_score).\n",
    "    Each wjacc_edges element = (t, comm_t, comm_tplus1, jaccard_score)\n",
    "    \"\"\"\n",
    "    out = defaultdict(list)\n",
    "    for t, c1, c2, s in wjacc_edges:\n",
    "        out[(int(t), int(c1))].append((int(c2), float(s)))\n",
    "    return out\n",
    "\n",
    "# ---------- Tracking ----------\n",
    "\n",
    "def track_paths(seeds, wj_lookup, ts):\n",
    "    \"\"\"Track each seed community forward by max Jaccard each step.\"\"\"\n",
    "    print(ts)\n",
    "    t0 = ts[0]\n",
    "    paths = {s: [(t0, s)] for s in seeds}\n",
    "    edges = []\n",
    "    for s in seeds:\n",
    "        cur = s\n",
    "        for i in range(len(ts) - 1):\n",
    "            t = ts[i]\n",
    "            tp1 = ts[i+1]\n",
    "            cand = wj_lookup.get((t, cur), [])\n",
    "            if not cand:\n",
    "                print(f\"Seed {s}, {t} to {tp1} has no outgoing edges. Termniated.\")\n",
    "                paths[s].append((tp1, None))\n",
    "                cur = None\n",
    "                break\n",
    "            nxt, score = max(cand, key=lambda x: x[1])\n",
    "            edges.append(((t, cur), (tp1, nxt), score))\n",
    "            paths[s].append((tp1, nxt))\n",
    "            cur = nxt\n",
    "    return paths, edges\n",
    "\n",
    "# MY VERSION\n",
    "# def track_paths(labels_by_t, seeds, wj_lookup, ts,score_cap = 0.1):\n",
    "#     \"\"\"Track each seed community forward by max Jaccard each step.\"\"\"\n",
    "#     t0 = ts[0]\n",
    "#     paths = {s: [(t0, s)] for s in seeds}\n",
    "#     edges = []\n",
    "#     curs = []\n",
    "#     curs_next = []\n",
    "#     for s in seeds:\n",
    "#         curs_next.append(s)\n",
    "#         for i in range(len(ts) - 1):\n",
    "#             curs = curs_next\n",
    "#             curs_next = []\n",
    "#             for cur in curs:\n",
    "#                 t = ts[i]\n",
    "#                 tp1 = ts[i+1]\n",
    "#                 cand = wj_lookup.get((t, cur), [])\n",
    "#                 if not cand:\n",
    "#                     if (t == t0):\n",
    "#                         print(f\"Community {cur} has no outgoing edges\")\n",
    "#                     paths[s].append((tp1, None))\n",
    "#                     cur = None\n",
    "#                     continue\n",
    "                \n",
    "                \n",
    "#                 for nxt,score in cand:\n",
    "#                     if (score >= score_cap):\n",
    "#                         edges.append(((t, cur), (tp1, nxt), score))\n",
    "#                         paths[s].append((tp1, nxt))\n",
    "#                         curs_next.append(nxt)\n",
    "\n",
    "#     return paths, edges\n",
    "\n",
    "# ---------- Get node sizes ----------\n",
    "\n",
    "def node_sizes_for_paths(labels_by_t, paths):\n",
    "    \"\"\"Return dict[(t, comm)] -> size (number of vertices).\"\"\"\n",
    "    out = {}\n",
    "    for s, seq in paths.items():\n",
    "        for t, c in seq:\n",
    "            if c is None: continue\n",
    "            sizes = community_sizes(labels_by_t[t])\n",
    "            out[(t, c)] = sizes.get(c, 0)\n",
    "    return out\n",
    "\n",
    "# ---------- Draw layered network ----------\n",
    "\n",
    "def draw_layered_paths(paths, edge_list, node_sizes, ts,\n",
    "                       node_size_scale=700.0, edge_width_scale=8.0,\n",
    "                       seed_gap=1.0, col_gap=3.0, score_cap = 0.5, title=None, save_path = None):\n",
    "    seeds = list(paths.keys())\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Add nodes\n",
    "    for s in seeds:\n",
    "        for t, c in paths[s]:\n",
    "            if c is None: continue\n",
    "            G.add_node((s, t, c), seed=s, t=t, comm=c, size=node_sizes.get((t, c), 0))\n",
    "    # Add edges\n",
    "    # Set your inclusion threshold (cap)\n",
    "    # include all edges with Jaccard >= 0.5\n",
    "\n",
    "    for (t, c1), (tp, c2), w in edge_list:\n",
    "        if w < score_cap:\n",
    "            continue\n",
    "        for s in seeds:\n",
    "            seq = paths[s]\n",
    "            for i in range(len(seq) - 1):\n",
    "                if seq[i] == (t, c1) and seq[i+1] == (tp, c2):\n",
    "                    G.add_edge((s, t, c1), (s, tp, c2), jacc=w)\n",
    "\n",
    "    # Layout positions\n",
    "    pos = {}\n",
    "    nodes_by_t = defaultdict(list)\n",
    "    for n in G.nodes:\n",
    "        t = G.nodes[n]['t']\n",
    "        nodes_by_t[t].append(n)\n",
    "\n",
    "    # Sort nodes in each column by community size (largest first)\n",
    "    pos = {}\n",
    "    for t in ts:\n",
    "        column_nodes = nodes_by_t.get(t, [])\n",
    "        # sort by size descending\n",
    "        column_nodes.sort(key=lambda n: G.nodes[n].get('size', G.nodes[n].get('size_w', 0)), reverse=True)\n",
    "        for i, n in enumerate(column_nodes):\n",
    "            pos[n] = (ts.index(t) * col_gap, -i * seed_gap)\n",
    "\n",
    "    # Scale sizes and widths\n",
    "    if len(G) == 0:\n",
    "        raise ValueError(\"No nodes to draw â€” check your data.\")\n",
    "    vals = np.array([G.nodes[n]['size'] for n in G.nodes], dtype=float)\n",
    "    vmin, vmax = vals.min(), vals.max()\n",
    "    sizes = []\n",
    "    for n in G.nodes:\n",
    "        w = G.nodes[n]['size']\n",
    "        a = (w - vmin) / (vmax - vmin) if vmax > vmin else 1.0\n",
    "        sizes.append((0.3 + 0.7*a) * node_size_scale)\n",
    "    widths = [max(0.5, d['jacc'] * edge_width_scale) for _, _, d in G.edges(data=True)]\n",
    "\n",
    "    # Draw\n",
    "    plt.figure(figsize=(max(10, len(ts)*1.4), max(6, len(seeds)*0.55 + 2)))\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=sizes)\n",
    "    nx.draw_networkx_edges(G, pos, width=widths, arrows=False)\n",
    "\n",
    "    edge_labels = {(u, v): f\"{d['jacc']:.2f}\" for u, v, d in G.edges(data=True)}\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8)\n",
    "    \n",
    "    plt.xticks([i * col_gap for i in range(len(ts))], [str(t) for t in ts])\n",
    "    plt.yticks([])\n",
    "    plt.title(title or f\"Top {len(seeds)} communities from t={ts[0]} â†’ t={ts[-1]} (edge âˆ Jaccard)\")\n",
    "    plt.tight_layout()\n",
    "    if (save_path != None):\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "# ---------- Main wrapper ----------\n",
    "\n",
    "def visualize_topk_from_edges(labels_by_t, wjacc_edges,\n",
    "                              t_start=1, t_end=16, top_k=10,\n",
    "                              node_size_scale=700.0, score_cap = 0.5,\n",
    "                              edge_width_scale=8.0, title=None, save_path = None):\n",
    "    ts = sorted(labels_by_t.keys())\n",
    "    if t_start not in labels_by_t:\n",
    "        raise ValueError(f\"Missing labels for t={t_start}\")\n",
    "\n",
    "    # Top-k seeds from t_start\n",
    "    seeds, _ = topk_at_t1(labels_by_t[t_start], k=top_k)\n",
    "\n",
    "    # Track forward using provided Jaccard edges\n",
    "    wj_lookup = build_edge_lookup(wjacc_edges)\n",
    "    paths, edges = track_paths(seeds, wj_lookup, ts)\n",
    "\n",
    "    # Compute community sizes (unweighted)\n",
    "    node_sizes = node_sizes_for_paths(labels_by_t, paths)\n",
    "\n",
    "    # Draw the layered visualization\n",
    "    print(ts)\n",
    "    draw_layered_paths(paths, edges, node_sizes, ts,\n",
    "                       node_size_scale=node_size_scale,\n",
    "                       edge_width_scale=edge_width_scale,\n",
    "                       title=title or f\"Top-{top_k} from t={t_start} â†’ t={t_end}\", score_cap = score_cap,\n",
    "                       save_path = save_path)\n",
    "\n",
    "    return paths, edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adc4df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load variables:\n",
    "# DATA_DIRECTORY = OUTPUT_DIRECTORY + \"leiden_result_variables_temp\"\n",
    "# with open(f\"{DATA_DIRECTORY}/label.pkl\", \"rb\") as f:\n",
    "#     label_list = pickle.load(f)\n",
    "# with open(f\"{DATA_DIRECTORY}/score.pkl\", \"rb\") as f:\n",
    "#     score_list = pickle.load(f)\n",
    "# with open(f\"{DATA_DIRECTORY}/graph.pkl\", \"rb\") as f:\n",
    "#     graph_list = pickle.load(f)\n",
    "# with open(f\"{DATA_DIRECTORY}/communities.pkl\", \"rb\") as f:\n",
    "#     communities_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b26217",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(graph_list))\n",
    "print(len(communities_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afb7162",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = {queue[i][0]: label_list[i] for i in range(len(queue))}\n",
    "wjacc = wjacc_edges_builder(graph_list,communities_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c48802",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels_dict)\n",
    "print(len(labels_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659b5d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wjacc)\n",
    "print(len(wjacc))\n",
    "save_path = f\"../output/{DISEASE}/leiden_tracking_results/leiden_tracking.png\"\n",
    "# save_path = None\n",
    "_, edges = visualize_topk_from_edges(labels_dict, wjacc, 1,24,top_k = 20,score_cap = 0.03,save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac873f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5c3bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = [[i] for i in range(1,25)]\n",
    "x = [T[0] for T in queue]\n",
    "y1 = score_list\n",
    "y2 = [len(communities_list[i]) for i in range(len(communities_list))]\n",
    "\n",
    "# Create a figure with 2 subplots (1 row, 2 columns)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Left graph\n",
    "axes[0].plot(x, y1, color='r')\n",
    "axes[0].set_title(\"quality score\")\n",
    "axes[0].set_xlabel(\"x\")\n",
    "axes[0].set_ylabel(\"quality_score\")\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Right graph\n",
    "axes[1].plot(x, y2, color='b')\n",
    "axes[1].set_title(\"community size\")\n",
    "axes[1].set_xlabel(\"x\")\n",
    "axes[1].set_ylabel(\"community size\")\n",
    "axes[1].grid(True)\n",
    "\n",
    "# Adjust spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d64ef50",
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_time = 1 / (1-eigenvalues[1])\n",
    "print(f\"Mixed Time: {mixed_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d9886b",
   "metadata": {},
   "source": [
    "# Final Average DDBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2323e2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Average Clustering\n",
    "average_t = [2,4,6,8,10,12]\n",
    "pdf_path = f\"../output/{DISEASE}/leiden_report.pdf\"\n",
    "method = \"modularity\"\n",
    "num_neighbors = 100\n",
    "resolution = 1.0\n",
    "label_list = []\n",
    "score_list = []\n",
    "graph_list = []\n",
    "communities_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f6e6954f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 107278214 stored elements and shape (26755, 26755)>\n",
      "  Coords\tValues\n",
      "  (0, 0)\t0.0035975759605646625\n",
      "  (0, 3)\t7.82211499604463e-05\n",
      "  (0, 20)\t0.0020964119994109705\n",
      "  (0, 31)\t0.0022703416667553124\n",
      "  (0, 35)\t0.0007699141610643613\n",
      "  (0, 36)\t0.0009730592197960951\n",
      "  (0, 38)\t0.0007955519059364313\n",
      "  (0, 47)\t0.00232262024143416\n",
      "  (0, 61)\t0.00162654973268523\n",
      "  (0, 63)\t0.0014869348608969408\n",
      "  (0, 74)\t0.0006730228550034888\n",
      "  (0, 82)\t0.0007556246177634382\n",
      "  (0, 85)\t0.001774035244748914\n",
      "  (0, 92)\t0.002528320157007392\n",
      "  (0, 96)\t0.000727920683698572\n",
      "  (0, 97)\t0.0020805114570988372\n",
      "  (0, 113)\t0.0019291117096864683\n",
      "  (0, 118)\t0.0007955519059364313\n",
      "  (0, 119)\t9.310094367649438e-05\n",
      "  (0, 122)\t7.072555260867182e-05\n",
      "  (0, 127)\t0.00016824086189720004\n",
      "  (0, 128)\t0.00011989112589680303\n",
      "  (0, 132)\t0.0023674742774648185\n",
      "  (0, 134)\t0.0013201894286087967\n",
      "  (0, 140)\t0.0008344725964602233\n",
      "  :\t:\n",
      "  (26754, 21609)\t1.888275402950826e-05\n",
      "  (26754, 21752)\t4.5948950127341795e-05\n",
      "  (26754, 22014)\t3.236223956773775e-05\n",
      "  (26754, 22361)\t2.2441588914200576e-05\n",
      "  (26754, 22491)\t1.6374200344816376e-05\n",
      "  (26754, 22616)\t2.578375845327296e-05\n",
      "  (26754, 23211)\t2.113830817189946e-05\n",
      "  (26754, 23384)\t9.338455385961685e-06\n",
      "  (26754, 23564)\t4.770433940307422e-05\n",
      "  (26754, 23625)\t1.7476212312979636e-05\n",
      "  (26754, 23870)\t4.71534957846864e-05\n",
      "  (26754, 24270)\t2.7873521347235418e-05\n",
      "  (26754, 24327)\t1.9507004292286797e-05\n",
      "  (26754, 24549)\t3.0399724649559713e-05\n",
      "  (26754, 24763)\t4.190351746889234e-05\n",
      "  (26754, 24948)\t9.654022452055117e-05\n",
      "  (26754, 25284)\t6.59390158426415e-05\n",
      "  (26754, 25337)\t5.3976206903900676e-05\n",
      "  (26754, 25486)\t0.00015305246686744389\n",
      "  (26754, 25832)\t9.651004463237438e-05\n",
      "  (26754, 26049)\t9.651004463237438e-05\n",
      "  (26754, 26053)\t2.584296846946814e-05\n",
      "  (26754, 26165)\t4.139337175584142e-05\n",
      "  (26754, 26699)\t9.651004463237438e-05\n",
      "  (26754, 26754)\t0.00015305246686744389\n"
     ]
    }
   ],
   "source": [
    "print(multilayer_transition_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d6ca7dee",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ddm \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../output/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mDISEASE\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/diffusion_dist_matrices/ddm_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43maverage_t\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_res-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mresolution\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_itp-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43minterlayer_transition_prob\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\celem\\Research\\DGI-Hypergraph-Fin\\.venv\\lib\\site-packages\\numpy\\lib\\npyio.py:456\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    453\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mopen_memmap(file, mode\u001b[38;5;241m=\u001b[39mmmap_mode,\n\u001b[0;32m    454\u001b[0m                                   max_header_size\u001b[38;5;241m=\u001b[39mmax_header_size)\n\u001b[0;32m    455\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 456\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;66;03m# Try a pickle\u001b[39;00m\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n",
      "File \u001b[1;32mc:\\Users\\celem\\Research\\DGI-Hypergraph-Fin\\.venv\\lib\\site-packages\\numpy\\lib\\format.py:809\u001b[0m, in \u001b[0;36mread_array\u001b[1;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isfileobj(fp):\n\u001b[0;32m    808\u001b[0m         \u001b[38;5;66;03m# We can use the fast fromfile() function.\u001b[39;00m\n\u001b[1;32m--> 809\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    810\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    811\u001b[0m         \u001b[38;5;66;03m# This is not a real file. We have to read it the\u001b[39;00m\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;66;03m# memory-intensive way.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[38;5;66;03m# not correctly instantiate zero-width string dtypes; see\u001b[39;00m\n\u001b[0;32m    821\u001b[0m         \u001b[38;5;66;03m# https://github.com/numpy/numpy/pull/6430\u001b[39;00m\n\u001b[0;32m    822\u001b[0m         array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mndarray(count, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ddm = np.load(f\"../output/{DISEASE}/diffusion_dist_matrices/ddm_{average_t}_res-{resolution}_itp-{interlayer_transition_prob}.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c35dacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, score, communities, graph = DDBC(eigenvalues, eigenvectors, num_neighbors, resolution, average_t, leiden_method = method)\n",
    "path = community_report_onepage(labels, score, out_pdf=pdf_path,\n",
    "                                extra_text=[f\"method={method}\", f\"resolution={resolution}\"], \n",
    "                                bins=100,time_steps = average_t)\n",
    "print(\"Wrote:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1f31d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "del ddm\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c75928",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_array_and_sparse_sizes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79007c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(communities)\n",
    "DATA_DIRECTORY = OUTPUT_DIRECTORY + \"leiden_results\"\n",
    "with open(f\"{DATA_DIRECTORY}/result_communities.pkl\", \"wb\") as f:\n",
    "    pickle.dump(communities, f)\n",
    "with open(f\"{DATA_DIRECTORY}/result_graph.pkl\", \"wb\") as f:\n",
    "    pickle.dump(graph, f)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cacbd1",
   "metadata": {},
   "source": [
    "# Robustness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e34bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddm_test = build_diffusion_dist_matrix_avg(eigenvalues, eigenvectors,[2,4,6,8,10,12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "fee4380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddm_test = ddm_test.astype(np.float32, copy = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "31ee0119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del multilayer_transition_matrix\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e8cce8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix,_ = build_aggregated_kNN(ddm_test, k = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de425fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_original,_,_ = leiden_from_knn_adjacency(similarity_matrix, method=\"modularity\",resolution=1.0,n_iterations=-1,seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "816a5fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [09:03<00:00, 10.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median ARI: 0.7982598551830488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_boots = 50\n",
    "ari_scores = []\n",
    "\n",
    "for b in tqdm(range(n_boots)):\n",
    "    # 1. Sample genes with replacement\n",
    "    n = similarity_matrix.shape[0]\n",
    "    idx = np.random.choice(np.arange(n), size=n, replace=True)\n",
    "    sim_resampled = similarity_matrix[idx, :][:, idx]\n",
    "    \n",
    "    # 2. Recompute similarity and cluster\n",
    "    labels_b, _, _ = leiden_from_knn_adjacency(sim_resampled, method=\"modularity\",resolution=1.0,n_iterations=-1,seed=42)\n",
    "    \n",
    "    # 3. Compare with original clustering\n",
    "    ari = adjusted_rand_score(labels_original[idx], labels_b)\n",
    "    ari_scores.append(ari)\n",
    "\n",
    "print(\"Median ARI:\", np.median(ari_scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
