{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e930760",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6b4cc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from scipy.sparse import load_npz,save_npz,diags,csr_matrix\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "import os\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from pypdf import PdfReader, PdfWriter\n",
    "from tempfile import NamedTemporaryFile\n",
    "import networkx as nx\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import community as community_louvain\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import gc\n",
    "from pympler import muppy, asizeof"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a2f0ed",
   "metadata": {},
   "source": [
    "# Building Multilayer Transition Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4778f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the matrices needed\n",
    "DISEASE = \"BIPOLAR\"\n",
    "OUTPUT_DIRECTORY = f\"../output/{DISEASE}/\"\n",
    "DGIDB_DIRECTORY = f\"../../Gen_Hypergraph/output/DGIDB_{DISEASE}/\"\n",
    "MSIGDB_DIRECTORY = \"../../Gen_Hypergraph/output/MSigDB_Full/\"\n",
    "\n",
    "## DGIDB\n",
    "DGIDB_binary_matrix = load_npz(DGIDB_DIRECTORY + \"hypergraph_incidence_matrix_binary.npz\")\n",
    "DGIDB_weighted_matrix = load_npz(DGIDB_DIRECTORY + \"hypergraph_incidence_matrix_weighted.npz\")\n",
    "DGIDB_diag_gene_weight_matrix = load_npz(DGIDB_DIRECTORY + \"diag_gene_weight_matrix.npz\")\n",
    "DGIDB_diag_node_degree_matrix = load_npz(DGIDB_DIRECTORY + \"diag_node_degree_matrix.npz\")\n",
    "DGIDB_inverse_diag_edge_degree_matrix = load_npz(DGIDB_DIRECTORY + \"inverse_diag_edge_degree_matrix.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51bde8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# queue = [[i] for i in range(1,25)]\n",
    "# resolution = 1.0\n",
    "# for T in queue:\n",
    "#     path = f\"../output/{DISEASE}/diffusion_dist_matrices/ddm_{T}_res-{resolution}.npy\"\n",
    "#     arr = np.load(path)\n",
    "#     arr = arr.astype(np.float32)\n",
    "#     np.save(path, arr)\n",
    "#     print(f\"Overwritten (float32): {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfd6acb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful Functions\n",
    "def csr_equal_tol(A, B, atol=1e-8):\n",
    "    # First check shapes and sparsity pattern\n",
    "    if A.shape != B.shape or not np.array_equal(A.indptr, B.indptr) or not np.array_equal(A.indices, B.indices):\n",
    "        return False\n",
    "    # Compare numeric values within tolerance\n",
    "    return np.allclose(A.data, B.data, atol=atol, rtol=0)\n",
    "\n",
    "def is_symmetric(W,tol = 1e-8):\n",
    "    diff = (W - W.T)\n",
    "    check = np.all(np.abs(diff.data) < tol)\n",
    "    return check\n",
    "\n",
    "def degree_array(W, a=1):\n",
    "    return np.asarray(W.sum(axis=a)).ravel()\n",
    "\n",
    "def degree_diagonal_matrix(W, a=1):\n",
    "    d = degree_array(W,a)\n",
    "    return sp.diags(d, offsets=0, format='csr')\n",
    "\n",
    "def symmetrically_normalize(W, a=1):\n",
    "    D = np.asarray(W.sum(axis=a)).ravel()\n",
    "    D_inv_sqrt = np.zeros_like(D)\n",
    "    nze = D != 0\n",
    "    D_inv_sqrt[nze] = 1 / np.sqrt(D[nze])\n",
    "\n",
    "    W_sym = W.multiply(D_inv_sqrt)              # scale columns\n",
    "    W_sym = W_sym.multiply(D_inv_sqrt[:, None]) # scale rows    \n",
    "    return W_sym.tocsr()\n",
    "\n",
    "def list_array_and_sparse_sizes(min_mb=1, include_locals=True, top=None):\n",
    "    \"\"\"\n",
    "    List NumPy arrays and SciPy sparse matrices in memory, ordered by size.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    min_mb : float, optional\n",
    "        Minimum size (in MB) to include in the listing. Default is 1 MB.\n",
    "    include_locals : bool, optional\n",
    "        Whether to include local variables in addition to globals. Default True.\n",
    "    top : int, optional\n",
    "        If given, show only the top N largest variables.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - This only scans for numpy.ndarray and scipy.sparse matrices.\n",
    "    - Avoids traversing other Jupyter internals (no traitlets warnings).\n",
    "    \"\"\"\n",
    "    # snapshot current namespace to avoid \"dictionary changed size\" errors\n",
    "    ns = dict(globals())\n",
    "    if include_locals:\n",
    "        ns.update(dict(locals()))\n",
    "\n",
    "    rows = []\n",
    "    for name, obj in ns.items():\n",
    "        if name.startswith(\"_\"):\n",
    "            continue\n",
    "        # Only handle ndarrays and sparse matrices\n",
    "        if not (isinstance(obj, np.ndarray) or sp.issparse(obj)):\n",
    "            continue\n",
    "        try:\n",
    "            size_b = asizeof.asizeof(obj)\n",
    "        except Exception:\n",
    "            continue\n",
    "        if size_b < min_mb * 1e6:\n",
    "            continue\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            kind = f\"ndarray {obj.shape} {obj.dtype}\"\n",
    "        elif sp.issparse(obj):\n",
    "            kind = f\"{type(obj).__name__} nnz={obj.nnz} shape={obj.shape}\"\n",
    "        else:\n",
    "            kind = type(obj).__name__\n",
    "        rows.append((name, kind, size_b))\n",
    "\n",
    "    # Sort descending by size\n",
    "    rows.sort(key=lambda r: r[2], reverse=True)\n",
    "    total = sum(s for *_, s in rows) or 1\n",
    "\n",
    "    # Print nicely formatted table\n",
    "    print(f\"{'Variable':<28}{'Type/Shape':<48}{'Size (MB)':>12}{'% of total':>12}\")\n",
    "    print(\"-\" * 100)\n",
    "    for i, (name, kind, size_b) in enumerate(rows):\n",
    "        if top and i >= top:\n",
    "            break\n",
    "        print(f\"{name:<28}{kind:<48}{size_b/1e6:12.2f}{100*size_b/total:12.1f}\")\n",
    "\n",
    "    print(f\"\\nTotal memory in arrays/sparse: {total/1e6:.2f} MB ({total/1e9:.2f} GB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd2b479c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Adjacency Matrices\n",
    "## DGIDB\n",
    "H, W_v, D_v, D_e_inv = DGIDB_weighted_matrix, DGIDB_diag_gene_weight_matrix, DGIDB_diag_node_degree_matrix, DGIDB_inverse_diag_edge_degree_matrix\n",
    "\n",
    "# Construct D_v^(-1/2)\n",
    "d = (D_v @ W_v).diagonal()\n",
    "d_inv_sqrt = np.zeros_like(d)\n",
    "nonzero_mask = d > 0\n",
    "d_inv_sqrt[nonzero_mask] = 1.0 / np.sqrt(d[nonzero_mask])\n",
    "D_v_sqrt_inv = diags(d_inv_sqrt)\n",
    "\n",
    "DGIDB_adjacency_matrix = D_v_sqrt_inv @ H @ D_e_inv @ H.T @ D_v_sqrt_inv\n",
    "\n",
    "# Compute Degree Diagonal Matrices\n",
    "DGIDB_rows_sums = degree_array(DGIDB_adjacency_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4400859f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(is_symmetric(DGIDB_adjacency_matrix,tol = 1e-7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75814a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Symmetric Normalization\n",
    "DGIDB_adjacency_matrix = symmetrically_normalize(DGIDB_adjacency_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adc7945f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(is_symmetric(DGIDB_adjacency_matrix,tol = 1e-7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3361ac03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero row indices: []\n",
      "Number of zero rows: 0\n"
     ]
    }
   ],
   "source": [
    "row_sums = np.sum(np.abs(DGIDB_adjacency_matrix), axis=1)\n",
    "\n",
    "# Indices of zero rows\n",
    "zero_row_indices = np.where(row_sums == 0)[0]\n",
    "\n",
    "# Count\n",
    "num_zero_rows = len(zero_row_indices)\n",
    "\n",
    "print(\"Zero row indices:\", zero_row_indices)\n",
    "print(\"Number of zero rows:\", num_zero_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e19d107b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# density = DGIDB_adjacency_matrix.nnz / (DGIDB_adjacency_matrix.shape[0] * DGIDB_adjacency_matrix.shape[1])\n",
    "# print(\"DGIDB Density:\", density)\n",
    "# density = MSIGDB_adjacency_matrix.nnz / (MSIGDB_adjacency_matrix.shape[0] * MSIGDB_adjacency_matrix.shape[1])\n",
    "# print(\"MSIGDB Density:\", density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c664dcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check nonzero average\n",
    "DGIDB_nonzero_average = DGIDB_adjacency_matrix[DGIDB_adjacency_matrix != 0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3b6b28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Density normalization version 1\n",
    "# target_average = MSIGDB_nonzero_average\n",
    "# DGIDB_adjacency_matrix = (target_average / DGIDB_nonzero_average) * DGIDB_adjacency_matrix\n",
    "# MSIGDB_adjacency_matrix = (target_average / MSIGDB_nonzero_average) * MSIGDB_adjacency_matrix\n",
    "\n",
    "# DGIDB_nonzero_average = DGIDB_adjacency_matrix[DGIDB_adjacency_matrix != 0].mean()\n",
    "# MSIGDB_nonzero_average = MSIGDB_adjacency_matrix[MSIGDB_adjacency_matrix != 0].mean()\n",
    "\n",
    "# print(DGIDB_nonzero_average,MSIGDB_nonzero_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21754911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Density normalization version 2\n",
    "# DGIDB_mean_degree = DGIDB_rows_sums[DGIDB_rows_sums != 0].mean()\n",
    "# MSIGDB_mean_degree = MSIGDB_rows_sums[MSIGDB_rows_sums != 0].mean()\n",
    "# print(DGIDB_mean_degree,MSIGDB_mean_degree)\n",
    "# # print(DGIDB_rows_sums,MSIGDB_rows_sums)\n",
    "\n",
    "# DGIDB_weight = (1/DGIDB_mean_degree) / ((1/DGIDB_mean_degree)+(1/MSIGDB_mean_degree))\n",
    "# MSIGDB_weight = (1/MSIGDB_mean_degree) / ((1/DGIDB_mean_degree)+(1/MSIGDB_mean_degree))\n",
    "# geo_mean_weight = (DGIDB_weight * MSIGDB_weight)**(1/2)\n",
    "# print(DGIDB_weight,MSIGDB_weight)\n",
    "# print(geo_mean_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23487d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Row-Normalization\n",
    "# # DGIDB\n",
    "# row_sums = np.array(DGIDB_adjacency_matrix.sum(axis=1)).ravel()\n",
    "# # inverse row sums (avoid division by zero)\n",
    "# inv_row_sums = np.reciprocal(row_sums, where=row_sums!=0)\n",
    "# # build diagonal matrix of inverses\n",
    "# D_inv = sp.diags(inv_row_sums)\n",
    "# DGIDB_adjacency_matrix = D_inv @ DGIDB_adjacency_matrix\n",
    "\n",
    "# # MSIGDB\n",
    "# row_sums = np.array(MSIGDB_adjacency_matrix.sum(axis=1)).ravel()\n",
    "# # inverse row sums (avoid division by zero)\n",
    "# inv_row_sums = np.reciprocal(row_sums, where=row_sums!=0)\n",
    "# # build diagonal matrix of inverses\n",
    "# D_inv = sp.diags(inv_row_sums)\n",
    "# MSIGDB_adjacency_matrix = D_inv @ MSIGDB_adjacency_matrix\n",
    "\n",
    "# # Coupling Matrix\n",
    "# row_sums = interlayer_transition_matrix.sum(axis = 1, keepdims= True)\n",
    "# row_sums[row_sums == 0] = 1 \n",
    "# interlayer_transition_matrix = interlayer_transition_matrix / row_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6a3f498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check for Row-stochastic\n",
    "# row_sums = np.array(DGIDB_adjacency_matrix.sum(axis=1)).ravel()\n",
    "# print(row_sums)\n",
    "# ok = np.all(np.isclose(row_sums, 1.0)|np.isclose(row_sums, 0))\n",
    "# print(\"Every row sums to 0 or 1?\", ok)\n",
    "\n",
    "# row_sums = np.array(MSIGDB_adjacency_matrix.sum(axis=1)).ravel()\n",
    "# print(row_sums)\n",
    "# ok = np.all(np.isclose(row_sums, 1.0)|np.isclose(row_sums, 0))\n",
    "# print(\"Every row sums to 0 or 1?\", ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "197f1b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Build the multilayer transition matrix\n",
    "# # interlayer_transition_prob = target_average\n",
    "# interlayer_transition_prob = 0.5\n",
    "\n",
    "# A = (1-interlayer_transition_prob) * DGIDB_weight * DGIDB_adjacency_matrix\n",
    "# B = interlayer_transition_prob * geo_mean_weight * interlayer_transition_matrix.T\n",
    "# C = interlayer_transition_prob * geo_mean_weight * interlayer_transition_matrix\n",
    "# D =(1-interlayer_transition_prob) * MSIGDB_weight * MSIGDB_adjacency_matrix\n",
    "\n",
    "# multilayer_transition_matrix = sp.bmat([\n",
    "#     [A, B],\n",
    "#     [C, D]\n",
    "# ]).tocsr()\n",
    "\n",
    "num_genes = DGIDB_adjacency_matrix.shape[0]\n",
    "\n",
    "# del A,B,C,D, DGIDB_adjacency_matrix,MSIGDB_adjacency_matrix\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56ceac7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float32'\n",
      "\twith 26927 stored elements and shape (359, 359)>\n",
      "  Coords\tValues\n",
      "  (0, 0)\t0.0008369960123673081\n",
      "  (0, 2)\t0.0009434158564545214\n",
      "  (0, 4)\t0.0007029924308881164\n",
      "  (0, 6)\t0.0011968211038038135\n",
      "  (0, 12)\t0.00020963586575817317\n",
      "  (0, 15)\t0.00013345778279472142\n",
      "  (0, 16)\t0.0007854874129407108\n",
      "  (0, 29)\t0.0029654873069375753\n",
      "  (0, 32)\t0.0001371209800709039\n",
      "  (0, 34)\t0.00015186805103439838\n",
      "  (0, 50)\t0.0011430479353293777\n",
      "  (0, 58)\t0.0014151954092085361\n",
      "  (0, 66)\t0.002317107981070876\n",
      "  (0, 70)\t0.0010352556128054857\n",
      "  (0, 74)\t0.0031226081773638725\n",
      "  (0, 83)\t0.0033895024098455906\n",
      "  (0, 88)\t0.0010350828524678946\n",
      "  (0, 90)\t0.0009474997641518712\n",
      "  (0, 94)\t0.0008779761265031993\n",
      "  (0, 111)\t0.00020286427752580494\n",
      "  (0, 112)\t0.0007632774650119245\n",
      "  (0, 114)\t0.00020286427752580494\n",
      "  (0, 128)\t0.0002008546725846827\n",
      "  (0, 141)\t0.0013963169185444713\n",
      "  (0, 162)\t0.0012974548153579235\n",
      "  :\t:\n",
      "  (358, 158)\t0.009568463079631329\n",
      "  (358, 162)\t0.005123235750943422\n",
      "  (358, 176)\t0.005332851782441139\n",
      "  (358, 183)\t0.011801895685493946\n",
      "  (358, 213)\t0.028648991137742996\n",
      "  (358, 227)\t0.0006992617272771895\n",
      "  (358, 242)\t0.024878477677702904\n",
      "  (358, 244)\t0.00676364079117775\n",
      "  (358, 274)\t0.020187970250844955\n",
      "  (358, 280)\t0.002421281998977065\n",
      "  (358, 281)\t0.024152060970664024\n",
      "  (358, 282)\t0.02427312731742859\n",
      "  (358, 288)\t0.0061764344573020935\n",
      "  (358, 299)\t0.012348798103630543\n",
      "  (358, 335)\t0.002421281998977065\n",
      "  (358, 341)\t0.002421281998977065\n",
      "  (358, 342)\t0.002421281998977065\n",
      "  (358, 343)\t0.022067731246352196\n",
      "  (358, 344)\t0.024964043870568275\n",
      "  (358, 350)\t0.03026166372001171\n",
      "  (358, 354)\t0.023966971784830093\n",
      "  (358, 355)\t0.002421281998977065\n",
      "  (358, 356)\t0.027016445994377136\n",
      "  (358, 357)\t0.02139822579920292\n",
      "  (358, 358)\t0.015771156176924706\n"
     ]
    }
   ],
   "source": [
    "print(DGIDB_adjacency_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd505c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26927\n"
     ]
    }
   ],
   "source": [
    "print(DGIDB_adjacency_matrix.nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cba8e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Row-normalize the multilayer transition matrix\n",
    "# row_sums = np.array(multilayer_transition_matrix.sum(axis=1)).ravel()\n",
    "# nonzero_rows = row_sums != 0\n",
    "# inv_row_sums = np.zeros_like(row_sums)\n",
    "# inv_row_sums[nonzero_rows] = 1.0 / row_sums[nonzero_rows]\n",
    "# multilayer_transition_matrix = diags(inv_row_sums) @ multilayer_transition_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1a99a2",
   "metadata": {},
   "source": [
    "# Computing Eigenvalues & Eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb638006",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_eigenvalues = 100\n",
    "num_neighbors = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4583a678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute top k eigenvalues by magnitude\n",
    "def top_k_eigenvalues(M,k):\n",
    "    vals, vecs = eigsh(M, k=k, which='LM')  # 'LM' = Largest Magnitude\n",
    "    idx = np.argsort(np.abs(vals))[::-1]\n",
    "    vals, vecs = vals[idx], vecs[:, idx]\n",
    "    return vals, vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c2a1ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues: [ 1.0000000e+00  9.0301019e-01  8.8085163e-01  8.2819939e-01\n",
      "  8.2655489e-01  8.1692380e-01  7.7282578e-01  7.5342405e-01\n",
      "  6.7450184e-01  6.3300347e-01  5.9870207e-01  5.5573106e-01\n",
      "  5.1420707e-01  4.8995283e-01  3.4668750e-01  2.3337238e-01\n",
      "  7.1868719e-08  6.8078322e-08 -5.3712011e-08 -4.0803922e-08\n",
      "  3.6205869e-08 -3.5784343e-08 -3.4883172e-08  3.4731805e-08\n",
      "  3.3450426e-08 -3.3353611e-08  3.3317708e-08  3.3214015e-08\n",
      " -3.2825113e-08  3.2559662e-08 -3.2308574e-08 -3.1462150e-08\n",
      "  3.1441356e-08 -3.1221052e-08  3.0854508e-08  3.0500921e-08\n",
      " -3.0245062e-08 -2.9821518e-08  2.9615567e-08 -2.9579036e-08\n",
      "  2.9377327e-08 -2.9148579e-08  2.8794874e-08 -2.8777707e-08\n",
      " -2.8224175e-08 -2.6944114e-08  2.6853042e-08  2.6821985e-08\n",
      " -2.6729310e-08  2.6285621e-08 -2.5735581e-08 -2.5455311e-08\n",
      "  2.5451385e-08  2.4930351e-08  2.4848609e-08  2.4467749e-08\n",
      " -2.4088925e-08 -2.4024590e-08  2.3730854e-08 -2.3550603e-08\n",
      "  2.3276632e-08 -2.2978980e-08  2.2839089e-08 -2.2634707e-08\n",
      "  2.2399243e-08 -2.2144755e-08  2.2088605e-08  2.1832777e-08\n",
      "  2.1771585e-08 -2.1764430e-08 -2.1761835e-08 -2.1752452e-08\n",
      " -2.1431951e-08 -2.1166841e-08  2.1086334e-08  2.1072768e-08\n",
      " -2.1041911e-08  2.0728955e-08  2.0673566e-08 -2.0537259e-08\n",
      "  2.0320446e-08 -2.0153520e-08 -1.9721226e-08  1.9699966e-08\n",
      "  1.9620854e-08 -1.9384666e-08  1.9235271e-08 -1.9184910e-08\n",
      "  1.8948757e-08 -1.8895676e-08  1.8587537e-08  1.8206988e-08\n",
      " -1.7828315e-08  1.7594203e-08 -1.7536655e-08  1.7433180e-08\n",
      " -1.7182689e-08 -1.7172962e-08  1.7156326e-08 -1.6900465e-08]\n",
      "Eigenvectors shape: (359, 100)\n"
     ]
    }
   ],
   "source": [
    "# Compute eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = top_k_eigenvalues(DGIDB_adjacency_matrix,num_eigenvalues)\n",
    "print(\"Eigenvalues:\", eigenvalues)\n",
    "print(\"Eigenvectors shape:\", eigenvectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdb0442",
   "metadata": {},
   "source": [
    "# KNN Graph Methods & Testing (commented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ed288ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build diffusion matrix from eigenvalues and eigenvectors (double check)\n",
    "def build_diffusion_dist_matrix(vals,vecs,t):\n",
    "    X_t = vecs * (vals ** (t))\n",
    "    D_t2 = squareform(pdist(X_t, metric='sqeuclidean'))\n",
    "    return D_t2\n",
    "\n",
    "# Build kNN\n",
    "def build_kNN(diffusion_dist_matrix,k, sym_method='average'):\n",
    "    # Choose sigma to be the median of pair-wise distance\n",
    "    tmp = diffusion_dist_matrix.astype(np.float32, copy=True)\n",
    "    np.sqrt(tmp, out=tmp)\n",
    "    sigma = np.median(tmp[tmp != 0], overwrite_input=True)\n",
    "    print(f\"{sigma}\")\n",
    "\n",
    "    n = diffusion_dist_matrix.shape[0]\n",
    "\n",
    "    # Used to indicate position of nonzero value needed to record (constructing a sparse matrix for efficiency)\n",
    "    rows, cols, vals = [], [], []\n",
    "    for i in range(n):\n",
    "        profile = np.exp(-diffusion_dist_matrix[i] / (2* (sigma**2))) \n",
    "        idx = np.argpartition(-profile, k+1)[:k+1]  # top-k+1 (includes self)\n",
    "        idx = idx[idx != i]                      # drop self\n",
    "        rows += [i]*k\n",
    "        cols += list(idx[:k])\n",
    "        vals += list(profile[idx[:k]])\n",
    "    adj_mat = csr_matrix((vals, (rows, cols)), shape=(n, n))\n",
    "\n",
    "    # Symmetrize the matrix by adding edges to one way edges and set weight to average\n",
    "    if (sym_method == 'average'):\n",
    "        adj_mat = (adj_mat + adj_mat.T).multiply(0.5).tocsr()\n",
    "\n",
    "    # Symmetrize the matrix by unioning\n",
    "    elif (sym_method == 'union'):\n",
    "        adj_mat = adj_mat.maximum(adj_mat.T)\n",
    "    elif (sym_method == 'none'):\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(\"sym_method must be 'average' or 'union'\")\n",
    "    \n",
    "    kNN_graph = nx.from_numpy_array(adj_mat)\n",
    "\n",
    "    \n",
    "    return adj_mat,kNN_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "996f9baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddm = build_diffusion_dist_matrix(eigenvalues, eigenvectors, [2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "235a4651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn_orig,_ = build_kNN(ddm,num_neighbors)\n",
    "# knn_new,_,knn_sus = build_aggregated_kNN(ddm,num_neighbors)\n",
    "# csr_equal_tol(knn_orig,knn_sus)\n",
    "# print(is_symmetric(knn_orig),is_symmetric(knn_new))\n",
    "# print(knn_orig)\n",
    "# print(knn_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de4a026f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(csr_diff_metrics(knn_orig, knn_new,use_top_left_overlap=True))\n",
    "# print(knn_orig.shape, knn_new.shape)\n",
    "# print(csr_diff_metrics(knn_orig[num_genes_dgidb:, num_genes_dgidb:], knn_new,use_top_left_overlap=True))\n",
    "# print(knn_orig[num_genes_dgidb:, num_genes_dgidb:].shape, knn_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6435413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# density_orig = knn_orig.nnz / (knn_orig.shape[0] * knn_orig.shape[1])\n",
    "# print(density_orig)\n",
    "# density_new = knn_new.nnz / (knn_new.shape[0] * knn_new.shape[1])\n",
    "# print(density_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9797c469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(knn_orig.nnz, knn_new.nnz)\n",
    "# print(knn_orig.data.mean(), knn_new.data.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "57c0350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del ddm, knn_orig, knn_sus, knn_new\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f669ca",
   "metadata": {},
   "source": [
    "# Clustering Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "33e30836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Louvain\n",
    "# def louvain_from_adj(A, resolution=1.0, random_state=0, keep_lcc=False):\n",
    "#     \"\"\"\n",
    "#     A: symmetric, non-negative scipy.sparse adjacency (CSR preferred).\n",
    "#     Returns: labels (np.ndarray of length n), graph G (NetworkX), node_order\n",
    "#     \"\"\"\n",
    "#     # if not sp.isspmatrix(A):  # allow dense but convert\n",
    "#     #     A = sp.csr_matrix(A)\n",
    "#     # # Optional: ensure symmetry numerically\n",
    "#     # if (A - A.T).nnz != 0:\n",
    "#     #     raise ValueError(\"Adjacency must be symmetric. Symmetrize first.\")\n",
    "\n",
    "#     # Build graph\n",
    "#     # networkx >=3.0: from_scipy_sparse_array; older: from_scipy_sparse_matrix\n",
    "#     G = nx.from_scipy_sparse_array(A, edge_attribute='weight')  # undirected by default\n",
    "\n",
    "#     if keep_lcc:\n",
    "#         # keep only the largest connected component if you prefer\n",
    "#         largest_cc = max(nx.connected_components(G), key=len)\n",
    "#         G = G.subgraph(largest_cc).copy()\n",
    "\n",
    "#     # Run Louvain\n",
    "#     part = community_louvain.best_partition(\n",
    "#         G, weight='weight', resolution=resolution, random_state=random_state\n",
    "#     )\n",
    "#     node_order = sorted(G.nodes())\n",
    "#     labels = np.array([part[i] for i in node_order], dtype=int)\n",
    "#     return labels, G, node_order\n",
    "\n",
    "# def DDBC(MTM,num_eigenvalues,num_neighbors,resolution,T):\n",
    "#     vals,vecs = top_k_eigenvalues(M = MTM, k = num_eigenvalues)\n",
    "\n",
    "#     diffusion_dist_matrix = np.empty((num_genes,num_genes))\n",
    "#     for t in T:\n",
    "#         np.add(diffusion_dist_matrix, \n",
    "#                build_diffusion_dist_matrix(vals,vecs,t,num_eigenvalues), \n",
    "#                out=diffusion_dist_matrix)\n",
    "#     diffusion_dist_matrix = diffusion_dist_matrix / len(T)\n",
    "    \n",
    "#     kNN_adjacency_matrix = build_kNN(diffusion_dist_matrix,num_neighbors)\n",
    "\n",
    "#     # # Symmetrize the matrix by adding edges to one way edges and set weight to average\n",
    "#     # kNN_adjacency_matrix = (kNN_adjacency_matrix + kNN_adjacency_matrix.T).multiply(0.5).tocsr()\n",
    "\n",
    "#     # Symmetrize the matrix by unioning\n",
    "#     kNN_adjacency_matrix = kNN_adjacency_matrix.maximum(kNN_adjacency_matrix.T)\n",
    "\n",
    "#     return louvain_from_adj(kNN_adjacency_matrix, resolution = resolution, keep_lcc = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "45534523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leiden_from_knn_adjacency(\n",
    "    A,\n",
    "    *,\n",
    "    method=\"modularity\",      # \"modularity\" | \"rb\" | \"cpm\"\n",
    "    resolution=1.0,           # used by \"rb\" and \"cpm\"\n",
    "    n_iterations=-1,          # -1 => until no improvement\n",
    "    seed=42,\n",
    "    use_weights=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Run Leiden on a symmetric, undirected (weighted) kNN adjacency (SciPy sparse).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    labels : np.ndarray[int]\n",
    "        Community ID per node (0..k-1)\n",
    "    quality : float\n",
    "        Objective value (modularity / RB / CPM depending on 'method')\n",
    "    \"\"\"\n",
    "    if not sp.issparse(A):\n",
    "        raise TypeError(\"A must be a SciPy sparse matrix.\")\n",
    "    # Normalize format & dtype\n",
    "    A = A.tocsr().astype(np.float32, copy=False)\n",
    "\n",
    "    # Clean diagonal and robustly symmetrize\n",
    "    A.setdiag(0.0)\n",
    "    A.eliminate_zeros()\n",
    "    A = A.maximum(A.T)  # keep max weight per undirected edge\n",
    "\n",
    "    # Build igraph from upper triangle (each undirected edge once)\n",
    "    U = sp.triu(A, k=1, format=\"coo\")\n",
    "    n = A.shape[0]\n",
    "    g = ig.Graph(n=n, edges=list(zip(U.row.tolist(), U.col.tolist())), directed=False)\n",
    "    wname = None\n",
    "    if use_weights:\n",
    "        g.es[\"weight\"] = U.data.tolist()\n",
    "        wname = \"weight\"\n",
    "\n",
    "    # Pick partition class + kwargs\n",
    "    m = method.lower()\n",
    "    if m == \"modularity\":\n",
    "        part_cls = la.ModularityVertexPartition\n",
    "        kwargs = dict(weights=wname)\n",
    "    elif m == \"rb\":\n",
    "        part_cls = la.RBConfigurationVertexPartition\n",
    "        kwargs = dict(weights=wname, resolution_parameter=resolution)\n",
    "    elif m == \"cpm\":\n",
    "        part_cls = la.CPMVertexPartition\n",
    "        kwargs = dict(weights=wname, resolution_parameter=resolution)\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'modularity', 'rb', or 'cpm'\")\n",
    "\n",
    "    # Run Leiden\n",
    "    part = la.find_partition(\n",
    "        g,\n",
    "        part_cls,\n",
    "        n_iterations=n_iterations,\n",
    "        seed=seed,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    labels = np.array(part.membership, dtype=np.int32)\n",
    "    communities = [list(c) for c in part]\n",
    "    return labels, float(part.quality()), communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "71564780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DDBC(vals,\n",
    "         vecs,\n",
    "         num_neighbors,\n",
    "         resolution,\n",
    "         T,\n",
    "         leiden_method = \"modularity\",\n",
    "         ddm = None):\n",
    "    if ddm is None:\n",
    "        diffusion_dist_matrix = np.zeros((num_genes,num_genes))\n",
    "        for t in T:\n",
    "            np.add(diffusion_dist_matrix, \n",
    "                build_diffusion_dist_matrix(vals,vecs,t), \n",
    "                out=diffusion_dist_matrix)\n",
    "        diffusion_dist_matrix = diffusion_dist_matrix / len(T)\n",
    "        print(diffusion_dist_matrix)\n",
    "        # np.save(f\"../output/{DISEASE}/diffusion_dist_matrices/ddm_{T}_res-{resolution}_DGIDB.npy\",diffusion_dist_matrix)\n",
    "    else:\n",
    "        diffusion_dist_matrix = ddm\n",
    "    # kNN_adjacency_matrix, kNN_graph = build_kNN(diffusion_dist_matrix,num_neighbors)\n",
    "    kNN_adjacency_matrix, kNN_graph = build_kNN(diffusion_dist_matrix,num_neighbors)\n",
    "\n",
    "    return (*leiden_from_knn_adjacency(kNN_adjacency_matrix,method=leiden_method,resolution=resolution,n_iterations=-1,seed=42), kNN_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "032d1e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def community_report_onepage(labels, score, out_pdf=\"leiden_community_report.pdf\", *,\n",
    "                             extra_text=None, bins=\"auto\", title=\"Community Sizes\", time_steps = \"N/A\"):\n",
    "    \"\"\"\n",
    "    Create ONE PDF page (top: text summary, bottom: histogram).\n",
    "    If out_pdf exists: append this page. Else: create it.\n",
    "\n",
    "    Requires: matplotlib, pypdf  (pip install pypdf)\n",
    "    \"\"\"\n",
    "    # ---- inputs ----\n",
    "    labels = np.asarray(labels)\n",
    "    if labels.ndim != 1:\n",
    "        raise ValueError(\"labels must be a 1-D array of community ids\")\n",
    "\n",
    "    # ---- stats ----\n",
    "    sizes = np.bincount(labels.astype(np.int64, copy=False))\n",
    "    sizes_sorted = np.sort(sizes)\n",
    "    n, k = sizes.sum(), sizes.size\n",
    "\n",
    "    lines = [\n",
    "        \"Leiden Partition Summary\",\n",
    "        \"========================\",\n",
    "        f\"Nodes (n):           {n:,}\",\n",
    "        f\"Time steps:          {time_steps}\",\n",
    "        f\"Communities (k):     {k:,}\",\n",
    "        f\"Size (min):          {int(sizes_sorted[0]) if k else 0:,}\",\n",
    "        f\"Size (median):       {float(np.median(sizes_sorted)) if k else 0.0:.3f}\",\n",
    "        f\"Size (mean):         {float(sizes_sorted.mean()) if k else 0.0:.6f}\",\n",
    "        f\"Size (max):          {int(sizes_sorted[-1]) if k else 0:,}\",\n",
    "        f\"Score:               {score}\",\n",
    "        \"\",\n",
    "        \"Top 10 largest communities (id: size):\",\n",
    "    ]\n",
    "    for cid in np.argsort(-sizes)[:min(10, k)]:\n",
    "        lines.append(f\"  {int(cid):5d}: {int(sizes[cid]):,}\")\n",
    "    if extra_text:\n",
    "        lines += [\"\", \"Extra:\", *([extra_text] if isinstance(extra_text, str) else list(extra_text))]\n",
    "    summary_text = \"\\n\".join(lines)\n",
    "\n",
    "    # ---- draw single-page figure ----\n",
    "    fig = plt.figure(figsize=(8.5, 11), dpi=150)          # US Letter, higher DPI\n",
    "    ax_text = fig.add_axes([0.06, 0.55, 0.88, 0.40])      # [left, bottom, width, height]\n",
    "    ax_text.axis(\"off\")\n",
    "    ax_text.text(0.0, 1.0, summary_text, va=\"top\", ha=\"left\", fontsize=11, family=\"monospace\")\n",
    "\n",
    "    ax_hist = fig.add_axes([0.10, 0.08, 0.80, 0.38])\n",
    "    ax_hist.hist(sizes, bins=bins)\n",
    "    ax_hist.set_xlabel(\"Community size\")\n",
    "    ax_hist.set_ylabel(\"Count of communities\")\n",
    "    ax_hist.set_title(f\"Distribution of {title}\")\n",
    "\n",
    "    # ---- write this page to a temp PDF on disk ----\n",
    "    with NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmpf:\n",
    "        tmp_page = Path(tmpf.name)\n",
    "    with PdfPages(tmp_page) as pdf:\n",
    "        pdf.savefig(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # ---- append/create using PdfReader/PdfWriter (robust across versions) ----\n",
    "    try:\n",
    "        from pypdf import PdfReader, PdfWriter\n",
    "    except Exception as e:\n",
    "        try: os.remove(tmp_page)\n",
    "        except: pass\n",
    "        raise RuntimeError(\"Please install 'pypdf' (e.g., `pip install pypdf`).\") from e\n",
    "\n",
    "    out_pdf = Path(out_pdf)\n",
    "    tmp_out = out_pdf.with_suffix(out_pdf.suffix + \".tmp\")\n",
    "\n",
    "    writer = PdfWriter()\n",
    "\n",
    "    # if existing, copy old pages first\n",
    "    if out_pdf.exists():\n",
    "        with open(out_pdf, \"rb\") as f_exist:\n",
    "            reader = PdfReader(f_exist)\n",
    "            for p in reader.pages:\n",
    "                writer.add_page(p)\n",
    "\n",
    "    # add the new single page\n",
    "    with open(tmp_page, \"rb\") as f_new:\n",
    "        reader_new = PdfReader(f_new)\n",
    "        for p in reader_new.pages:\n",
    "            writer.add_page(p)\n",
    "\n",
    "    # atomic write\n",
    "    with open(tmp_out, \"wb\") as f_out:\n",
    "        writer.write(f_out)\n",
    "    os.replace(tmp_out, out_pdf)\n",
    "\n",
    "    # cleanup\n",
    "    try: os.remove(tmp_page)\n",
    "    except: pass\n",
    "\n",
    "    return out_pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52195a5",
   "metadata": {},
   "source": [
    "# Run DDBC 1 - 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0400590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leiden Clustering\n",
    "pdf_path = f\"../output/{DISEASE}/leiden_report.pdf\"\n",
    "method = \"modularity\"\n",
    "resolution = 1.0\n",
    "num_bins = 100\n",
    "write = True\n",
    "queue = [[i] for i in range(1,21)]\n",
    "label_list = []\n",
    "score_list = []\n",
    "graph_list = []\n",
    "communities_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c240ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for T in queue:\n",
    "    # ddm = np.load(f\"../output/{DISEASE}/diffusion_dist_matrices/ddm_{T}_res-{resolution}.npy\")\n",
    "    labels, score, communities, graph = DDBC(eigenvalues, eigenvectors, num_neighbors, resolution, T, leiden_method = method)\n",
    "    if (write):\n",
    "        path = community_report_onepage(labels, score, out_pdf=pdf_path,\n",
    "                                        extra_text=[f\"method={method}\", f\"resolution={resolution}, DB = DGIDB\"], \n",
    "                                        bins=num_bins,time_steps = T)\n",
    "        print(\"Wrote:\", path)\n",
    "    label_list.append(labels)\n",
    "    score_list.append(score) \n",
    "    graph_list.append(graph)\n",
    "    communities_list.append(communities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957026c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save resulting variables to save time\n",
    "# DATA_DIRECTORY = OUTPUT_DIRECTORY + \"leiden_result_variables_temp\"\n",
    "# with open(f\"{DATA_DIRECTORY}/label.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(label_list, f)\n",
    "# with open(f\"{DATA_DIRECTORY}/score.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(score_list, f)\n",
    "# with open(f\"{DATA_DIRECTORY}/graph.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(graph_list, f)\n",
    "# with open(f\"{DATA_DIRECTORY}/communities.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(communities_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8068439",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a73059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def community_central_genes(G, community_nodes, weight=\"weight\", top_n=20):\n",
    "    C = set(community_nodes)\n",
    "    H = G.subgraph(C).copy()                       # induced subgraph\n",
    "    # within-community (weighted) degree\n",
    "    k = {u: H.degree(u, weight=weight) for u in H}\n",
    "    ks = np.array(list(k.values()), dtype=float)\n",
    "    mu, sigma = ks.mean(), ks.std() if ks.std() > 0 else 1.0\n",
    "    Z = {u: (k[u] - mu)/sigma for u in H}          # within-module degree z-score\n",
    "\n",
    "    # rank by z\n",
    "    ranked = sorted(H.nodes(), key=lambda u: (Z[u]), reverse=True)\n",
    "    return {u : Z[u] for u in ranked[:top_n]}\n",
    "\n",
    "def weighted_jaccard(scoresA, scoresB):\n",
    "    \"\"\"\n",
    "    Compute Weighted Jaccard similarity between two communities\n",
    "    based on gene importance scores.\n",
    "    \n",
    "    Parameters:\n",
    "        scoresA, scoresB : dict\n",
    "            {gene: importance_score}\n",
    "            Scores can be any nonnegative values (e.g., PageRank, Z-score).\n",
    "    Returns:\n",
    "        float\n",
    "            Weighted Jaccard similarity in [0, 1].\n",
    "    \"\"\"\n",
    "    genes = set(scoresA) | set(scoresB)\n",
    "    if not genes:\n",
    "        return 0.0\n",
    "    num = sum(min(scoresA.get(g, 0.0), scoresB.get(g, 0.0)) for g in genes)\n",
    "    den = sum(max(scoresA.get(g, 0.0), scoresB.get(g, 0.0)) for g in genes)\n",
    "    return num / den if den > 0 else 0.0\n",
    "\n",
    "def weighted_overlap_coefficient(dictA, dictB):\n",
    "    \"\"\"\n",
    "    Weighted Szymkiewicz–Simpson (Overlap) coefficient ∈ [0,1].\n",
    "    \"\"\"\n",
    "    if not dictA or not dictB:\n",
    "        return 0.0\n",
    "\n",
    "    common = set(dictA) & set(dictB)\n",
    "    inter_sum = sum(min(dictA[v], dictB[v]) for v in common)\n",
    "\n",
    "    sumA = sum(max(0, w) for w in dictA.values())\n",
    "    sumB = sum(max(0, w) for w in dictB.values())\n",
    "    denom = min(sumA, sumB)\n",
    "\n",
    "    return inter_sum / denom if denom > 0 else 0.0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dedd81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wjacc_edges_builder(g_list,c_list,score_cap = 0,com_size_cap = 0):\n",
    "    result = []\n",
    "    for i in range(len(g_list)-1):\n",
    "        prev_G = g_list[i]\n",
    "        curr_G = g_list[i+1]\n",
    "        prev_com = c_list[i]\n",
    "        curr_com = c_list[i+1]\n",
    "        prev_z = []\n",
    "        curr_z = []\n",
    "        \n",
    "        for com in prev_com:\n",
    "            if (len(com) > com_size_cap):\n",
    "                prev_z.append(community_central_genes(prev_G,com))\n",
    "        for com in curr_com:\n",
    "            if (len(com) > com_size_cap):\n",
    "                curr_z.append(community_central_genes(curr_G,com))\n",
    "\n",
    "        for j in range(len(prev_z)):\n",
    "            for k in range(len(curr_z)):\n",
    "                jaccard_score = weighted_jaccard(prev_z[j],curr_z[k])\n",
    "                if (jaccard_score >= score_cap):\n",
    "                    result.append((queue[i][0],j,k,jaccard_score))\n",
    "    \n",
    "    return result\n",
    "\n",
    "def community_sizes(labels):\n",
    "    \"\"\"Return dict[label] -> number of nodes in that community.\"\"\"\n",
    "    lab = np.asarray(labels)\n",
    "    uniq, cnt = np.unique(lab, return_counts=True)\n",
    "    return {int(c): int(n) for c, n in zip(uniq, cnt)}\n",
    "\n",
    "def topk_at_t1(labels_t1, k=10):\n",
    "    \"\"\"Return list of top-k community labels at t=1 by size.\"\"\"\n",
    "    sizes = community_sizes(labels_t1)\n",
    "    return [c for c, _ in sorted(sizes.items(), key=lambda x: x[1], reverse=True)[:k]], sizes\n",
    "\n",
    "# ---------- Jaccard lookup ----------\n",
    "\n",
    "def build_edge_lookup(wjacc_edges):\n",
    "    \"\"\"\n",
    "    Build a dict mapping (t, comm_t) -> list of (comm_t+1, jaccard_score).\n",
    "    Each wjacc_edges element = (t, comm_t, comm_tplus1, jaccard_score)\n",
    "    \"\"\"\n",
    "    out = defaultdict(list)\n",
    "    for t, c1, c2, s in wjacc_edges:\n",
    "        out[(int(t), int(c1))].append((int(c2), float(s)))\n",
    "    return out\n",
    "\n",
    "# ---------- Tracking ----------\n",
    "\n",
    "def track_paths(seeds, wj_lookup, ts):\n",
    "    \"\"\"Track each seed community forward by max Jaccard each step.\"\"\"\n",
    "    print(ts)\n",
    "    t0 = ts[0]\n",
    "    paths = {s: [(t0, s)] for s in seeds}\n",
    "    edges = []\n",
    "    for s in seeds:\n",
    "        cur = s\n",
    "        for i in range(len(ts) - 1):\n",
    "            t = ts[i]\n",
    "            tp1 = ts[i+1]\n",
    "            cand = wj_lookup.get((t, cur), [])\n",
    "            if not cand:\n",
    "                print(f\"Seed {s}, {t} to {tp1} has no outgoing edges. Termniated.\")\n",
    "                paths[s].append((tp1, None))\n",
    "                cur = None\n",
    "                break\n",
    "            nxt, score = max(cand, key=lambda x: x[1])\n",
    "            edges.append(((t, cur), (tp1, nxt), score))\n",
    "            paths[s].append((tp1, nxt))\n",
    "            cur = nxt\n",
    "    return paths, edges\n",
    "\n",
    "# MY VERSION\n",
    "# def track_paths(labels_by_t, seeds, wj_lookup, ts,score_cap = 0.1):\n",
    "#     \"\"\"Track each seed community forward by max Jaccard each step.\"\"\"\n",
    "#     t0 = ts[0]\n",
    "#     paths = {s: [(t0, s)] for s in seeds}\n",
    "#     edges = []\n",
    "#     curs = []\n",
    "#     curs_next = []\n",
    "#     for s in seeds:\n",
    "#         curs_next.append(s)\n",
    "#         for i in range(len(ts) - 1):\n",
    "#             curs = curs_next\n",
    "#             curs_next = []\n",
    "#             for cur in curs:\n",
    "#                 t = ts[i]\n",
    "#                 tp1 = ts[i+1]\n",
    "#                 cand = wj_lookup.get((t, cur), [])\n",
    "#                 if not cand:\n",
    "#                     if (t == t0):\n",
    "#                         print(f\"Community {cur} has no outgoing edges\")\n",
    "#                     paths[s].append((tp1, None))\n",
    "#                     cur = None\n",
    "#                     continue\n",
    "                \n",
    "                \n",
    "#                 for nxt,score in cand:\n",
    "#                     if (score >= score_cap):\n",
    "#                         edges.append(((t, cur), (tp1, nxt), score))\n",
    "#                         paths[s].append((tp1, nxt))\n",
    "#                         curs_next.append(nxt)\n",
    "\n",
    "#     return paths, edges\n",
    "\n",
    "# ---------- Get node sizes ----------\n",
    "\n",
    "def node_sizes_for_paths(labels_by_t, paths):\n",
    "    \"\"\"Return dict[(t, comm)] -> size (number of vertices).\"\"\"\n",
    "    out = {}\n",
    "    for s, seq in paths.items():\n",
    "        for t, c in seq:\n",
    "            if c is None: continue\n",
    "            sizes = community_sizes(labels_by_t[t])\n",
    "            out[(t, c)] = sizes.get(c, 0)\n",
    "    return out\n",
    "\n",
    "# ---------- Draw layered network ----------\n",
    "\n",
    "def draw_layered_paths(paths, edge_list, node_sizes, ts,\n",
    "                       node_size_scale=700.0, edge_width_scale=8.0,\n",
    "                       seed_gap=1.0, col_gap=3.0, score_cap = 0.5, title=None, save_path = None):\n",
    "    seeds = list(paths.keys())\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Add nodes\n",
    "    for s in seeds:\n",
    "        for t, c in paths[s]:\n",
    "            if c is None: continue\n",
    "            G.add_node((s, t, c), seed=s, t=t, comm=c, size=node_sizes.get((t, c), 0))\n",
    "    # Add edges\n",
    "    # Set your inclusion threshold (cap)\n",
    "    # include all edges with Jaccard >= 0.5\n",
    "\n",
    "    for (t, c1), (tp, c2), w in edge_list:\n",
    "        if w < score_cap:\n",
    "            continue\n",
    "        for s in seeds:\n",
    "            seq = paths[s]\n",
    "            for i in range(len(seq) - 1):\n",
    "                if seq[i] == (t, c1) and seq[i+1] == (tp, c2):\n",
    "                    G.add_edge((s, t, c1), (s, tp, c2), jacc=w)\n",
    "\n",
    "    # Layout positions\n",
    "    pos = {}\n",
    "    nodes_by_t = defaultdict(list)\n",
    "    for n in G.nodes:\n",
    "        t = G.nodes[n]['t']\n",
    "        nodes_by_t[t].append(n)\n",
    "\n",
    "    # Sort nodes in each column by community size (largest first)\n",
    "    pos = {}\n",
    "    for t in ts:\n",
    "        column_nodes = nodes_by_t.get(t, [])\n",
    "        # sort by size descending\n",
    "        column_nodes.sort(key=lambda n: G.nodes[n].get('size', G.nodes[n].get('size_w', 0)), reverse=True)\n",
    "        for i, n in enumerate(column_nodes):\n",
    "            pos[n] = (ts.index(t) * col_gap, -i * seed_gap)\n",
    "\n",
    "    # Scale sizes and widths\n",
    "    if len(G) == 0:\n",
    "        raise ValueError(\"No nodes to draw — check your data.\")\n",
    "    vals = np.array([G.nodes[n]['size'] for n in G.nodes], dtype=float)\n",
    "    vmin, vmax = vals.min(), vals.max()\n",
    "    sizes = []\n",
    "    for n in G.nodes:\n",
    "        w = G.nodes[n]['size']\n",
    "        a = (w - vmin) / (vmax - vmin) if vmax > vmin else 1.0\n",
    "        sizes.append((0.3 + 0.7*a) * node_size_scale)\n",
    "    widths = [max(0.5, d['jacc'] * edge_width_scale) for _, _, d in G.edges(data=True)]\n",
    "\n",
    "    # Draw\n",
    "    plt.figure(figsize=(max(10, len(ts)*1.4), max(6, len(seeds)*0.55 + 2)))\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=sizes)\n",
    "    nx.draw_networkx_edges(G, pos, width=widths, arrows=False)\n",
    "\n",
    "    edge_labels = {(u, v): f\"{d['jacc']:.2f}\" for u, v, d in G.edges(data=True)}\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8)\n",
    "    \n",
    "    plt.xticks([i * col_gap for i in range(len(ts))], [str(t) for t in ts])\n",
    "    plt.yticks([])\n",
    "    plt.title(title or f\"Top {len(seeds)} communities from t={ts[0]} → t={ts[-1]} (edge ∝ Jaccard)\")\n",
    "    plt.tight_layout()\n",
    "    if (save_path != None):\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "# ---------- Main wrapper ----------\n",
    "\n",
    "def visualize_topk_from_edges(labels_by_t, wjacc_edges,\n",
    "                              t_start=1, t_end=16, top_k=10,\n",
    "                              node_size_scale=700.0, score_cap = 0.5,\n",
    "                              edge_width_scale=8.0, title=None, save_path = None):\n",
    "    ts = sorted(labels_by_t.keys())\n",
    "    if t_start not in labels_by_t:\n",
    "        raise ValueError(f\"Missing labels for t={t_start}\")\n",
    "\n",
    "    # Top-k seeds from t_start\n",
    "    seeds, _ = topk_at_t1(labels_by_t[t_start], k=top_k)\n",
    "\n",
    "    # Track forward using provided Jaccard edges\n",
    "    wj_lookup = build_edge_lookup(wjacc_edges)\n",
    "    paths, edges = track_paths(seeds, wj_lookup, ts)\n",
    "\n",
    "    # Compute community sizes (unweighted)\n",
    "    node_sizes = node_sizes_for_paths(labels_by_t, paths)\n",
    "\n",
    "    # Draw the layered visualization\n",
    "    print(ts)\n",
    "    draw_layered_paths(paths, edges, node_sizes, ts,\n",
    "                       node_size_scale=node_size_scale,\n",
    "                       edge_width_scale=edge_width_scale,\n",
    "                       title=title or f\"Top-{top_k} from t={t_start} → t={t_end}\", score_cap = score_cap,\n",
    "                       save_path = save_path)\n",
    "\n",
    "    return paths, edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adc4df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load variables:\n",
    "# DATA_DIRECTORY = OUTPUT_DIRECTORY + \"leiden_result_variables_temp\"\n",
    "# with open(f\"{DATA_DIRECTORY}/label.pkl\", \"rb\") as f:\n",
    "#     label_list = pickle.load(f)\n",
    "# with open(f\"{DATA_DIRECTORY}/score.pkl\", \"rb\") as f:\n",
    "#     score_list = pickle.load(f)\n",
    "# with open(f\"{DATA_DIRECTORY}/graph.pkl\", \"rb\") as f:\n",
    "#     graph_list = pickle.load(f)\n",
    "# with open(f\"{DATA_DIRECTORY}/communities.pkl\", \"rb\") as f:\n",
    "#     communities_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b26217",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(graph_list))\n",
    "print(len(communities_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afb7162",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = {queue[i][0]: label_list[i] for i in range(len(queue))}\n",
    "wjacc = wjacc_edges_builder(graph_list,communities_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c48802",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels_dict)\n",
    "print(len(labels_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659b5d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wjacc)\n",
    "print(len(wjacc))\n",
    "save_path = f\"../output/{DISEASE}/leiden_tracking_results/leiden_tracking.png\"\n",
    "# save_path = None\n",
    "_, edges = visualize_topk_from_edges(labels_dict, wjacc, 1,24,top_k = 20,score_cap = 0.03,save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac873f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5c3bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = [[i] for i in range(1,25)]\n",
    "x = [T[0] for T in queue]\n",
    "y1 = score_list\n",
    "y2 = [len(communities_list[i]) for i in range(len(communities_list))]\n",
    "\n",
    "# Create a figure with 2 subplots (1 row, 2 columns)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Left graph\n",
    "axes[0].plot(x, y1, color='r')\n",
    "axes[0].set_title(\"quality score\")\n",
    "axes[0].set_xlabel(\"x\")\n",
    "axes[0].set_ylabel(\"quality_score\")\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Right graph\n",
    "axes[1].plot(x, y2, color='b')\n",
    "axes[1].set_title(\"community size\")\n",
    "axes[1].set_xlabel(\"x\")\n",
    "axes[1].set_ylabel(\"community size\")\n",
    "axes[1].grid(True)\n",
    "\n",
    "# Adjust spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d64ef50",
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_time = 1 / (1-eigenvalues[1])\n",
    "print(f\"Mixed Time: {mixed_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d9886b",
   "metadata": {},
   "source": [
    "# Final Average DDBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2323e2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Average Clustering\n",
    "average_t = [2,4,6,8]\n",
    "pdf_path = f\"../output/{DISEASE}/leiden_report.pdf\"\n",
    "method = \"modularity\"\n",
    "num_neighbors = 100\n",
    "resolution = 1.05\n",
    "label_list = []\n",
    "score_list = []\n",
    "graph_list = []\n",
    "communities_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9c35dacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00000000e+00 9.66696495e-03 8.68655885e-03 ... 8.29415814e-03\n",
      "  5.14272483e-03 2.75711577e-03]\n",
      " [9.66696495e-03 0.00000000e+00 6.90730793e-03 ... 5.35240604e-05\n",
      "  7.24914633e-04 2.16814155e-03]\n",
      " [8.68655885e-03 6.90730793e-03 0.00000000e+00 ... 6.43801114e-03\n",
      "  5.70815567e-03 5.74792841e-03]\n",
      " ...\n",
      " [8.29415814e-03 5.35240604e-05 6.43801114e-03 ... 0.00000000e+00\n",
      "  3.84482480e-04 1.54035000e-03]\n",
      " [5.14272483e-03 7.24914633e-04 5.70815567e-03 ... 3.84482480e-04\n",
      "  0.00000000e+00 3.85692997e-04]\n",
      " [2.75711577e-03 2.16814155e-03 5.74792841e-03 ... 1.54035000e-03\n",
      "  3.85692997e-04 0.00000000e+00]]\n",
      "0.09119807183742523\n",
      "Wrote: ..\\output\\BIPOLAR\\leiden_report.pdf\n"
     ]
    }
   ],
   "source": [
    "labels, score, communities, graph = DDBC(eigenvalues, eigenvectors, num_neighbors, resolution, average_t, leiden_method = method)\n",
    "path = community_report_onepage(labels, score, out_pdf=pdf_path,\n",
    "                                extra_text=[f\"method={method}\", f\"resolution={resolution}\", \"DGIDB-specific\"], \n",
    "                                bins=100,time_steps = average_t)\n",
    "print(\"Wrote:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5ef5f24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 4, 6, 7, 9, 10, 13, 16, 18, 19, 27, 29, 31, 36, 38, 43, 44, 45, 46, 48, 50, 51, 58, 59, 60, 61, 62, 63, 65, 66, 69, 70, 71, 72, 73, 74, 75, 78, 80, 83, 84, 85, 86, 87, 88, 89, 90, 91, 94, 95, 98, 100, 103, 104, 105, 106, 107, 112, 118, 119, 122, 124, 125, 126, 131, 132, 133, 136, 138, 140, 141, 144, 145, 154, 156, 157, 158, 159, 161, 162, 163, 164, 170, 171, 172, 174, 176, 177, 181, 182, 183, 186, 188, 197, 203, 204, 206, 211, 213, 216, 220, 223, 226, 242, 244, 245, 250, 257, 260, 263, 265, 269, 273, 274, 275, 276, 278, 279, 281, 282, 288, 289, 290, 292, 295, 296, 299, 300, 304, 305, 306, 311, 312, 315, 319, 323, 326, 327, 329, 330, 331, 334, 339, 343, 344, 348, 349, 350, 353, 354, 356, 357, 358], [0, 3, 5, 8, 11, 12, 15, 17, 24, 26, 28, 32, 33, 34, 37, 40, 41, 42, 47, 49, 56, 57, 64, 67, 68, 76, 79, 82, 93, 96, 101, 102, 108, 110, 111, 113, 114, 115, 116, 117, 120, 123, 127, 128, 130, 134, 135, 137, 139, 142, 143, 148, 149, 150, 152, 153, 155, 160, 166, 167, 168, 169, 173, 175, 179, 180, 184, 185, 190, 191, 193, 194, 195, 196, 198, 199, 201, 209, 210, 214, 221, 222, 224, 227, 228, 229, 230, 231, 232, 234, 236, 237, 239, 240, 252, 258, 261, 264, 272, 277, 280, 284, 286, 291, 309, 313, 316, 317, 318, 320, 321, 332, 333, 335, 336, 338, 341, 342, 351, 355], [21, 22, 23, 30, 53, 81, 109, 129, 146, 147, 151, 189, 205, 215, 218, 241, 248, 251, 254, 268, 283, 285, 287, 293, 294, 297, 298, 301, 307, 308, 310, 314, 322, 328, 337, 340, 345, 346, 352], [20, 25, 35, 52, 92, 99, 165, 192, 208, 212, 217, 243, 246, 253, 259, 267, 271, 302, 303, 324, 325, 347], [14, 39, 77, 97, 178, 187, 207, 219, 225, 233, 235, 249, 255, 256, 262, 266, 270], [54, 55, 121, 200, 202, 238, 247]]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(communities)\n",
    "print(len(communities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "79007c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 4, 6, 7, 9, 10, 13, 16, 18, 19, 27, 29, 31, 36, 38, 43, 44, 45, 46, 48, 50, 51, 58, 59, 60, 61, 62, 63, 65, 66, 69, 70, 71, 72, 73, 74, 75, 78, 80, 83, 84, 85, 86, 87, 88, 89, 90, 91, 94, 95, 98, 100, 103, 104, 105, 106, 107, 112, 118, 119, 122, 124, 125, 126, 131, 132, 133, 136, 138, 140, 141, 144, 145, 154, 156, 157, 158, 159, 161, 162, 163, 164, 170, 171, 172, 174, 176, 177, 181, 182, 183, 186, 188, 197, 203, 204, 206, 211, 213, 216, 220, 223, 226, 242, 244, 245, 250, 257, 260, 263, 265, 269, 273, 274, 275, 276, 278, 279, 281, 282, 288, 289, 290, 292, 295, 296, 299, 300, 304, 305, 306, 311, 312, 315, 319, 323, 326, 327, 329, 330, 331, 334, 339, 343, 344, 348, 349, 350, 353, 354, 356, 357, 358], [0, 3, 5, 8, 11, 12, 15, 17, 24, 26, 28, 32, 33, 34, 37, 40, 41, 42, 47, 49, 56, 57, 64, 67, 68, 76, 79, 82, 93, 96, 101, 102, 108, 110, 111, 113, 114, 115, 116, 117, 120, 123, 127, 128, 130, 134, 135, 137, 139, 142, 143, 148, 149, 150, 152, 153, 155, 160, 166, 167, 168, 169, 173, 175, 179, 180, 184, 185, 190, 191, 193, 194, 195, 196, 198, 199, 201, 209, 210, 214, 221, 222, 224, 227, 228, 229, 230, 231, 232, 234, 236, 237, 239, 240, 252, 258, 261, 264, 272, 277, 280, 284, 286, 291, 309, 313, 316, 317, 318, 320, 321, 332, 333, 335, 336, 338, 341, 342, 351, 355], [21, 22, 23, 30, 53, 81, 109, 129, 146, 147, 151, 189, 205, 215, 218, 241, 248, 251, 254, 268, 283, 285, 287, 293, 294, 297, 298, 301, 307, 308, 310, 314, 322, 328, 337, 340, 345, 346, 352], [20, 25, 35, 52, 92, 99, 165, 192, 208, 212, 217, 243, 246, 253, 259, 267, 271, 302, 303, 324, 325, 347], [14, 39, 77, 97, 178, 187, 207, 219, 225, 233, 235, 249, 255, 256, 262, 266, 270], [54, 55, 121, 200, 202, 238, 247]]\n"
     ]
    }
   ],
   "source": [
    "print(communities)\n",
    "DATA_DIRECTORY = OUTPUT_DIRECTORY + \"leiden_results\"\n",
    "with open(f\"{DATA_DIRECTORY}/result_communities_DGIDB.pkl\", \"wb\") as f:\n",
    "    pickle.dump(communities, f)\n",
    "with open(f\"{DATA_DIRECTORY}/result_graph_DGIDB.pkl\", \"wb\") as f:\n",
    "    pickle.dump(graph, f)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
