{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import json\n",
    "from scipy.sparse import load_npz,save_npz,diags,csr_matrix\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from pypdf import PdfReader, PdfWriter\n",
    "from tempfile import NamedTemporaryFile\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import gseapy as gp\n",
    "import mygene\n",
    "from IPython.display import display, HTML\n",
    "import re\n",
    "from collections import deque\n",
    "from goatools.obo_parser import GODag\n",
    "import math\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "from gseapy.parser import read_gmt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.width', None)      # No line-wrapping\n",
    "pd.set_option('display.max_columns', None)  # Show all columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISEASE = \"BIPOLAR\"\n",
    "DISEASE_FOLDER = f\"../output/{DISEASE}/\"\n",
    "RESULT_FOLDER = DISEASE_FOLDER + \"leiden_results/\"\n",
    "DGIDB_DIRECTORY = f\"../../Gen_Hypergraph/output/DGIDB_{DISEASE}/\"\n",
    "MSIGDB_DIRECTORY = \"../../Gen_Hypergraph/output/MSigDB_Full/\"\n",
    "RESULT_COMMUNITIES = \"result_communities_new\"\n",
    "RESULT_GRAPH = \"result_graph_new\"\n",
    "\n",
    "with open(DISEASE_FOLDER + \"gene_to_index_distinct.json\", \"r\") as file:\n",
    "    gene_to_index_distinct = json.load(file)\n",
    "with open(DGIDB_DIRECTORY + \"gene_to_index_BIPOLAR.json\", \"r\") as file:\n",
    "    DGIDB_gene_to_index = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_gene_distinct = {v: k for k, v in gene_to_index_distinct.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Community preprocessing\n",
    "def zscore(values):\n",
    "    arr = np.asarray(values, dtype=float)\n",
    "    if arr.size == 0:\n",
    "        return arr\n",
    "\n",
    "    mean = arr.mean()\n",
    "    std = arr.std(ddof=0)\n",
    "\n",
    "    if std == 0 or np.isnan(std):\n",
    "        # no variation: all z-scores = 0\n",
    "        return np.zeros_like(arr)\n",
    "\n",
    "    return (arr - mean) / std\n",
    "\n",
    "def communities_cutoff(communities, cutoff = 100):\n",
    "    result = []\n",
    "    for community in communities:\n",
    "        if len(community) >= cutoff:\n",
    "            result.append(community)\n",
    "\n",
    "    return result, len(result)\n",
    "\n",
    "def community_central_genes_by_num(G, community_nodes, weight=\"weight\", top_n=20):\n",
    "    C = set(community_nodes)\n",
    "    H = G.subgraph(C).copy()                       # induced subgraph\n",
    "    # within-community (weighted) degree\n",
    "    k = {u: H.degree(u, weight=weight) for u in H}\n",
    "    ks = np.array(list(k.values()), dtype=float)\n",
    "    zscore_list = zscore(ks)\n",
    "    Z = dict(zip(H,zscore_list))        # within-module degree z-score\n",
    "\n",
    "    # rank by z\n",
    "    ranked = sorted(H.nodes(), key=lambda u: (Z[u]), reverse=True)\n",
    "    return [u for u in ranked[:top_n]]\n",
    "\n",
    "def community_central_genes_by_score(G, community_nodes, weight=\"weight\",score_cap = 1):\n",
    "    C = set(community_nodes)\n",
    "    H = G.subgraph(C).copy()                       # induced subgraph\n",
    "    # within-community (weighted) degree\n",
    "    k = {u: H.degree(u, weight=weight) for u in H}\n",
    "    ks = np.array(list(k.values()), dtype=float)\n",
    "    zscore_list = zscore(ks)\n",
    "    Z = dict(zip(H,zscore_list))        # within-module degree z-score\n",
    "\n",
    "    # rank by z\n",
    "    ranked = sorted(H.nodes(), key=lambda u: (Z[u]), reverse=True)\n",
    "    return [u for u in ranked if Z[u] >= score_cap]\n",
    "\n",
    "def community_central_genes_by_pct(G, community_nodes, weight=\"weight\",pct = 0.3):\n",
    "    C = set(community_nodes)\n",
    "    H = G.subgraph(C).copy()                       # induced subgraph\n",
    "    # within-community (weighted) degree\n",
    "    k = {u: H.degree(u, weight=weight) for u in H}\n",
    "    ks = np.array(list(k.values()), dtype=float)\n",
    "    zscore_list = zscore(ks)\n",
    "    Z = dict(zip(H,zscore_list))        # within-module degree z-score\n",
    "\n",
    "    # rank by z\n",
    "    ranked = sorted(H.nodes(), key=lambda u: (Z[u]), reverse=True)\n",
    "    top = int(len(ranked)*pct)\n",
    "    return [u for u in ranked[:top]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpful functions\n",
    "def drop_nan_from_communities(communities):\n",
    "    cleaned_communities = []\n",
    "    total_dropped = 0\n",
    "\n",
    "    for i, community in enumerate(communities):\n",
    "        cleaned = []\n",
    "        dropped = 0\n",
    "        for g in community:\n",
    "            if g is None or (isinstance(g, float) and math.isnan(g)):\n",
    "                dropped += 1\n",
    "            else:\n",
    "                cleaned.append(g)\n",
    "        cleaned_communities.append(cleaned)\n",
    "        total_dropped += dropped\n",
    "        print(f\"Community {i}: dropped {dropped} NaN entries\")\n",
    "\n",
    "    print(f\"\\nTotal dropped across all communities: {total_dropped}\")\n",
    "    return cleaned_communities\n",
    "\n",
    "def big_objects(n=10, min_mb=1):\n",
    "    \"\"\"\n",
    "    Show the largest objects currently in memory.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int\n",
    "        Number of top objects to show.\n",
    "    min_mb : float\n",
    "        Minimum size (in MB) to include.\n",
    "    \"\"\"\n",
    "    import sys\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import scipy.sparse as sp\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    def get_size(obj):\n",
    "        try:\n",
    "            if isinstance(obj, np.ndarray):\n",
    "                return obj.nbytes\n",
    "            elif isinstance(obj, pd.DataFrame) or isinstance(obj, pd.Series):\n",
    "                return obj.memory_usage(deep=True).sum()\n",
    "            elif sp.issparse(obj):\n",
    "                return (obj.data.nbytes +\n",
    "                        obj.indptr.nbytes +\n",
    "                        obj.indices.nbytes)\n",
    "            else:\n",
    "                return sys.getsizeof(obj)\n",
    "        except Exception:\n",
    "            return 0\n",
    "\n",
    "    ip = get_ipython()\n",
    "    if ip is None:\n",
    "        ns = globals()\n",
    "    else:\n",
    "        ns = ip.user_ns\n",
    "\n",
    "    items = []\n",
    "    for name, val in ns.items():\n",
    "        if name.startswith('_'):\n",
    "            continue  # skip internals\n",
    "        size = get_size(val)\n",
    "        if size > min_mb * 1024 ** 2:\n",
    "            items.append((name, type(val).__name__, size))\n",
    "\n",
    "    items.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    print(f\"{'Variable':30s} {'Type':25s} {'Size (MB)':>10s}\")\n",
    "    print(\"-\" * 70)\n",
    "    for name, t, size in items[:n]:\n",
    "        print(f\"{name:30s} {t:25s} {size / 1024 ** 2:10.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-score distribution\n",
    "def compute_within_community_zscores(G, communities):\n",
    "    \"\"\"\n",
    "    Compute within-community degree z-scores for each node.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    G : networkx.Graph\n",
    "        Graph whose nodes match those in `communities`.\n",
    "        Uses edge weight from 'weight' attribute if present; otherwise 1.0.\n",
    "    communities : list of list\n",
    "        communities[c] is a list of node IDs in community c.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Columns: ['node', 'community', 'k_in', 'zscore']\n",
    "    \"\"\"\n",
    "    # 1. Node -> community map\n",
    "    node_to_comm = {\n",
    "        node: cid\n",
    "        for cid, comm in enumerate(communities)\n",
    "        for node in comm\n",
    "    }\n",
    "\n",
    "    # 2. Initialize within-community degree for every node\n",
    "    k_in = {node: 0.0 for node in node_to_comm.keys()}\n",
    "\n",
    "    # 3. Iterate over edges once, add weight if endpoints in same community\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        if u not in node_to_comm or v not in node_to_comm:\n",
    "            continue  # skip nodes not in any community (if that happens)\n",
    "\n",
    "        cu = node_to_comm[u]\n",
    "        cv = node_to_comm[v]\n",
    "\n",
    "        if cu == cv:\n",
    "            w = data.get(\"weight\", 1.0)\n",
    "            k_in[u] += w\n",
    "            k_in[v] += w\n",
    "\n",
    "    # 4. Build DataFrame\n",
    "    rows = [\n",
    "        (node, node_to_comm[node], k_val)\n",
    "        for node, k_val in k_in.items()\n",
    "    ]\n",
    "    df = pd.DataFrame(rows, columns=[\"node\", \"community\", \"k_in\"])\n",
    "\n",
    "    # 5. Compute z-scores per community using the helper\n",
    "    df[\"zscore\"] = df.groupby(\"community\")[\"k_in\"].transform(zscore)\n",
    "\n",
    "    return df\n",
    "\n",
    "def plot_zscore_hist_all(df_z, bins=20, per_row=4, max_communities=None):\n",
    "    communities = sorted(df_z[\"community\"].unique())\n",
    "    if max_communities is not None:\n",
    "        communities = communities[:max_communities]\n",
    "\n",
    "    n = len(communities)\n",
    "    nrows = int(np.ceil(n / per_row))\n",
    "    ncols = per_row\n",
    "\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(4*ncols, 3*nrows))\n",
    "    axes = np.array(axes).flatten()\n",
    "\n",
    "    for ax, cid in zip(axes, communities):\n",
    "        zvals = df_z.loc[df_z[\"community\"] == cid, \"zscore\"]\n",
    "        ax.hist(zvals, bins=bins)\n",
    "        ax.set_title(f\"Community {cid}\")\n",
    "        ax.set_xlabel(\"z-score\")\n",
    "        ax.set_ylabel(\"Frequency\")\n",
    "\n",
    "    for ax in axes[len(communities):]:\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree distribution\n",
    "def compute_within_community_degrees(G, communities):\n",
    "    \"\"\"Compute within-community degrees for each node.\"\"\"\n",
    "    node_to_comm = {node: cid for cid, comm in enumerate(communities) for node in comm}\n",
    "    k_in = {node: 0.0 for node in node_to_comm}\n",
    "\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        if u not in node_to_comm or v not in node_to_comm:\n",
    "            continue\n",
    "        cu, cv = node_to_comm[u], node_to_comm[v]\n",
    "        if cu == cv:\n",
    "            w = data.get(\"weight\", 1.0)\n",
    "            k_in[u] += w\n",
    "            k_in[v] += w\n",
    "\n",
    "    df = pd.DataFrame([(n, node_to_comm[n], d) for n, d in k_in.items()],\n",
    "                      columns=[\"node\", \"community\", \"k_in\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_topk_community_degree_distributions(G, communities, top_k=20, normalize=True, bins=20):\n",
    "    \"\"\"\n",
    "    Plot within-community degree distributions for the top_k largest communities.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    G : networkx.Graph\n",
    "        Input graph.\n",
    "    communities : list[list]\n",
    "        List of node lists (each representing a community).\n",
    "    top_k : int, optional\n",
    "        Number of largest communities (by size) to plot. Default is 20.\n",
    "    normalize : bool, optional\n",
    "        If True, y-axis shows probability instead of count. Default is True.\n",
    "    bins : int, optional\n",
    "        Number of bins in each histogram. Default is 20.\n",
    "    \"\"\"\n",
    "    df = compute_within_community_degrees(G, communities)\n",
    "\n",
    "    # Select top_k largest communities by node count\n",
    "    comm_sizes = df[\"community\"].value_counts().sort_values(ascending=False)\n",
    "    top_comms = comm_sizes.index[:top_k]\n",
    "\n",
    "    # Determine subplot grid (square-ish)\n",
    "    ncols = 5\n",
    "    nrows = int(np.ceil(top_k / ncols))\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(4*ncols, 3*nrows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, cid in enumerate(top_comms):\n",
    "        ax = axes[i]\n",
    "        vals = df.loc[df[\"community\"] == cid, \"k_in\"].values\n",
    "        if len(vals) == 0:\n",
    "            continue\n",
    "\n",
    "        weights = np.ones_like(vals) / len(vals) if normalize else None\n",
    "        ax.hist(vals, bins=bins, weights=weights, color=\"steelblue\", alpha=0.7)\n",
    "        ax.set_title(f\"Community {cid} (n={len(vals)})\", fontsize=10)\n",
    "        ax.set_xlabel(\"$k_{in}$\", fontsize=9)\n",
    "        ax.set_ylabel(\"Prob.\" if normalize else \"Count\", fontsize=9)\n",
    "        ax.tick_params(axis=\"both\", labelsize=8)\n",
    "\n",
    "    # Turn off unused axes\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "\n",
    "    fig.suptitle(f\"Within-community degree distributions (Top {top_k})\", fontsize=14)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading result graph and communities\n",
    "with open(f\"{RESULT_FOLDER}/{RESULT_COMMUNITIES}.pkl\", \"rb\") as f:\n",
    "    communities = pickle.load(f)\n",
    "with open(f\"{RESULT_FOLDER}/{RESULT_GRAPH}.pkl\", \"rb\") as f:\n",
    "    graph = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "communities_df = compute_within_community_zscores(graph, communities)\n",
    "plot_zscore_hist_all(communities_df, bins=20, per_row=4,max_communities=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_topk_community_degree_distributions(graph, communities, top_k=20, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking communities greater than 100\n",
    "communities_greater_than_cutoff,num_greater_cutoff = communities_cutoff(communities,30)\n",
    "print(communities_greater_than_cutoff, '\\n', num_greater_cutoff)\n",
    "print(len(communities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Update communities to include only the important genes, determined by a community-size_cap and percent_cap\n",
    "# SIZE_CAP = 30\n",
    "# PCT = 0.3\n",
    "# communities_selected = []\n",
    "# for community in communities:\n",
    "#     if (len(community) >= SIZE_CAP):\n",
    "#         important_nodes = community_central_genes_by_pct(graph,community,pct = PCT)\n",
    "#         communities_selected.append(important_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update communities to include only the important genes, determined by a community-size_cap and z-score_cap\n",
    "SIZE_CAP = 30\n",
    "SCORE_CAP = 0.5\n",
    "communities_selected = []\n",
    "for community in communities:\n",
    "    if (len(community) >= SIZE_CAP):\n",
    "        important_nodes = community_central_genes_by_score(graph,community,score_cap = SCORE_CAP)\n",
    "        a = len(important_nodes)\n",
    "        b = len(community)\n",
    "        print(a,b,a/b)\n",
    "        communities_selected.append(important_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(communities_selected)\n",
    "print(len(communities_selected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the empty communities after first filter\n",
    "i = 0\n",
    "for i in range(len(communities_selected)):\n",
    "    cl = len(communities_selected[i])\n",
    "    if (cl == 0): del(communities_selected[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(communities_selected)\n",
    "print(len(communities_selected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert index to ncbi\n",
    "def index_to_ncbi(comms,index_to_ncbi = index_to_gene_distinct):\n",
    "    comms_ncbi = [list(map(index_to_ncbi.get, c)) for c in comms]\n",
    "    return comms_ncbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert index to ncbi\n",
    "communities_ncbi = index_to_ncbi(communities_selected,index_to_gene_distinct)\n",
    "print(communities_ncbi)\n",
    "print(len(communities_ncbi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NCBI to HGNC symbol\n",
    "def ncbi_to_HGNC(comms_ncbi):\n",
    "    comms_HGNC = []\n",
    "    for community in comms_ncbi:\n",
    "        mg = mygene.MyGeneInfo()\n",
    "        entrez_ids = [str(e) for e in community]\n",
    "\n",
    "        results = mg.querymany(\n",
    "            entrez_ids,\n",
    "            scopes=\"entrezgene\",\n",
    "            fields=\"symbol\",\n",
    "            species=\"human\"\n",
    "        )\n",
    "\n",
    "        # Build a mapping: input ID -> symbol (or None)\n",
    "        id_to_symbol = {}\n",
    "        for r in results:\n",
    "            q = str(r.get(\"query\"))\n",
    "            id_to_symbol[q] = r.get(\"symbol\") if not r.get(\"notfound\") else None\n",
    "\n",
    "        # Preserve original order\n",
    "        symbols = [id_to_symbol.get(str(e), None) for e in entrez_ids]\n",
    "        comms_HGNC.append(symbols)\n",
    "    return comms_HGNC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMUNITIES_HGNC = ncbi_to_HGNC(communities_ncbi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(COMMUNITIES_HGNC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMUNITIES_HGNC = drop_nan_from_communities(COMMUNITIES_HGNC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_selected_comm = len(COMMUNITIES_HGNC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "# Categoization Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### GO-slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = \"../../data\"\n",
    "GO_OBO = f\"{DATA_DIRECTORY}/GO/go-basic.obo\"            # put the file in your working dir (or give full path)\n",
    "GOSLIM_OBO = f\"{DATA_DIRECTORY}/GO/goslim_generic.obo\"  # swap to another slim if you prefer\n",
    "GOSLIM_PIR_OBO = f\"{DATA_DIRECTORY}/GO/goslim_pir.obo\"  # swap to another slim if you prefer\n",
    "GOSLIM_YEAST_OBO = f\"{DATA_DIRECTORY}/GO/goslim_yeast.obo\"\n",
    "GOSLIM_AGR_OBO = f\"{DATA_DIRECTORY}/GO/goslim_agr.obo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GO library\n",
    "go = GODag(GO_OBO)\n",
    "\n",
    "# SLIM libraries\n",
    "slim = GODag(GOSLIM_OBO)\n",
    "slim_pir = GODag(GOSLIM_PIR_OBO)\n",
    "slim_yeast = GODag(GOSLIM_YEAST_OBO)\n",
    "slim_agr = GODag(GOSLIM_AGR_OBO)\n",
    "\n",
    "slim_ids = set(slim.keys())\n",
    "slim_pir_ids = set(slim_pir.keys())\n",
    "slim_yeast_ids = set(slim_yeast.keys())\n",
    "slim_agr_ids = set(slim_agr.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "GO_RE = re.compile(r\"(GO:\\d{7})\")\n",
    "\n",
    "def get_goid(term: str):\n",
    "    if isinstance(term, str):\n",
    "        m = GO_RE.search(term)\n",
    "        if m:\n",
    "            return m.group(1)\n",
    "    raise RuntimeError(\"Term not found!!\")\n",
    "\n",
    "def get_go_ancestors(go_id):\n",
    "    \"\"\"Return a list of ancestor GO term IDs for the given GO ID using QuickGO.\"\"\"\n",
    "    url = f\"https://www.ebi.ac.uk/QuickGO/services/ontology/go/terms/{go_id}/ancestors\"\n",
    "    headers = {\"Accept\": \"application/json\"}\n",
    "\n",
    "    r = requests.get(url, headers=headers)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    data = r.json()\n",
    "    results = data.get(\"results\", [])\n",
    "    if not results:\n",
    "        return []\n",
    "\n",
    "    # Ancestors come back as a simple list of GO IDs (strings)\n",
    "    ancestors = results[0].get(\"ancestors\", [])\n",
    "    return set(ancestors)\n",
    "\n",
    "\n",
    "def get_go_ancestors_in_slim(go_id):\n",
    "    ancestors = get_go_ancestors(go_id)\n",
    "    return slim_ids & ancestors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_go_ancestors_at_depth(go_id, depth, include_relations=(\"is_a\", \"part_of\")):\n",
    "    \"\"\"\n",
    "    Return the set of GO term IDs that are ancestors of `go_id` and have\n",
    "    absolute depth == `depth` in the GO DAG.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    go_id : str\n",
    "        Starting GO term (e.g., \"GO:0051310\").\n",
    "    depth : int\n",
    "        Absolute depth in the GO DAG (e.g., 3 means all ancestors at depth=3).\n",
    "    include_relations : tuple[str]\n",
    "        Relation types to traverse upward, e.g. (\"is_a\", \"part_of\", \"regulates\", ...).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    set[str]\n",
    "        Ancestor GO IDs whose term.depth == `depth`. Empty set if none.\n",
    "    \"\"\"\n",
    "    if depth < 0:\n",
    "        return set()\n",
    "    if go_id not in go:\n",
    "        return set()\n",
    "\n",
    "    # One-hop function honoring relation filter\n",
    "    def parent_ids(term):\n",
    "        ids = set()\n",
    "        if \"is_a\" in include_relations:\n",
    "            # GOATOOLS usually puts is_a parents here (and sometimes part_of merged)\n",
    "            ids.update(p.id for p in term.parents)\n",
    "\n",
    "        rel = getattr(term, \"relationship\", {}) or {}\n",
    "        for r in include_relations:\n",
    "            # relationship entries are already GO IDs\n",
    "            ids.update(rel.get(r, []))\n",
    "\n",
    "        # ensure IDs exist in DAG\n",
    "        return {pid for pid in ids if pid in go}\n",
    "\n",
    "    result = set()\n",
    "    frontier = {go_id}\n",
    "    visited = {go_id}\n",
    "\n",
    "    # BFS upwards, but pruning branches that are already above the target depth\n",
    "    while frontier:\n",
    "        next_frontier = set()\n",
    "        for node in frontier:\n",
    "            for pid in parent_ids(go[node]):\n",
    "                if pid in visited:\n",
    "                    continue\n",
    "                visited.add(pid)\n",
    "                d = go[pid].depth  # absolute depth in DAG\n",
    "\n",
    "                if d == depth:\n",
    "                    # ancestor at the exact target depth\n",
    "                    result.add(pid)\n",
    "                elif d > depth:\n",
    "                    # still \"below\" target depth (further from root),\n",
    "                    # its parents might reach the target depth\n",
    "                    next_frontier.add(pid)\n",
    "                # if d < depth: this branch has gone above the target,\n",
    "                # and all further ancestors will have depth <= d, so we can skip\n",
    "        frontier = next_frontier\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "### KEGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_kegg_name_to_id(species=\"hsa\"):\n",
    "    \"\"\"Map KEGG pathway name -> 'hsaXXXXX' (species-specific).\"\"\"\n",
    "    lines = requests.get(f\"https://rest.kegg.jp/list/pathway/{species}\").text.strip().splitlines()\n",
    "    name_to_id = {}\n",
    "    for ln in lines:\n",
    "        pid, raw = ln.split(\"\\t\")\n",
    "        pid = pid.replace(\"path:\", \"\")  # e.g. hsa03010\n",
    "        # strip \" - Homo sapiens (human)\" suffix\n",
    "        name = re.sub(r\"\\s*-\\s*Homo sapiens.*$\", \"\", raw).strip()\n",
    "        name_to_id[name.lower()] = pid\n",
    "    return name_to_id\n",
    "\n",
    "name_to_id = build_kegg_name_to_id(\"hsa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kegg_level2(hsa_id: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Return the KEGG Level 2 category for a pathway like 'hsa03040'.\n",
    "    Example: get_kegg_level2(\"hsa03040\") -> 'Transcription'\n",
    "    \"\"\"\n",
    "    url = f\"http://rest.kegg.jp/get/{hsa_id}\"\n",
    "    try:\n",
    "        text = requests.get(url, timeout=10).text\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    for line in text.splitlines():\n",
    "        if line.startswith(\"CLASS\"):\n",
    "            # CLASS line looks like: CLASS       Genetic Information Processing; Transcription\n",
    "            parts = [p.strip() for p in line.split(\";\", maxsplit=2)]\n",
    "            if len(parts) >= 2:\n",
    "                return [parts[1]]\n",
    "            elif len(parts) == 1:\n",
    "                return [parts[0].replace(\"CLASS\", \"\").strip()]\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### Reactome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_reactome_level1_map(species=\"9606\"):\n",
    "    \"\"\"\n",
    "    Returns { 'R-HSA-xxxxx': ['Top Level Name', ...], ... } for the given species.\n",
    "    species can be a taxonomy id ('9606') or a name ('Homo sapiens').\n",
    "    \"\"\"\n",
    "    # ensure spaces are encoded if a name is used\n",
    "    species_path = species.replace(\" \", \"+\")\n",
    "    url = f\"https://reactome.org/ContentService/data/eventsHierarchy/{species_path}\"\n",
    "    r = requests.get(url, headers={\"Accept\": \"application/json\"}, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    trees = r.json()  # list of trees, one per TopLevelPathway\n",
    "\n",
    "    mapping = {}\n",
    "    def walk(node, top_name):\n",
    "        st_id = node.get(\"stId\")\n",
    "        if st_id:\n",
    "            mapping.setdefault(st_id, set()).add(top_name)\n",
    "        for child in node.get(\"children\", []):\n",
    "            walk(child, top_name)\n",
    "\n",
    "    for top in trees:\n",
    "        top_name = top[\"name\"]  # level-1 category name\n",
    "        walk(top, top_name)\n",
    "\n",
    "    # sets -> sorted lists\n",
    "    return {k: sorted(v) for k, v in mapping.items()}\n",
    "\n",
    "# Example:\n",
    "reactome_level1 = build_reactome_level1_map(\"9606\")\n",
    "  # -> ['Signal Transduction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reactome_level1[\"R-HSA-9007101\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "# Run Enrichment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "TERM_SCORE_CAP = 0.001\n",
    "PERCENTAGE = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "### GO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GO Analysis; save terms with small size and high p-value\n",
    "def go_enrichment(communities,\n",
    "                  term_score_cap,\n",
    "                  percentage, \n",
    "                  slim_ids = slim_yeast_ids,\n",
    "                  depth = 1):\n",
    "    important_terms = pd.DataFrame(columns=[\"Community Index\",\"Community Size\",\"Term\", \"Overlap\", \"Adjusted P-value\",\"Category\"])\n",
    "    category_counts_and_overlap_score_list = {}\n",
    "    i = 0\n",
    "    num_nonzero_communities = 0\n",
    "    \n",
    "    for community in communities:\n",
    "        # Gene Ontology enrichment\n",
    "        enr_go = gp.enrichr(\n",
    "            gene_list=community,\n",
    "            gene_sets=['GO_Biological_Process_2023',\n",
    "                    'GO_Molecular_Function_2023',\n",
    "                    'GO_Cellular_Component_2023'],\n",
    "            organism='Human',\n",
    "            outdir=None # don't write to disk\n",
    "        )\n",
    "        go_df = enr_go.results\n",
    "        \n",
    "\n",
    "        # Filter by overlap percentage and adjusted p-value\n",
    "        mask =  (go_df[\"Adjusted P-value\"] < term_score_cap) & (go_df[\"Overlap\"].apply(lambda x: int(x.split(\"/\")[0])/int(x.split(\"/\")[1]) > percentage))\n",
    "        filtered = go_df[mask].copy()\n",
    "        \n",
    "        # Categorization from GO-Slim\n",
    "        filtered[\"GO_ID\"] = filtered[\"Term\"].apply(get_goid)\n",
    "        # filtered[\"Slim_IDs\"] = filtered[\"GO_ID\"].apply(get_go_ancestors_in_slim)\n",
    "        filtered[\"Slim_IDs\"] = filtered[\"GO_ID\"].apply(lambda id: get_go_ancestors_at_depth(id, depth=depth, include_relations=(\"is_a\", \"part_of\")))\n",
    "        \n",
    "        # Get empty count\n",
    "        empty_count = (filtered[\"Slim_IDs\"].apply(len) == 0).sum()\n",
    "        \n",
    "        # Get slim names    \n",
    "        filtered[\"Category\"] = filtered[\"Slim_IDs\"].apply(lambda ids: [go[i].name for i in ids])\n",
    "        \n",
    "        # Sort\n",
    "        filtered['Overlap (value)'] = filtered['Overlap'].apply(lambda x: int(x.split(\"/\")[0])/int(x.split(\"/\")[1]))\n",
    "        filtered = filtered.sort_values(['Overlap (value)'], ascending=False)\n",
    "        \n",
    "        # Compute overlap score for every category:\n",
    "        filtered_exploded = filtered.explode('Category').reset_index(drop=True)\n",
    "        category_counts_and_overlap_score = {}\n",
    "        for val, group in filtered_exploded.groupby('Category'):\n",
    "            overlap_list = group[\"Overlap\"].tolist()\n",
    "            numerators = [(lambda x: int(x.split(\"/\")[0]))(e) for e in overlap_list]\n",
    "            denominators = [(lambda x: int(x.split(\"/\")[1]))(e) for e in overlap_list]\n",
    "            overlap_score = sum(numerators)/sum(denominators)\n",
    "            \n",
    "            category_counts_and_overlap_score[val] = (len(group),overlap_score,)\n",
    "        \n",
    "        category_counts_and_overlap_score_list[i] = category_counts_and_overlap_score\n",
    "        \n",
    "        # Add results to important terms\n",
    "        if not filtered.empty:\n",
    "            # print size of community\n",
    "            print(f\"Size of community: {len(community)}\")\n",
    "            \n",
    "            # print number of filtered terms\n",
    "            print(f\"Number of filtered terms: {len(filtered)}\")\n",
    "            print(f\"Number of unmapped terms: {empty_count}\")      \n",
    "            \n",
    "            filtered.loc[:, \"Community Index\"] = i\n",
    "            filtered.loc[:, \"Community Size\"] = len(community)\n",
    "            important_terms = pd.concat([important_terms, filtered], ignore_index=True)\n",
    "            display(HTML(filtered[[\"Community Index\",'Term','Overlap','Adjusted P-value',\"Slim_IDs\",\"Category\"]].head(10).to_html(max_cols=None)))\n",
    "            print(category_counts_and_overlap_score)\n",
    "            num_nonzero_communities += 1\n",
    "\n",
    "        i += 1\n",
    "    print(f\"{num_nonzero_communities} out of {len(communities)} communities had significant GO terms.\")\n",
    "    return important_terms,category_counts_and_overlap_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "go_important_terms,go_category_counts_and_overlap_score = go_enrichment(COMMUNITIES_HGNC,TERM_SCORE_CAP,PERCENTAGE,slim_ids,depth = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "go_important_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "### KEGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KEGG\n",
    "def kegg_enrichment(communities,\n",
    "                    term_score_cap,\n",
    "                    percentage):\n",
    "    important_terms = pd.DataFrame(columns=[\"Community Index\",\"Community Size\",\"Term\", \"Overlap\", \"Adjusted P-value\",\"Category\"])\n",
    "    category_counts_and_overlap_score_list = {}\n",
    "    i = 0\n",
    "    num_nonzero_communities = 0\n",
    "    for community in communities:\n",
    "        enr_path = gp.enrichr(\n",
    "            gene_list=community,\n",
    "            gene_sets=['KEGG_2021_Human'],\n",
    "            organism='Human',\n",
    "            outdir=None\n",
    "        )\n",
    "        KEGG_df = enr_path.results\n",
    "\n",
    "        # Filter by overlap percentage and adjusted p-value\n",
    "        mask =  (KEGG_df[\"Adjusted P-value\"] < term_score_cap) & (KEGG_df[\"Overlap\"].apply(lambda x: int(x.split(\"/\")[0])/int(x.split(\"/\")[1]) > percentage))\n",
    "        filtered = KEGG_df[mask].copy()\n",
    "        \n",
    "        # Categorization from KEGG Level 2\n",
    "        filtered[\"KEGG_ID\"] = filtered[\"Term\"].str.replace(r\"\\s*-\\s*Homo sapiens.*$\", \"\", regex=True).str.lower().map(name_to_id)\n",
    "        filtered[\"Category\"] = filtered[\"KEGG_ID\"].map(get_kegg_level2)\n",
    "        \n",
    "        # Sort\n",
    "        filtered['Overlap (value)'] = filtered['Overlap'].apply(lambda x: int(x.split(\"/\")[0])/int(x.split(\"/\")[1]))\n",
    "        filtered = filtered.sort_values(['Overlap (value)'], ascending=False)\n",
    "        \n",
    "        # Compute overlap score for every category:\n",
    "        filtered_exploded = filtered.explode('Category').reset_index(drop=True)\n",
    "        category_counts_and_overlap_score = {}\n",
    "        for val, group in filtered_exploded.groupby('Category'):\n",
    "            overlap_list = group[\"Overlap\"].tolist()\n",
    "            numerators = [(lambda x: int(x.split(\"/\")[0]))(e) for e in overlap_list]\n",
    "            denominators = [(lambda x: int(x.split(\"/\")[1]))(e) for e in overlap_list]\n",
    "            overlap_score = sum(numerators)/sum(denominators)\n",
    "            \n",
    "            category_counts_and_overlap_score[val] = (len(group),overlap_score,)\n",
    "        \n",
    "        category_counts_and_overlap_score_list[i] = category_counts_and_overlap_score\n",
    "        \n",
    "        # Add results to important terms\n",
    "        if not filtered.empty:\n",
    "            # print size of community\n",
    "            print(f\"Size of community: {len(community)}\")   \n",
    "            \n",
    "            # print number of filtered terms\n",
    "            print(f\"Number of filtered terms: {len(filtered)}\")\n",
    "            filtered.loc[:, \"Community Index\"] = i\n",
    "            filtered.loc[:, \"Community Size\"] = len(community)\n",
    "            important_terms = pd.concat([important_terms, filtered], ignore_index=True)\n",
    "            \n",
    "            # show results\n",
    "            display(HTML(filtered[[\"Community Index\",'Term','Overlap','Adjusted P-value',\"KEGG_ID\",\"Category\"]].head(10).to_html(max_cols=None)))\n",
    "            print(category_counts_and_overlap_score)\n",
    "            num_nonzero_communities += 1\n",
    "\n",
    "        i += 1\n",
    "    print(f\"{num_nonzero_communities} out of {len(communities)} communities had significant GO terms.\")\n",
    "    return important_terms,category_counts_and_overlap_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "kegg_important_terms,kegg_category_counts_and_overlap_score = kegg_enrichment(COMMUNITIES_HGNC,TERM_SCORE_CAP,PERCENTAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "kegg_important_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "### Reactome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reactome enrichment\n",
    "def reactome_enrichment(communities,\n",
    "                        term_score_cap,\n",
    "                        percentage):\n",
    "    important_terms = pd.DataFrame(columns=[\"Community Index\",\"Community Size\",\"Term\", \"Overlap\", \"Adjusted P-value\",\"Category\"])\n",
    "    category_counts_and_overlap_score_list= {}\n",
    "    i = 0\n",
    "    num_nonzero_communities = 0\n",
    "    for community in communities:\n",
    "        enr_path = gp.enrichr(\n",
    "            gene_list=community,\n",
    "            gene_sets=['Reactome_2022'],\n",
    "            organism='Human',\n",
    "            outdir=None\n",
    "        )\n",
    "        Reactome_df = enr_path.results\n",
    "\n",
    "        # Filter by overlap percentage and adjusted p-value\n",
    "        mask =  (Reactome_df[\"Adjusted P-value\"] < term_score_cap) & (Reactome_df[\"Overlap\"].apply(lambda x: int(x.split(\"/\")[0])/int(x.split(\"/\")[1]) > percentage))\n",
    "        filtered = Reactome_df[mask].copy()\n",
    "        \n",
    "        # Categorization from Reactome Level 1\n",
    "        filtered[\"Category\"] = filtered[\"Term\"].str.extract(r\"(R-[A-Z]+-\\d+)\", expand=False).map(reactome_level1)\n",
    "        \n",
    "        # Sort\n",
    "        filtered['Overlap (value)'] = filtered['Overlap'].apply(lambda x: int(x.split(\"/\")[0])/int(x.split(\"/\")[1]))\n",
    "        filtered = filtered.sort_values(['Overlap (value)'], ascending=False)\n",
    "        \n",
    "        # Compute overlap score for every category:\n",
    "        filtered_exploded = filtered.explode('Category').reset_index(drop=True)\n",
    "        category_counts_and_overlap_score = {}\n",
    "        for val, group in filtered_exploded.groupby('Category'):\n",
    "            overlap_list = group[\"Overlap\"].tolist()\n",
    "            numerators = [(lambda x: int(x.split(\"/\")[0]))(e) for e in overlap_list]\n",
    "            denominators = [(lambda x: int(x.split(\"/\")[1]))(e) for e in overlap_list]\n",
    "            overlap_score = sum(numerators)/sum(denominators)\n",
    "            \n",
    "            category_counts_and_overlap_score[val] = (len(group),overlap_score,)\n",
    "        \n",
    "        category_counts_and_overlap_score_list[i] = category_counts_and_overlap_score\n",
    "        \n",
    "        # Add results to important terms\n",
    "        if not filtered.empty:\n",
    "            print(f\"Size of community: {len(community)}\")\n",
    "            print(f\"Number of filtered terms: {len(filtered)}\")\n",
    "            filtered.loc[:, \"Community Index\"] = i\n",
    "            filtered.loc[:, \"Community Size\"] = len(community)\n",
    "            important_terms = pd.concat([important_terms, filtered], ignore_index=True)\n",
    "            display(HTML(filtered[[\"Community Index\",'Term','Overlap','Adjusted P-value',\"Category\"]].head(10).to_html(max_cols=None)))\n",
    "            print(category_counts_and_overlap_score)\n",
    "            num_nonzero_communities += 1\n",
    "        i += 1\n",
    "    print(f\"{num_nonzero_communities} out of {len(communities)} communities had significant GO terms.\")\n",
    "    return important_terms,category_counts_and_overlap_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "reactome_important_terms,reactome_category_counts_and_overlap_score = reactome_enrichment(COMMUNITIES_HGNC,TERM_SCORE_CAP,PERCENTAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "reactome_important_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "### Disease Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disease_term_score_cap = 0.001\n",
    "# disease_percentage = 0.1\n",
    "# important_diseases = pd.DataFrame(columns=[\"Community Index\",\"Community Size\",\"Term\", \"Overlap\", \"Adjusted P-value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Disease-gene enrichment libraries\n",
    "# disease_sets = [\n",
    "#     'DisGeNET_2020', # curated geneâ€“disease associations\n",
    "#     'GWAS_Catalog_2023', # genome-wide association hits\n",
    "#     'OMIM_Disease', # Mendelian disorders\n",
    "#     'Jensen_DISEASES' # text-mined associations\n",
    "# ]\n",
    "\n",
    "# # # Disease-gene enrichment Analysis; save terms with small size and high p-value\n",
    "# i = 0\n",
    "# for community in communities_HGNC:\n",
    "#     # Gene Ontology enrichment\n",
    "#     enr_disease = gp.enrichr(\n",
    "#         gene_list=community,\n",
    "#         gene_sets=disease_sets,\n",
    "#         organism='Human',\n",
    "#         outdir=None # don't write to disk\n",
    "#     )\n",
    "#     enr_disease_df = enr_disease.results.sort_values('Adjusted P-value')\n",
    "#     print(f\"Size of community: {len(community)}\")\n",
    "\n",
    "#     mask =  (enr_disease_df[\"Adjusted P-value\"] < disease_term_score_cap) & (enr_disease_df[\"Overlap\"].apply(lambda x: int(x.split(\"/\")[0])/int(x.split(\"/\")[1]) > disease_percentage))\n",
    "        \n",
    "#     filtered = enr_disease_df[mask].copy()\n",
    "#     if not filtered.empty:\n",
    "#         filtered.loc[:, \"Community Index\"] = i\n",
    "#         filtered.loc[:, \"Community Size\"] = len(community)\n",
    "#         important_diseases = pd.concat([important_diseases, filtered], ignore_index=True)\n",
    "\n",
    "#     display(HTML(filtered[['Term','Overlap','Adjusted P-value']].head(10).to_html(max_cols=None)))\n",
    "#     i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "# Important Terms Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "### Graph Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comm_similarity_with_term(x,y):\n",
    "    return 1-(abs(x-y)/max(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_terms = pd.DataFrame(columns=[\"Community Index\",\"Community Size\",\"Term\", \"Overlap\", \"Adjusted P-value\",\"Category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [go_important_terms,kegg_important_terms,reactome_important_terms]\n",
    "important_terms = pd.concat(c, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# important_terms = important_terms.sort_values(by=\"Overlap (value)\",ascending=False)\n",
    "important_terms = important_terms.sort_values(by=\"Community Index\")\n",
    "important_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Community id to size dict\n",
    "com_id_to_size = {i : len(COMMUNITIES_HGNC[i]) for i in range(len(COMMUNITIES_HGNC))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_com_id_to_size = important_terms.drop_duplicates(subset=\"Community Index\", keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_size_dict = dict(zip(unique_com_id_to_size[\"Community Index\"], unique_com_id_to_size[\"Community Size\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df must have: \"Community Index\", \"Term\", \"Overlap (value)\"\n",
    "work = important_terms.loc[:, [\"Community Index\", \"Term\", \"Overlap (value)\",\"Category\"]].copy()\n",
    "work[\"Overlap (value)\"] = work[\"Overlap (value)\"].astype(float)\n",
    "\n",
    "# Ensure (community, term) uniqueness\n",
    "dupes = work.duplicated(subset=[\"Community Index\", \"Term\"], keep=False)\n",
    "if dupes.any():\n",
    "    raise ValueError(\"Duplicated (Community Index, Term) rows found; ensure uniqueness first.\")\n",
    "\n",
    "# --- Build edge weights AND collect contributing terms per pair ---\n",
    "edge_weights = {}              # (u, v) -> float\n",
    "edge_counts  = {}              # (u, v) -> int\n",
    "edge_terms   = {}              # (u, v) -> list[(term, contrib)]\n",
    "\n",
    "for term, sub in work.groupby(\"Term\", sort=False):\n",
    "    comms  = sub[\"Community Index\"].to_numpy()\n",
    "    scores = sub[\"Overlap (value)\"].to_numpy()\n",
    "    if len(comms) < 2:\n",
    "        continue\n",
    "    for i, j in combinations(range(len(comms)), 2):\n",
    "        u, v = comms[i], comms[j]\n",
    "        if u > v: u, v = v, u  # canonical ordering\n",
    "        contrib = comm_similarity_with_term(scores[i], scores[j])\n",
    "\n",
    "        edge_weights[(u, v)] = edge_weights.get((u, v), 0.0) + contrib\n",
    "        edge_counts[(u, v)]  = edge_counts.get((u, v), 0)    + 1\n",
    "        edge_terms.setdefault((u, v), []).append((term, contrib))\n",
    "\n",
    "# Sort contributing terms by contribution desc for each edge\n",
    "for key in edge_terms:\n",
    "    edge_terms[key].sort(key=lambda t: t[1], reverse=True)\n",
    "\n",
    "# --- Build edge list DataFrame (optional, useful to inspect) ---\n",
    "edge_df = pd.DataFrame(\n",
    "    [(u, v, edge_weights[(u, v)], edge_counts[(u, v)], edge_terms.get((u, v), []))\n",
    "     for (u, v) in edge_weights.keys()],\n",
    "    columns=[\"u\", \"v\", \"weight\", \"shared_terms\", \"terms_contrib\"]\n",
    ").sort_values([\"weight\", \"shared_terms\"], ascending=[False, False]).reset_index(drop=True)\n",
    "\n",
    "# --- Build NetworkX graph with attributes ---\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(pd.unique(work[\"Community Index\"]))\n",
    "for _, r in edge_df.iterrows():\n",
    "    G.add_edge(\n",
    "        int(r.u), int(r.v),\n",
    "        weight=float(r.weight),\n",
    "        shared_terms=int(r.shared_terms),\n",
    "        terms_contrib=r.terms_contrib  # list of (term, contrib) sorted desc\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "### Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------Table------------------\n",
    "term_contribs = []\n",
    "\n",
    "for term, sub in work.groupby(\"Term\", sort=False):\n",
    "    comms  = sub[\"Community Index\"].to_numpy()\n",
    "    scores = sub[\"Overlap (value)\"].to_numpy()\n",
    "    if len(comms) < 2:\n",
    "        continue\n",
    "    for i, j in combinations(range(len(comms)), 2):\n",
    "        u, v = comms[i], comms[j]\n",
    "        if u > v:\n",
    "            u, v = v, u\n",
    "        contrib = comm_similarity_with_term(scores[i], scores[j])\n",
    "        cat = sub[\"Category\"].iloc[0] if \"Category\" in sub.columns else None\n",
    "        term_contribs.append((u, v, term, contrib, cat))\n",
    "\n",
    "# 2) Build DataFrame\n",
    "term_df = pd.DataFrame(term_contribs, columns=[\"u\", \"v\", \"Term\", \"Contribution\",\"Category\"])\n",
    "# 3) Sort and aggregate terms per edge (keep per-term order)\n",
    "agg_blocks = []\n",
    "for (u, v), sub in term_df.groupby([\"u\", \"v\"]):\n",
    "    sub_sorted = sub.sort_values(\"Contribution\", ascending=False)\n",
    "    \n",
    "    # Create category count dictionary\n",
    "    category_counts = Counter(\n",
    "        c\n",
    "        for cats in sub_sorted[\"Category\"].dropna()\n",
    "        for c in cats\n",
    "    )\n",
    "    category_counts_dict = dict(category_counts)\n",
    "\n",
    "    # sub_sorted = sub.sort_values(sub_sorted[\"Category\"].apply(tuple), ascending=False)\n",
    "    block = \"\\n\".join(\n",
    "        [f\"  - {t} {cat} ({c:.3f})\"\n",
    "        for t, c, cat in zip(sub_sorted[\"Term\"], sub_sorted[\"Contribution\"], sub_sorted[\"Category\"])]\n",
    "    )\n",
    "    total = sub_sorted[\"Contribution\"].sum()\n",
    "    agg_blocks.append({\n",
    "        \"u\": u,\n",
    "        \"v\": v,\n",
    "        \"Total Weight\": total,\n",
    "        \"Terms (by contribution)\": block,\n",
    "        \"Category Count\": category_counts_dict\n",
    "    })\n",
    "\n",
    "# 4) Create final block table\n",
    "block_df = pd.DataFrame(agg_blocks).sort_values(\"Total Weight\", ascending=False).reset_index(drop=True)\n",
    "# 5) Display nicely\n",
    "for _, row in block_df.iterrows():\n",
    "    print(f\"Community pair ({row.u}, {row.v}) â€” Total Weight = {row['Total Weight']:.3f}\")\n",
    "    print(row[\"Terms (by contribution)\"])\n",
    "    \n",
    "    print()\n",
    "    print(\"Category Count:\")\n",
    "    for key, value in sorted(row[\"Category Count\"].items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# block_df.to_excel(\"output.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "### Category Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_category_count_by_comm(category_count_by_comm):\n",
    "    for comm_id, cat_dict in category_count_by_comm.items():\n",
    "        print(f\"\\nðŸ§© Community {comm_id}\")\n",
    "        print(\"-\" * (14 + len(str(comm_id))))\n",
    "\n",
    "        if not cat_dict:\n",
    "            print(\"  (no categories)\")\n",
    "            continue\n",
    "\n",
    "        # Sort categories by descending count\n",
    "        for cat, count in sorted(cat_dict.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  â€¢ {cat:<50} {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_count_by_comm = {}\n",
    "for i in range(num_selected_comm):\n",
    "    comm_cates = go_category_counts_and_overlap_score[i] | kegg_category_counts_and_overlap_score[i] | reactome_category_counts_and_overlap_score[i]\n",
    "    category_count_by_comm[i] = dict(sorted(comm_cates.items(), key=lambda x: x[1],reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_category_count_by_comm(category_count_by_comm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Visualization ==========\n",
    "\n",
    "# 1) Remove isolates\n",
    "non_isolates = [n for n, d in G.degree() if d > 0]\n",
    "H = G.subgraph(non_isolates).copy()\n",
    "\n",
    "# 2) Layout per connected component (circular)\n",
    "components = [H.subgraph(c).copy() for c in nx.connected_components(H)]\n",
    "pos = {}\n",
    "offset_x = 0.0\n",
    "gap = 4.0      # horizontal gap between components\n",
    "radius = 1.5   # radius of each component circle\n",
    "\n",
    "for comp in components:\n",
    "    sub_pos = nx.circular_layout(comp, scale=radius)\n",
    "    sub_pos = {n: (x + offset_x, y) for n, (x, y) in sub_pos.items()}\n",
    "    pos.update(sub_pos)\n",
    "    offset_x += 2 * radius + gap\n",
    "\n",
    "# 3) Scale edge widths by weight\n",
    "weights = np.array([H[u][v]['weight'] for u, v in H.edges()], dtype=float)\n",
    "wmin, wmax = (weights.min(), weights.max()) if len(weights) else (1.0, 1.0)\n",
    "widths = 2.5 if wmax == wmin else 1.0 + 6.0 * (weights - wmin) / (wmax - wmin)\n",
    "\n",
    "# 4) Node size proportional to your actual community size\n",
    "sizes_raw = np.array([comm_size_dict.get(n, 1) for n in H.nodes()], dtype=float)\n",
    "\n",
    "# Normalize sizes to a good plotting range\n",
    "sizes = 200 + 1000 * (sizes_raw - sizes_raw.min()) / (sizes_raw.max() - sizes_raw.min() + 1e-9)\n",
    "\n",
    "# 5) Edge labels\n",
    "edge_labels = {(u, v): f\"{H[u][v]['weight']:.2f}\" for u, v in H.edges()}\n",
    "\n",
    "# 6) Draw graph\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "nx.draw_networkx_nodes(\n",
    "    H, pos,\n",
    "    node_size=sizes,\n",
    "    node_color=\"skyblue\",\n",
    "    edgecolors=\"black\",\n",
    "    linewidths=0.6,\n",
    ")\n",
    "nx.draw_networkx_edges(\n",
    "    H, pos,\n",
    "    width=widths,\n",
    "    edge_color=\"gray\",\n",
    "    alpha=0.9,\n",
    ")\n",
    "nx.draw_networkx_labels(\n",
    "    H, pos,\n",
    "    font_size=9,\n",
    "    font_weight=\"bold\",\n",
    ")\n",
    "\n",
    "# Compute custom label positions: move labels closer to one endpoint\n",
    "edge_label_pos = {}\n",
    "for (u, v) in H.edges():\n",
    "    x_u, y_u = pos[u]\n",
    "    x_v, y_v = pos[v]\n",
    "\n",
    "    t = 0.7  # 0.5 = middle of edge, >0.5 moves toward u, <0.5 toward v\n",
    "    x_label = t * x_u + (1 - t) * x_v\n",
    "    y_label = t * y_u + (1 - t) * y_v\n",
    "\n",
    "    edge_label_pos[(u, v)] = (x_label, y_label)\n",
    "\n",
    "nx.draw_networkx_edge_labels(\n",
    "    H,\n",
    "    pos,\n",
    "    edge_labels=edge_labels,\n",
    "    font_size=8,\n",
    "    rotate=False,\n",
    "    label_pos=0.7,  # move label away from the center of each edge\n",
    "    bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"white\", ec=\"none\", alpha=0.7),\n",
    ")\n",
    "\n",
    "plt.title(\"Community Similarity Graph â€” Node Size âˆ Community Size\", fontsize=12)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "### Aggregate Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {},
   "source": [
    "# Robustness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_enrichment_func(community,term_score_cap,percentage):\n",
    "    # GO df\n",
    "    enr_go = gp.enrichr(\n",
    "        gene_list=community,\n",
    "        gene_sets=['GO_Biological_Process_2023',\n",
    "                'GO_Molecular_Function_2023',\n",
    "                'GO_Cellular_Component_2023'],\n",
    "        organism='Human',\n",
    "        outdir=None # don't write to disk\n",
    "    )\n",
    "    GO_df = enr_go.results\n",
    "    mask =  (GO_df[\"Adjusted P-value\"] < term_score_cap) & (GO_df[\"Overlap\"].apply(lambda x: int(x.split(\"/\")[0])/int(x.split(\"/\")[1]) > percentage))\n",
    "    GO_df = GO_df[mask].copy()   \n",
    "    \n",
    "    # KEGG df\n",
    "    enr_kegg = gp.enrichr(\n",
    "        gene_list=community,\n",
    "        gene_sets=['KEGG_2021_Human'],\n",
    "        organism='Human',\n",
    "        outdir=None\n",
    "    )\n",
    "    KEGG_df = enr_kegg.results\n",
    "    mask =  (KEGG_df[\"Adjusted P-value\"] < term_score_cap) & (KEGG_df[\"Overlap\"].apply(lambda x: int(x.split(\"/\")[0])/int(x.split(\"/\")[1]) > percentage))\n",
    "    KEGG_df = KEGG_df[mask].copy() \n",
    "       \n",
    "    # Reactome df\n",
    "    enr_reactome = gp.enrichr(\n",
    "        gene_list=community,\n",
    "        gene_sets=['Reactome_2022'],\n",
    "        organism='Human',\n",
    "        outdir=None\n",
    "    )\n",
    "    Reactome_df = enr_reactome.results  \n",
    "    mask =  (Reactome_df[\"Adjusted P-value\"] < term_score_cap) & (Reactome_df[\"Overlap\"].apply(lambda x: int(x.split(\"/\")[0])/int(x.split(\"/\")[1]) > percentage))\n",
    "    Reactome_df = Reactome_df[mask].copy()\n",
    "    \n",
    "    \n",
    "    all_df = [GO_df,KEGG_df,Reactome_df]\n",
    "    # build result df by concatenating\n",
    "    result = pd.concat(all_df, ignore_index=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import JSONDecodeError\n",
    "\n",
    "# ---------------- 1) Safe wrapper that calls YOUR enrichr function ----------------\n",
    "_ENR_CACHE = {}  # key: tuple(sorted(genes)) -> DataFrame (copy)\n",
    "\n",
    "def run_enrichment_safe(run_enrichment_func, community, retries=5, base_sleep=0.8):\n",
    "    \"\"\"\n",
    "    Calls user's run_enrichment_func(community) with retries + memoization.\n",
    "    Returns a DataFrame (possibly empty). Never raises JSONDecodeError outward.\n",
    "    \"\"\"\n",
    "    # Ensure we always pass a list of gene symbols (never a bare string)\n",
    "    genes = np.atleast_1d(np.array(community, dtype=object)).tolist()\n",
    "    if len(genes) == 0:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    key = tuple(sorted(genes))\n",
    "    if key in _ENR_CACHE:\n",
    "        return _ENR_CACHE[key].copy()\n",
    "\n",
    "    for a in range(retries):\n",
    "        try:\n",
    "            df = run_enrichment_func(genes,TERM_SCORE_CAP,PERCENTAGE)\n",
    "            if df is None:\n",
    "                # treat as transient failure to trigger retry\n",
    "                raise RuntimeError(\"run_enrichment_func returned None\")\n",
    "            _ENR_CACHE[key] = df.copy()\n",
    "            return df\n",
    "        except (JSONDecodeError, OSError, RuntimeError, ValueError) as e:\n",
    "            # Transient errors from HTTP/JSON/file handling inside gseapy\n",
    "            if a == retries - 1:\n",
    "                # Give up: return empty so pipeline continues\n",
    "                return pd.DataFrame()\n",
    "            time.sleep(base_sleep * (2 ** a) + np.random.rand() * 0.3)\n",
    "\n",
    "    return pd.DataFrame()\n",
    "\n",
    "# ---------------- 2) Minimal bootstrap to record robust terms ----------------\n",
    "def get_robust_terms(communities_HGNC, run_enrichment_func,\n",
    "                     R=50, leaveout=0.10, recurrence_cutoff=0.70, seed=42):\n",
    "    \"\"\"\n",
    "    Uses YOUR run_enrichment_func(community)->DataFrame (already filtered to significant terms).\n",
    "    Returns DataFrame with columns: community_id, term, recurrence (and Gene_set if available).\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rows = []\n",
    "\n",
    "    for cid, community in enumerate(communities_HGNC):\n",
    "        n = len(community)\n",
    "        if n == 0:\n",
    "            continue\n",
    "        drop_k = max(1, int(np.floor(leaveout * n)))\n",
    "        counts = Counter()\n",
    "\n",
    "        for _ in range(R):\n",
    "            # Jackknife subset (ensure not empty)\n",
    "            keep = np.ones(n, dtype=bool)\n",
    "            keep[rng.choice(n, size=min(drop_k, n), replace=False)] = False\n",
    "            sub = np.atleast_1d(np.array(community, dtype=object)[keep]).tolist()\n",
    "            if len(sub) == 0:\n",
    "                continue\n",
    "\n",
    "            df = run_enrichment_safe(run_enrichment_func, sub)\n",
    "            if df is None or df.empty:\n",
    "                continue\n",
    "\n",
    "            # Your function already returns significant terms; just count them.\n",
    "            # If it includes multiple libraries, preserve Gene_set to disambiguate names.\n",
    "            if 'Term' not in df.columns:\n",
    "                continue  # be defensive\n",
    "\n",
    "            if 'Gene_set' in df.columns:\n",
    "                terms = (df[['Term', 'Gene_set']]\n",
    "                         .dropna()\n",
    "                         .drop_duplicates()\n",
    "                         .apply(lambda r: f\"{r['Term']}|{r['Gene_set']}\", axis=1)\n",
    "                         .tolist())\n",
    "            else:\n",
    "                terms = df['Term'].dropna().drop_duplicates().tolist()\n",
    "\n",
    "            counts.update(terms)\n",
    "\n",
    "            # tiny pause helps with API rate limits if your func calls Enrichr internally\n",
    "            time.sleep(0.03)\n",
    "\n",
    "        # Keep only robust terms\n",
    "        for t, c in counts.items():\n",
    "            freq = c / max(R, 1)\n",
    "            if freq >= recurrence_cutoff:\n",
    "                if '|' in t:\n",
    "                    term, gene_set = t.split('|', 1)\n",
    "                    rows.append({'Community Index': cid, 'Term': term, 'recurrence': freq, 'Gene_set': gene_set})\n",
    "                else:\n",
    "                    rows.append({'Community Index': cid, 'Term': t, 'recurrence': freq})\n",
    "\n",
    "    return (pd.DataFrame(rows)\n",
    "              .sort_values(['Community Index', 'recurrence'], ascending=[True, False])\n",
    "              .reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "twr3 = get_robust_terms([COMMUNITIES_HGNC[1]], run_enrichment_func,\n",
    "                                R=25, leaveout=0.1, recurrence_cutoff=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "twr3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_with_recurrence = get_robust_terms(COMMUNITIES_HGNC, run_enrichment_func,\n",
    "                                R=10, leaveout=0.1, recurrence_cutoff=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_with_recurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename important terms to match terms_with_recurrence\n",
    "important_terms = important_terms.rename(columns={'index': 'community_id'})\n",
    "important_terms = important_terms.rename(columns={'Term': 'term'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_with_rec_merged = important_terms.merge(\n",
    "    terms_with_recurrence[['community_id', 'term', 'Gene_set', 'recurrence']],\n",
    "    on=['community_id', 'term', 'Gene_set'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "terms_with_rec_merged['recurrence'] = terms_with_rec_merged['recurrence'].fillna(0.0)\n",
    "\n",
    "terms_with_rec_merged = terms_with_rec_merged.sort_values(\n",
    "    ['community_id', 'recurrence'],\n",
    "    ascending=[True, False]\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_with_rec_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "community_summary = (\n",
    "    terms_with_rec_merged\n",
    "    .groupby(\"community_id\")[\"recurrence\"]\n",
    "    .agg(mean_recurrence=\"mean\", term_count=\"count\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(community_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(terms_with_recurrence.to_html(max_cols=None)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89",
   "metadata": {},
   "source": [
    "# Checks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = communities_ncbi[2]\n",
    "DGIDB_genes_ncbi = list(DGIDB_gene_to_index.keys())\n",
    "print(c1)\n",
    "print(DGIDB_genes_ncbi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comms_ncbi = index_to_ncbi(communities,index_to_gene_distinct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DGIDB_count(c):\n",
    "    return len(set(c) & set(DGIDB_genes_ncbi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = communities[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_ncbi = index_to_ncbi([c1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "DGIDB_count(c1_ncbi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_selected = community_central_genes_by_pct(graph,c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_ncbi_selected = index_to_ncbi([c1_selected])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "DGIDB_count(c1_ncbi_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comms_ncbi = [c for c in all_comms_ncbi if len(c) > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_comms_ncbi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comms_HGNC  = ncbi_to_HGNC(all_comms_ncbi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_enrichment_func(all_comms_HGNC[1],TERM_SCORE_CAP,PERCENTAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in all_comms_ncbi:\n",
    "    print(DGIDB_count(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
